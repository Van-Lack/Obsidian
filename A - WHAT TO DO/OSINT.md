

## General resources:

>[https://start.me/p/DPYPMz/the-ultimate-osint-collection](https://start.me/p/DPYPMz/the-ultimate-osint-collection "https://start.me/p/DPYPMz/the-ultimate-osint-collection")

>https://start.me/p/L1rEYQ/osint4all

```
https://irbis.espysys.com/auth
```

```
OperatorBlood's OSINT Guide:

//: TOOLS

» E-MAILS & DATA BREACHES
  ⤷ intelx.io 
  ⤷ intelfetch.net 
  ⤷ osint.industries 
  ⤷ epieos.com 
  ⤷ dehashed.com 
  ⤷ snusbase.com 

» PHONE NUMBERS
  ⤷ www.truecaller.com 
  ⤷ freecarrierlookup.com 
  ⤷ thatsthem.com/reverse-phone-lookup 

» MIXED TOOLS
  ⤷ intelx.io 
  ⤷ csint.org 
  ⤷ intelfetch.net 
  ⤷ csint.tools 

» PROPERTY RECORDS
  ⤷ www.redfin.com 
  ⤷ www.zillow.com 
  ⤷ neighbor.report 

» USERNAMES
  ⤷ github.com/sherlock-project/sherlock 
  ⤷ instantusername.com 

» TLOs
  ⤷ t.me/NekoTLO_bot 
  ⤷ t.me/XXLeakCheckBot 
  ⤷ https://t.me/uklookups_bot 
```

## Threat Intelligence resources:

### [Intelligence X](https://intelx.io/) — **Dark Web and Historical Data Search**

> https://intelx.io/

#### **Features**:

- Access historical versions of websites and files.
- Search leaked credentials and breaches.
- Investigate PGP keys, email addresses, and domains.

### [Pulsedive](https://pulsedive.com/) — **Threat Intelligence Platform**

> https://pulsedive.com/
#### **Features**:

- IOC enrichment with details about IPs, domains, and URLs.
- Free and premium threat feeds.
- Visualize relationships between threat indicators.


_**SOCRadar — Find out how popular you are on the dark webWith SOCRadarLabs's Dark Web Report instantly finds out if your data has been exposed on dark web forums, black market, leak sites, or Telegram channels**_

>https://socradar.io/labs/dark-web-report/

---
## breach-parse

- [https://github.com/hmaverickadams/breach-parse](https://github.com/hmaverickadams/breach-parse "https://github.com/hmaverickadams/breach-parse") theHarvester - [https://github.com/laramies/theHarvester](https://github.com/laramies/theHarvester "https://github.com/laramies/theHarvester") h8mail - [https://github.com/khast3x/h8mail](https://github.com/khast3x/h8mail "https://github.com/khast3x/h8mail")
- [https://monitor.mozilla.org/](https://monitor.mozilla.org/ "https://monitor.mozilla.org/")
- [https://haveibeenpwned.com/](https://haveibeenpwned.com/ "https://haveibeenpwned.com/")
- [https://t.me/telesint_bot](https://t.me/telesint_bot "https://t.me/telesint_bot")
- [https://leakcheck.io/](https://leakcheck.io/ "https://leakcheck.io/")
- [https://checkleaked.cc/](https://checkleaked.cc/ "https://checkleaked.cc/")
- [https://t.me/tgdb_bot](https://t.me/tgdb_bot "https://t.me/tgdb_bot")
### Rev Image searching tips

GeoGuessr - The Top Tips, Tricks and Techniques - [https://somerandomstuff1.wordpress.com/2019/02/08/geoguessr-the-top-tips-tricks-and-techniques/](https://somerandomstuff1.wordpress.com/2019/02/08/geoguessr-the-top-tips-tricks-and-techniques/ "https://somerandomstuff1.wordpress.com/2019/02/08/geoguessr-the-top-tips-tricks-and-techniques/")

### Email Osint

Hunter.io - [https://hunter.io/](https://hunter.io/ "https://hunter.io/") Phonebook.cz - [https://phonebook.cz/](https://phonebook.cz/ "https://phonebook.cz/") VoilaNorbert - [https://www.voilanorbert.com/](https://www.voilanorbert.com/ "https://www.voilanorbert.com/") Email Hippo - [https://tools.verifyemailaddress.io/](https://tools.verifyemailaddress.io/ "https://tools.verifyemailaddress.io/") Email Checker - [https://email-checker.net/validate](https://email-checker.net/validate "https://email-checker.net/validate") Clearbit Connect - [https://chrome.google.com/webstore/detail/clearbit-connect-supercha/pmnhcgfcafcnkbengdcanjablaabjplo?hl=en](https://chrome.google.com/webstore/detail/clearbit-connect-supercha/pmnhcgfcafcnkbengdcanjablaabjplo?hl=en "https://chrome.google.com/webstore/detail/clearbit-connect-supercha/pmnhcgfcafcnkbengdcanjablaabjplo?hl=en")

### Passwords

Dehashed - [https://dehashed.com/](https://dehashed.com/ "https://dehashed.com/") Pentester - [https://pentester.com/](https://pentester.com/ "https://pentester.com/") Hashes - <[https://hashes.org/](https://hashes.org/ "https://hashes.org/") > WeLeakInfo - [https://weleakinfo.to/v2/](https://weleakinfo.to/v2/ "https://weleakinfo.to/v2/") SnusBase - [https://snusbase.com/](https://snusbase.com/ "https://snusbase.com/") Scylla.sh - [https://scylla.sh/](https://scylla.sh/ "https://scylla.sh/")
### Social Media

Twint - [https://github.com/twintproject/twint](https://github.com/twintproject/twint "https://github.com/twintproject/twint")

### Website

Subfinder - [https://github.com/projectdiscovery/subfinder](https://github.com/projectdiscovery/subfinder "https://github.com/projectdiscovery/subfinder") Assetfinder - [https://github.com/tomnomnom/assetfinder](https://github.com/tomnomnom/assetfinder "https://github.com/tomnomnom/assetfinder") httprobe - [https://github.com/tomnomnom/httprobe](https://github.com/tomnomnom/httprobe "https://github.com/tomnomnom/httprobe") httpx Amass - [https://github.com/OWASP/Amass](https://github.com/OWASP/Amass "https://github.com/OWASP/Amass") Aquatone GoWitness - [https://github.com/sensepost/gowitness/wiki/Installation](https://github.com/sensepost/gowitness/wiki/Installation "https://github.com/sensepost/gowitness/wiki/Installation")

### OSINT Frameworks

Maltego recon-ng spiderfoot sn0int LittleBrother Wikileaker FinalRecon

### other

Hunchly - [https://hunch.ly/](https://hunch.ly/ "https://hunch.ly/")

### People

WhitePages - [https://www.whitepages.com/](https://www.whitepages.com/ "https://www.whitepages.com/") TruePeopleSearch - [https://www.truepeoplesearch.com/](https://www.truepeoplesearch.com/ "https://www.truepeoplesearch.com/") FastPeopleSearch - [https://www.fastpeoplesearch.com/](https://www.fastpeoplesearch.com/ "https://www.fastpeoplesearch.com/") FastBackgroundCheck - [https://www.fastbackgroundcheck.com/](https://www.fastbackgroundcheck.com/ "https://www.fastbackgroundcheck.com/") WebMii - [https://webmii.com/](https://webmii.com/ "https://webmii.com/") PeekYou - [https://peekyou.com/](https://peekyou.com/ "https://peekyou.com/") 411 - [https://www.411.com/](https://www.411.com/ "https://www.411.com/") Spokeo - [https://www.spokeo.com/](https://www.spokeo.com/ "https://www.spokeo.com/") That'sThem - [https://thatsthem.com/](https://thatsthem.com/ "https://thatsthem.com/")

## Voter records

Voter Records - [https://www.voterrecords.com/](https://www.voterrecords.com/ "https://www.voterrecords.com/") (modificato)

---

```python
import os  
import re  
from collections import defaultdict

def separate_urls_by_extension(filename):  
    # Regex to match file extensions correctly  
    url_pattern = re.compile(r'\.([a-zA-Z0-9]+)(?=[?#]|$)')    urls_by_extension = defaultdict(list)    with open(filename, 'r') as file:  
        for line in file:  
            match = url_pattern.search(line)  
            if match:  
                ext = match.group(1)  
                if len(ext) <= 5:  # Avoid capturing domain extensions like .com, .org  
                    urls_by_extension[ext].append(line.strip())    # Save URLs into separate files named after their extension  
    for ext, urls in urls_by_extension.items():  
        output_file = f"{ext}.txt"  
        with open(output_file, 'w') as f:  
            f.write("\n".join(urls))  
        print(f"Saved {len(urls)} URLs to {output_file}")if __name__ == "__main__":  
    input_file = "allurls.txt"  
    if os.path.exists(input_file):  
        separate_urls_by_extension(input_file)  
    else:  
        print(f"File {input_file} not found!")
```

**NOTE : IN THE ABOVE CODE THE ALLURLS.TXT U HAVE TO SAVE ALL COLLECTED DOMAIN IN THE FILE.AND U CAN NAME IT WHATEVER U WANT**


---

## Find Subdomains with VirusTotal and Security Trails API Key:

#VirusTotal_Recon

**Now let’s see how to create an account on security trails**

**open this below link**

[https://securitytrails.com](https://securitytrails.com/)

**click on signup**

![](https://miro.medium.com/v2/resize:fit:700/0*vDFGIkWm-ySeWohJ.png)


**Enter required details**

![](https://miro.medium.com/v2/resize:fit:700/0*-L9k_u2FYvWXY14P.png)

**Click on sign up for free**

![](https://miro.medium.com/v2/resize:fit:700/0*FaN0ULgoHDApuzT7.png)


**Go to email and confirm the email**

![](https://miro.medium.com/v2/resize:fit:700/0*kWiMszY0Mf_VFtOO.png)


**After completing the creation of your account**

**Go to api keys session**

![](https://miro.medium.com/v2/resize:fit:700/0*O1Qcqew-7i81vXai.png)


**Copy that api key**

![](https://miro.medium.com/v2/resize:fit:700/0*7Dp3Y8iX2YnQE7Nn.png)

**open this default location of haktrails config file :**

```bash
nano ~/.config/haktools/haktrails-config.yml
```

![](https://miro.medium.com/v2/resize:fit:700/0*JQF58fVt8p23iOhM.png)


**paste your api keys**

![](https://miro.medium.com/v2/resize:fit:700/0*l827wu8Z2uBW44y9.png)


> **_After save it this using ctrl+s ctrl+x_**

**Find subdomains using virus total**:

![](https://miro.medium.com/v2/resize:fit:700/0*VOuyUeAURlzT_zE0.png)

**You can see 279 subdomains**

![](https://miro.medium.com/v2/resize:fit:356/0*gIGYlUMKua9Wl6uy.png)

**paste those subs into a file**

> nano evil_subs

![](https://miro.medium.com/v2/resize:fit:700/0*tIJK_nXeTIeOZrWI.png)


> **_ctrl+s ctrl+x to save and exit._**

**Use this command to find subdomains with haktrails tool**

> cat evil_subs | haktrails subdomains | tee evil_subs_round_1

![](https://miro.medium.com/v2/resize:fit:700/0*XtpBq0CUTZioeZdY.png)

**Do the same process again**

> cat evil_subs_round_1 | haktrails subdomains | tee evil_subs_round_2

![](https://miro.medium.com/v2/resize:fit:700/0*OuYdPrGOrja3BAIJ.png)


**Do the same process again**

> cat evil_subs_round_2 | haktrails subdomains | tee evil_subs_round_3

![](https://miro.medium.com/v2/resize:fit:700/0*mOzQnMz-ZE7-mG7A.png)


**Do the same process again**

**You will find many subdomains using this technique.**

> **NOTE: CREATE MORE ACCOUNTS TO GET MORE SUBDOMAINS.**

————————————————————————————————————

## Threat Hunting AsyncRAT C2 Servers Using OSINT

### RAT Information Gathering using Dorking

**FOFA Dorking**

```ini
cert="AsyncRAT Server"
```

![None](https://miro.medium.com/v2/resize:fit:700/1*cbWfufVZbmXMe82kg0v1Dg.png)


![None](https://miro.medium.com/v2/resize:fit:700/1*J2K_nyYFoUX_TZOGGyQtTQ.png)

🔍Certificate Generation Code Search

![None](https://miro.medium.com/v2/resize:fit:700/1*2-CwB0B87glVdDltudIbWg.png)

						Github AsyncRAT Code

![None](https://miro.medium.com/v2/resize:fit:700/1*_o2PXOjDj1pygCIZFoxXXQ.png)

Next , check where this function `CreateCertificateAuthority` is getting called

![None](https://miro.medium.com/v2/resize:fit:700/1*ZaucPhPYOli1e0wq8sk5Mg.png)

Builder where it prompts the user to enter the CN value.

So we verified that **CN value is user-controlled** and not set by default and definitely it's not a good idea to set the value as the name of RAT itself.

So the FOFA results are most probably traps or deployed by beginners and script kiddies.

#### 🔍Hybrid Analysis Results Dorking

```vb
site:hybrid-analysis[.]com "AsyncRAT"
```

![None](https://miro.medium.com/v2/resize:fit:700/1*qChneP0ciHX6u4YgNr5WWw.png)

Based on the file submission name, it appears to be AsyncRAT

![None](https://miro.medium.com/v2/resize:fit:700/1*KCJMriP8BKEklMlAmcLyQw.png)

```vb
site:bazaar[.]abuse[.]ch "AsyncRAT"
site:bazaar[.]abuse[.]ch "AsyncRAT" inurl:sample
```

![None](https://miro.medium.com/v2/resize:fit:700/1*2Bcyqp2CnSSG79bsY1G5Ag.png)

![None](https://miro.medium.com/v2/resize:fit:700/1*RyVgKDCUgDHDx5XJjF47Xw.png)

					MalwareBazaar Sample Results

![None](https://miro.medium.com/v2/resize:fit:700/1*uwP-1f2XEHFkgC9Ah_Ja2w.png)

					MalwareBazaar Sample Results

![None](https://miro.medium.com/v2/resize:fit:700/1*AWD5afJNb0Q9TKkaavXqoA.png)


#### 🐞VirusTotal Results Dorking

![None](https://miro.medium.com/v2/resize:fit:700/1*hTDQVh1KzbqMIlh3-OM7fA.png)

							VirusTotal Google Indexed Pages

![None](https://miro.medium.com/v2/resize:fit:700/1*twDkfUhw3ywbW7LiJ8H4nQ.png)

**🕸️Domains & IPs contacted by this RAT**

![None](https://miro.medium.com/v2/resize:fit:700/1*j_LNGmpMCaZJnbLqHjylRA.png)

**IP Dorking using FOFA & Shodan**

![None](https://miro.medium.com/v2/resize:fit:700/1*6XzeoPGp4i8rt0gAGB_OnQ.png)

Ports: 10049, 10255, 12020, 12214, 12309

Protocols: ssh (6), http(1)

```
shodan[.]io/host/enter_ip_here
```

![None](https://miro.medium.com/v2/resize:fit:700/1*cu4ykgt1RiJWQesSvozbqQ.png)

— — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — 
# Software as a Service (SaaS) platform for Subdomain Enumeration

✔️ **Already Resolved**: Only valid, live subdomains are stored.

✔️ **Cross-Checked**: Data is gathered from multiple sources for better accuracy.

✔️ **Easily Accessible**: No setup required — just enter a domain and get results instantly.

![None](https://miro.medium.com/v2/resize:fit:700/1*fLafN41Hcd4Vd2OjP0LpiQ.jpeg)


### How to Use PugRecon?

Using PugRecon is **super simple**. Follow these steps:

1. **Go to** **[PugRecon Dashboard](https://dash.pugrecon.celes.in/)**.
2. **Enter the target domain** (e.g., `example.com`).
3. **Hit search**, and PugRecon will return a list of subdomains.
4. **Export and analyze** the results for potential vulnerabilities.

🔹 _Tip_: Always cross-verify results with tools like [Subfinder](https://github.com/projectdiscovery/subfinder) or [Amass](https://github.com/owasp-amass) for deeper insights.

## Subdomains 

_multiple tools in one:_

> repo: https://github.com/0xKayala/NucleiFuzzer

![[Pasted image 20250209231207.png]]

> sudo  nf -d https://perdoom.com

! **Make sure to install all the tools before running:**

![[Pasted image 20250209233500.png]]



![[Pasted image 20250209231001.png]]

> subfinder -d example.com -o subdomains.txt

> subfinder -d test.com -o passive2.txt -all

> amass enum -passive -d example.com -o amass_subdomains.txt

or

> amass enum -passive -df domains.txt -o amass_subdomains.txt

### Utilizing machine learning to collect more subdomains using subwiz

# Installing subwiz:

> pip install subwiz

# Subwiz usage:

“Subwiz” itself is pretty simple to use, there isn’t much arguments we need to worry about as you can see:

![[Pasted image 20250205090906.png]]

We only care about the `-i` and `-n` arguments here, Where we provide the input file that contains the subdomains, and the number of predictions for the model

> `I been testing subwiz for a week or two now, and the number of predictions actually matter, whenever the normal 500–5000 number of predications would fail to locate any new subdomain I would use 10000 and such high numbers and end up finding more and more subdomains, yet the process takes time and set your laptop on fire`

A simple subwiz usage would be something like this

```Shell
subwiz -i nba_subdomains.txt -n 500 
```

> `Subwiz’s default number of predictions is actually 500 by default, once subwiz stops giving you results you should always increase the predictions number until it can’t anything anymore`

# Automating subwiz for your recon:

Finally for the fun part, subwiz is okay but it lacks the actual functionality to be affective as a recon tool, in order to make it an affective tool it must enumrate until there’s no results anymore, and we wouldn’t do that manually

That’s why I did write [wizsub.sh](https://github.com/DEMON1A/wizsub.sh), a bash script that automates the process of subdomains prediction using subwiz **and appends the subdomains found by subwiz to the original subdomains file**

Once you can download wizsub.sh and use it using the following commands:

```
git clone https://github.com/DEMON1A/wizsub.sh  
cd wizsub.sh/  
chmod +x wizsub.sh  
./wizsub.sh
```

Wizsub’s usage is pretty simple

```
Usage: ./wizsub.sh <domain_name> <number_of_loops> <subdomains_file_path> <number_of_predictions>
```

You just need to provide the domain name, number of loops, and the path to your subdomains file.

> `Number of predictions is optional, it’s set to 500 by default by the bash script script too, yet if wizsub says subwiz stopped giving results you should run wizsub again but with a higher number of predictions number`

Wizsub do have an infinite loop option too, setting the number of loops to `x` would do an infinite loop that would only be stopped **when subwiz stop giving results**

> ./wizsub.sh nba.com x nba_subdomains.txt

And with that, we simply found **33 more subdomains**, that we wouldn’t have found without subwiz, and we can find more and more increasing the number of prediction

![[Pasted image 20250205092126.png]]


## General Scraping:


- **Internet Archive →** [district →](https://github.com/lc/gau) [waybackurls](https://github.com/tomnomnom/waybackurls)
- **Github Scraping →** [github-subdomains](https://github.com/gwen001/github-subdomains)
- **GitLab Scraping →** [gitlab-subdomains](https://github.com/gwen001/gitlab-subdomains)
- [https://chaos.projectdiscovery.io/#/](https://chaos.projectdiscovery.io/#/) → it is like database or something here u can get all subdomains for public bug bounty programs, yeah it is useless when you work in a private one.

## Look for Alive SWAGGER APIs:

```
then look for live targets with httpx and probe tool.
```

![[Pasted image 20241227223143.png]]

⚡️You can use #httpx to request any path and see the status code and other details on the go, filter, or matcher flags if you want to be more specific.   
  
✅httpx -path /swagger-api/ -status-code -content-length

Once I had the live subdomains, I crawled them to extract URLs that could have vulnerabilities. I used a combination of tools: waybackurls, **Go**, and **Katana** to gather potential attack points.

(use waymore too)


## Commands:

katana -u https://example.com -o urls.txt

cat live_subdomains.txt | waybackurls > waybackurls.txt

**Waybackurls** gathers archived URLs from Wayback Machine and **Katana** performs deeper crawling to discover potential attack surfaces. 


# Step 4: Vulnerability Scanning with Nuclei

Next, I used **Nuclei** to scan the URLs for vulnerabilities. Instead of relying only on default templates, I used **custom Nuclei templates** that I had previously created, stored in `/opt/rooter/`. This was key to finding a **Local File Inclusion (LFI)** vulnerability.

## Command:

nuclei -l live_subdomains.txt -t /opt/rooter/custom_templates/ -o nuclei_report.txt

This command ran Nuclei using my custom templates and output the results to `nuclei_report.txt`.

# Step 5: Vulnerability Verification

After the automated scan, I manually verified the vulnerability by exploiting the LFI using standard file inclusion payloads, confirming that sensitive server files could be accessed through the vulnerable endpoint.

![](https://miro.medium.com/v2/resize:fit:875/1*dsElyWI7zg3gzvkIWUYqRQ.png)

# Step 6: Reporting and Reward

After confirming the LFI vulnerability, I submitted it through the target’s bug bounty program. The vulnerability was validated quickly, and I received a bounty of **$3000**.

----

# Finding Hidden Assets — Deep Recon 

Find Origin IP:

#### What is Favicon?

- Go to your target domain or subdomain
- View page-source
- Ctrl+F (favicon)

![None](https://miro.medium.com/v2/resize:fit:700/1*pYQ_iGnYSt_wA1ga6nRtsw.png)



![None](https://miro.medium.com/v2/resize:fit:700/1*o25h0fahCNGsoqXdyViecg.png)



> Hash Algorithm

![None](https://miro.medium.com/v2/resize:fit:700/1*yokIKu6SVSSsA8IGvSHwEg.png)



#### ⚙️Methods to calculate

- Using online tools.
- Using a simple script locally.

#### 1️⃣ Calculate the hash value using

https://www.favihash.com/

![None](https://miro.medium.com/v2/resize:fit:700/1*cI_hozoBwshLacL1JmpJug.png)

> If you don't want to use such websites, as they might store our data like: IP and the target we are searching for, here is a Python script to calculate favicon hash easily:

#### 2️⃣ Python Script

```python
import mmh3
import requests
import base64
import argparse
import codecs
from requests.exceptions import RequestException
from urllib.parse import urlparse
from requests.packages.urllib3.exceptions import InsecureRequestWarning
from requests.packages.urllib3 import disable_warnings

disable_warnings(InsecureRequestWarning)

ascii_art = r"""
  █████▒▄▄▄    ██▒   █▓ ██▓ ▄████▄  ▒█████   ███▄    █         ██▀███  ▓█████ ▄████▄  ▒█████   ███▄    █ 
▓██   ▒▒████▄ ▓██░   █▒▓██▒▒██▀ ▀█ ▒██▒  ██▒ ██ ▀█   █        ▓██ ▒ ██▒▓█   ▀▒██▀ ▀█ ▒██▒  ██▒ ██ ▀█   █ 
▒████ ░▒██  ▀█▄▓██  █▒░▒██▒▒▓█    ▄▒██░  ██▒▓██  ▀█ ██▒       ▓██ ░▄█ ▒▒███  ▒▓█    ▄▒██░  ██▒▓██  ▀█ ██▒
░▓█▒  ░░██▄▄▄▄██▒██ █░░░██░▒▓▓▄ ▄██▒██   ██░▓██▒  ▐▌██▒       ▒██▀▀█▄  ▒▓█  ▄▒▓▓▄ ▄██▒██   ██░▓██▒  ▐▌██▒
░▒█░    ▓█   ▓██▒▒▀█░  ░██░▒ ▓███▀ ░ ████▓▒░▒██░   ▓██░       ░██▓ ▒██▒░▒████▒ ▓███▀ ░ ████▓▒░▒██░   ▓██░
 ▒ ░    ▒▒   ▓▒█░░ ▐░  ░▓  ░ ░▒ ▒  ░ ▒░▒░▒░ ░ ▒░   ▒ ▒        ░ ▒▓ ░▒▓░░░ ▒░ ░ ░▒ ▒  ░ ▒░▒░▒░ ░ ▒░   ▒ ▒ 
 ░       ▒   ▒▒ ░░ ░░   ▒ ░  ░  ▒    ░ ▒ ▒░ ░ ░░   ░ ▒░         ░▒ ░ ▒░ ░ ░  ░ ░  ▒    ░ ▒ ▒░ ░ ░░   ░ ▒░
 ░ ░     ░   ▒     ░░   ▒ ░░       ░ ░ ░ ▒     ░   ░ ░          ░░   ░    ░  ░       ░ ░ ░ ▒     ░   ░ ░ 
             ░  ░   ░   ░  ░ ░         ░ ░           ░           ░        ░  ░ ░         ░ ░           ░ 
                   ░       ░                                                 ░                           
"""


RED = '\033[31m'
CYAN = '\033[96m'
GREEN = '\033[92m'
RESET = '\033[0m'

print(f"{RED}{ascii_art}{RESET}")
print(f"{CYAN}Script by LegionHunter{RESET}")

def fetch_favicon(url):
    try:
        response = requests.get(url, timeout=10, verify=False)
        response.raise_for_status()
        return response.content
    except RequestException as e:
        raise RuntimeError(f"Failed to fetch favicon: {e}")

def calculate_mmh3_hash(content):
    b64_content = codecs.encode(content,"base64")
    return mmh3.hash(b64_content)

def main():
    parser = argparse.ArgumentParser(description="Calculate MurmurHash3 hash of a favicon from a URL.")
    parser.add_argument(
        '-u', '--url',
        required=True,
        help="URL to fetch the favicon (e.g., https://example.com/favicon.ico)"
    )
    parser.add_argument(
        '--shodan',
        action='store_true',
        help="Include a Shodan search link in the output"
    )
    args = parser.parse_args()

    try:
        favicon_content = fetch_favicon(args.url)
        hash_value = calculate_mmh3_hash(favicon_content)

        print(f"{GREEN}[+] Favicon Hash (MurmurHash3):{RESET} {hash_value}")
        if args.shodan:
            print(f"{GREEN}[+] Shodan Search:{RESET} https://www.shodan.io/search?query=http.favicon.hash%3A{hash_value}")

    except Exception as e:
        print(f"Error: {e}")

if __name__ == "__main__":
    main()
```

First, install the required libraries and modules

```
pip3 install mmh3
pip3 install requests
```

#### ✅Execute

```
python hash.py -u https://gitlab.com/assets/favicon-72a2cad5025aa931d6ea56c3201d1f18e68a8cd39788c7c80d5b2b82aa5143ef.png --shodan
```

In Windows, `python` works fine and in Linux, `python3` will be feasible, check accordingly!

#### 📍Output

Just for example, I took one favicon.

![None](https://miro.medium.com/v2/resize:fit:700/1*S9IEmu5EZ_hC6NRc73i6eg.png)

									Windows Powershell 

#### ⌛Limitations of the script

- You need to provide the exact location of the favicon image.
- Only one URL at a time.
- No support for mass recon.

> Now we search on Shodan via the dork

```
> http.favicon.hash:<valuehere>
```



![None](https://miro.medium.com/v2/resize:fit:700/1*A8oUDsVGYKjKjZCTEa7jRA.png)

#### It helps to find ❓

- Origin IP
- Hidden assets
- Phishing domains as well
- Assets hosted on the Cloud which is not searchable by general Shodan filters like org, s**sl.cert.subject.cn, IP ranges, CIDR ranges, domain, hostname, etc..because it is hosted on IPs owned by Cloud providers** but the things hosted on it are controlled and owned by the customers of it. The difficult part is to explain to the triager the asset is in scope, although it is hosted on the Cloud.
- Example: Some of the hidden assets might be hosted on GCP, Microsoft Azure, DigitalOcean, AWS, Linode, etc…
- Some might be honeypots as well.

---

Learn new exploits: https://owasp.org/www-project-web-security-testing-guide/v42/

creds for: https://trickest.io/auth/login

email: befiwos533@gianes.com

passwd: hyy467%vtdhuiudS

---

# Trickster.io

#trick
## DNS-BRUTE-FORCING:

```js
gobuster dns -d atg.se -w wordlist.txt
```

![](https://www.yeswehack.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fd51e1jt0%2Fproduction%2F10a3e83adac19c84bdf8da1016239a271f98ec0a-1882x1318.png&w=3840&q=75)

As we can see above, our Gobuster scan uncovered a couple of related subdomains. To adapt enumeration to your target, you should create a custom wordlist by analysing patterns in subdomains discovered from third-party databases. These patterns typically reflect an organisation's naming conventions.

If you then use this wordlist with [mksub](https://github.com/trickest/mksub?tab=readme-ov-file) by [Trickest](https://github.com/trickest), you can further refine these patterns and generate additional subdomain variations. We can then perform a more optimised brute-force attack against our target domain.

![](https://www.yeswehack.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fd51e1jt0%2Fproduction%2Fb076544c9542a06b31b985f8ff40212adeaf171d-1393x1016.png&w=3840&q=75)


in trickest.io look for amazon AWS websites:

![[Pasted image 20241115182747.png]]

Look for hostnames:


![[Pasted image 20241115183012.png]]

**The most valuable thing is to find the path where you have found a new endpoint if you didn't found it before**


An example:

![[Pasted image 20241115184541.png]]

### Leads

Look for web servers:

![[Pasted image 20241115184656.png]]

Look for login pages or "forgot password" pages:

![[Pasted image 20241115184735.png]]

![[Pasted image 20241115184806.png]]

![[Pasted image 20241115184853.png]]

look for applications that are APIs or endpoints:

![[Pasted image 20241115184954.png]]

or ->   final_url ~ "swagger"


## Internal assets

![[Pasted image 20241115185517.png]]


![[Pasted image 20241115185458.png]]

You can use this WebApp too: [Penetration testing toolkit, ready to use - Pentest-Tools.com](https://pentest-tools.com/)
![[Pasted image 20241118204845.png]]

![[Pasted image 20241118204833.png]]


----


Download a nuclei template:

search the desired repository in github and copy the url:

![[Pasted image 20241116112959.png]]



Then download the template with the "wget   {github link} --no-check-certificate"

![[Pasted image 20241116112716.png]]

![[Pasted image 20241116112617.png]]

Use template:

![[Pasted image 20241116113331.png]]


Now try this:

https://github.com/coffinxp/nuclei-templates 

```
how to use in bug bounty programs:
subfinder -d xyz.com -all  | nuclei -t crlf.yaml -rl 50
subfinder -d xyz.com -all  | nuclei -t openRedirect.yaml -rl 100
subfinder -d xyz.com -all  | nuclei -t iis.yaml
subfinder -d xyz.com -all  | nuclei -t cors.yaml -rl 100
subfinder -d xyz.com -all  | waybackurls | gf sqli | uro | nuclei -t errorsqli.yaml -rl 50
```

![[Pasted image 20241223175248.png]]


---

# Recon

```bash
subfinder -all -silent -d example.com -o subfinder.txt

Next, I checked which subdomains were alive using `httpx`:

cat subfinder.txt | httpx-toolkit  -silent -sc -probe -title -td -ip -t 90 -mc 200,404,403,302,301,303,304 > alive01.txt

```


**TIP: do subdomain scan on  2° level and 3° level subdomains too.**

test for xss with subfinder and bxss:

![[Pasted image 20241223215205.png]]


look for logs files:

![[Pasted image 20241223214340.png]]

After that, test for subdomain takeover:

![[Pasted image 20241223215314.png]]

![[Pasted image 20241223160640.png]]

## Overly Permissive CORS Policies: A Subtle Yet Dangerous Oversight

**Tools and Techniques**

> **Tools:**

- `Postman`: To test CORS headers and simulate cross-origin requests.
- `CORS Misconfig Scanner`: Automated tools to identify overly permissive CORS policies.
- Browser DevTools: Quick manual checks for `Access-Control-Allow-Origin` headers.

> **Methodology:**

- Use browser developer tools to identify CORS headers on API responses.
- Craft malicious requests to test the impact of cross-origin access.
- Leverage exposed credentials or tokens in conjunction with CORS exploits.

**The Exploit**

I exploited an API with `Access-Control-Allow-Origin: *` and `Access-Control-Allow-Credentials: true`, enabling me to steal user session tokens and hijack accounts.


### nuclei:

![[Pasted image 20241223215054.png]]


#ffuf

![[Pasted image 20250421134929.png]]

## Bypass Rate Limiting

**Implement Rate Limiting**: Set a rate limit to ensure you're not sending too many requests in a short period. This can often prevent you from triggering anti-bot mechanisms.

```bash
ffuf -u https://targetsite.com/FUZZ -w wordlist.txt -rate 1-10

```

**Use Rotating Proxies**: By rotating your proxies, you can distribute the load across multiple IPs, reducing the chances of getting blocked.

```bash
ffuf -u https://targetsite.com/FUZZ -w wordlist.txt -x http://proxy1:port,http://proxy2:port,http://proxy3:port

```

```bash
ffuf -u https://targetsite.com/FUZZ -w wordlist.txt -H "User-Agent: CustomAgent"

```


[7 Overlooked recon techniques to find more vulnerabilities](https://www.intigriti.com/researchers/blog/hacking-tools/7-overlooked-recon-techniques-to-find-more-vulnerabilities)

_7 different techniques that are often overlooked but are still effective in finding new API endpoints, app routes and input parameters. Let's take a look at each one of them in detail._

1. A more recommended approach is using targeted wordlists that are crafted based on your target. Tools like [CeWL](https://github.com/digininja/CeWL) can help you generate custom wordlists based on product and API documentation or help resources.

You can later use this custom wordlist to find more undocumented and unreferenced API endpoints that you can test for security vulnerabilities.

### 2: Virtual host (VHost) enumeration

Most hosts and servers deployed today serve multiple applications

 For example, multiple (sub)domains may point to a single IP address or host that has a running reverse proxy server (such as Nginx). This proxy server will determine based on the host header what application to serve.

Take a look at the example configuration file below:

```python
# Nginx reverse-proxy configuration
server {
    server_name app.example.com api.example.com;
    location / {
        proxy_pass http://localhost:8080;
        proxy_set_header Host $host;
    }

    # ...
}

server {
    server_name app-stg.example.com api-stg.example.com;
    location / {
        proxy_pass http://localhost:8081;
        proxy_set_header Host $host;
    }

    # ...
}
```

We can look for possible virtual hosts on our target by bruteforcing the host header. And we can do this with Ffuf for example:

```
$ ffuf -u https://example.com -H "Host: FUZZ.example.com" -w /path/to/wordlist
```

Afterward, once we've enumerated all virtual hosts, we can easily test each one of these applications individually.

![Example of a virtual host scan using Ffuf](https://www.datocms-assets.com/85623/1736582940-2025-01-11_09-08.png?auto=format "Example of a virtual host scan using Ffuf")

			Example of a virtual host scan using Ffuf

### 3: Forced browsing using different HTTP methods

Take a look at the following code snippet:

```python
import express, { Request } from 'express';

const app = express();
const port = 8080;

...

app.post('/api/render-report', async (req: Request, res: any) => {
  const { data } = req.body;

  const res: string = await renderReport(data);

  res.send(res);
});

app.listen(port, () => {
  console.log(`Server is running at port ${port}`);
});	
```

We can see on line 8 that an API endpoint has been defined that only accepts the POST HTTP method. We would've never been able to find that specific API endpoint if we only bruteforced using the GET HTTP method, the default request method for most fuzzing and content discovery tools.

It's therefore recommended to always test for multiple HTTP methods, with Ffuf, you can easily do so by specifying the HTTP method option:

```
$ ffuf -u https://api.example.com/PATH -X METHOD -w /path/to/wordlist:PATH -w /path/to/http_methods:METHOD
```

![The "/path/to/http_methods" is the path to a wordlist file containing all your HTTP methods separated by a newline feed character.](https://www.datocms-assets.com/85623/1736584341-0723659a9b2d4110412c6f55fe81dfe2-original.png?auto=format "The "/path/to/http_methods" is the path to a wordlist file containing all your HTTP methods separated by a newline feed character.")

	The "/path/to/http_methods" is the path to a wordlist file containing all your HTTP methods separated by a newline feed character.

Using this fuzzing approach will **ensure that you can also take these potential edge cases into account when certain API endpoints or application routes only respond when a specific HTTP method is used.**


### 4: JavaScript file monitoring

JavaScript file monitoring comes with benefits as it can help you get notified when your target application gets updated and new API endpoints, app routes and input parameters have been referenced.

It's also an effective approach as it ensures that you'll be one of the first to test a new component, feature or API endpoint.

However, it does require you to set up a server that you can use to monitor JavaScript files periodically. Luckily for us, there are several open-source tools that you can make use of (such as [JSMON](https://github.com/robre/jsmon)), some of which even provide support for notifications.

### Crawling with different user-agent headers

Some servers serve different versions of the application based on the user agent. This approach is most commonly used to provide extensive support for smaller screens such as mobile devices.

Therefore, it's always recommended that when crawling or intercepting requests, to set a mobile-specific user agent as well to try to discover differences in server responses and hidden functionality such as mobile-specific app routes and API endpoints.

Application versions designed for mobile devices may also contain different features. Next time when testing your target, try changing your user-agent to emulate an iOS or Android device and look for changes or application routes and API endpoints that you haven't come across earlier through the desktop version.

![Configure your proxy interceptor to emulate a mobile device](https://www.datocms-assets.com/85623/1736783464-emulate_mobile.png?auto=format "Configure your proxy interceptor to emulate a mobile device")

		Configure your proxy interceptor to emulate a mobile device

----

https://freelancermijan.github.io/reconengine/



----

![[Pasted image 20241201010606.png]]
## automated Subdomains  enumeration:

With that I knew for sure my recon setup must have as much tools as I can add, and I ended up building a script that collects all those tools outputs: **Amass, Assetfinder, Subfinder, Findomain, Github-subdomains** and even though the subdomains collection process took so much time but I ended up with much better results than just using **subfinder** or **assetfinder**

![](https://miro.medium.com/v2/resize:fit:875/1*uVdXxcW8rOSC9H2DCqn34g.png)

## Why amass is my favorite tool of all time

Amass is just better than all of those tools, much heavier yes and required me to upgrade my VPS **twice** but it was worth it, upon testing those tools amass would have the most unique subdomains compared to all of those other tools, So if your machine can handle it I would honestly recommend you add amass to your recon flow/scripts


#subdomain_collections

## How to actually build your own subdomain collection script

You can obviously use any programming/scripting language for this, but since we’re on Linux ( I assume you’re on Linux ) bash would be the best option for you, and it’s really easy you just put commands in order, and whenever you need to store data you just write it to local files, anyways a script would be something similar to this

```Bash
cd recon/$1  
echo $1 | assetfinder -subs-only | tee $1.assetfinder  
echo $1 | subfinder -all --recursive -o $1.subfinder  
findomain -t $1 --external-subdomains -u $1.findomain  
echo ">> Finished collecting subdomains" | tee -a ../../logs.txt  
sleep 1  
  
# Filter duplicate subdomains  
cat $1.assetfinder $1.subfinder $1.findomain | sort -u | filter-resolved | tee $1.resolved  
echo ">> Removed duplicate subdomains" | tee -a ../../logs.txt  
echo ">> Found $(cat $1.resolved | wc -l) subdomains for $1" | tee -a ../../logs.txt  
  
# Check alive subdomains  
cat $1.resolved | httpx -title -web-server -status-code -follow-redirects -o $1.info  
cat $1.resolved | httprobe -c 100 | tee $1.robe  
cat $1.info | awk '{ print $1 }' | tee $1.httpx  
  
echo ">> Found $(cat $1.robe | wc -l) HTTProbe and $(cat $1.httpx | wc -l) HTTPx subdomains for $1" | tee -a ../../logs.txt
```

Let me explain what the script does in details, It’s simply using as different subdomain enumeration tools and storing their output to local files in the system, upon those tools are finished, the subdomains are getting filtered from duplicates using `sort -u` in Linux, then we filter the alive hosts only that can actually get resolved from those subdomains list

Once we have the resolved list, we can do a quick **httpx** and **httprobe** checks for a quick list that I can navigate through while the rest of the recon is going, and yeah I use both httpx and httprobe cause I do believe httpx miss some results, and httprobe do actually print both active protocols **http** and **https** which is actually helpful when you pass it to a scanner or something.

With that said, You can actually build your own script following the same pattern, I would still recommend you learn some bash first though as it would be really helpful for you as a bug bounty hunter.

# It’s all about port scanning

You must wonder by now, why would I just filter the resolved subdomains instead of just using httpx or httprobe directly on the collected subdomains, well what’s a subdomain? It’s just an easier way to visit your server’s public IP address and as we know your machine can actually have a web server running on literally any port, httpx and httprobe would just check for **http:80 and https:443** on the server, and that’s what most people would go for, but if you really look into it, **httpx** would return **70** subdomains or something when the resolved hosts are like **100**, that’s 30 alive hosts missed out, don’t you think atleast 10 of them have a web server running on them?

If you really want to find hidden assets no one would find, you have to spend more time than them doing the recon, that’s why you always have to make sure you scan all **65,535** ports on the server, and trust me even though a port scan like that would take a day or two to finish but it will be really worth it, I can’t even tell you how many times I found dashboards and instances that haven’t been configured yet on those ports, and if you really put time into it that’s some really easy findings.

What to use? I would recommend **rustscan** on a target that have no WAFs, as rustscan can get you banned within 10 seconds, 5000 ports a second is crazy, yet the safer option is to use naabu, it’s not the fastest but you would get some nice results without getting banned.

### Install required tools in a single command

```
# install go-based tools  
go install -v github.com/tomnomnom/assetfinder@latest  
go install -v github.com/projectdiscovery/subfinder/v2/cmd/subfinder@latest  
go install -v github.com/tomnomnom/hacks/filter-resolved@latest  
go install -v github.com/projectdiscovery/httpx/cmd/httpx@latest  
go install -v github.com/tomnomnom/httprobe@latest  
go install -v github.com/projectdiscovery/naabu/v2/cmd/naabu@latest  
  
# findomain requires a manual installation to get the latest version  
# and i'm too lazy to write an automation script for it  
# you can still have findomain v9.0.4 by uncommenting the commands below  
# wget "https://github.com/Findomain/Findomain/releases/download/9.0.4/findomain-linux.zip"  
# unzip "findomain-linux.zip"  
# chmod +x findomain  
# sudo mv findomain /usr/local/bin  
# rm findomain-linux.zip  
# findomain -h
```

🔖Domain Checker Tool

A high-performance Go-based tool for checking the availability and responsiveness of domains, utilizing both HTTP requests and browser automation for comprehensive analysis.

📱Github: 🔗Link (https://github.com/murat-exp/Domain-Checker-Tool)

I recall the guy who developed 📱 Naabu (https://github.com/projectdiscovery/naabu) at NahamCon. He mentioned he created it because he struggled to parse 📱 Nmap (https://nmap.org/) results. 💀 Hey, no worries! You can always use the 📱 nmap-parse-output (https://github.com/ernw/nmap-parse-output) tool for that!

Naabu is fast, but is it as accurate as Nmap? No! Don't sacrifice accuracy because of speed.

---

```
installa katana:

step 1: go install github.com/projectdiscovery/katana/cmd/katana@latest  
step 2:   cd go/bin 
step 3: sudo cp katana /usr/bin/
confirm: katana -version
```


---

## API key recon:


#source: https://youtu.be/4ukbgT5mQBk


----

## ASN Recon:

![[Pasted image 20241205175807.png]]
## POC:

<iframe width="928" height="522" src="https://www.youtube.com/embed/yVqm1tvCZF4" title="ASN Recon find secret IP&#39;s and Subdomains #bugbounty #chatgpt #subdomain #takeover #tips #AI #hacker" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

![[Pasted image 20241205175857.png]]

![[Pasted image 20241205180145.png]]


![[Pasted image 20241205175430.png]]

![[Pasted image 20241205180757.png]]

Then use one of those IPs on shodan to discover more:

![[Pasted image 20241205181049.png]]

![[Pasted image 20241205181125.png]]
![[Pasted image 20241205181249.png]]

#  Hidden secrets and urls in JS Mass hunting

### Installation

![[Pasted image 20241207231626.png]]

> [gh-ost00/JS-Scanner: The powerfull Extract and Scanner Javascript urls (Upgrade Deeper search)](https://github.com/gh-ost00/JS-Scanner)

![[Pasted image 20241207232000.png]]

After that look for "secrets" with this command:

![[Pasted image 20241207232144.png]]

![[Pasted image 20241207232309.png]]

----

https://systemweakness.com/master-advanced-subdomain-enumeration-with-alterx-a-bug-bounty-hunters-ultimate-guide-d754a29738d9


---

## Exstensions

## WayBackURL

faster way to scan for waybackurls:

https://addons.mozilla.org/en-US/firefox/addon/waybackurl/

### TruffleHog

Find hidden API keys and secrets in websites

>[Trufflehog – Get this Extension for 🦊 Firefox (en-US)](https://addons.mozilla.org/en-US/firefox/addon/trufflehog/)



### Greb

_Easily capture and manipulate form parameters, URL parameters, and form data._

> [1hehaq/greb: Grab form parameters easily](https://github.com/1hehaq/greb)



### Cookie Editor

Advanced cookie management with security flag detection

> [Cookie Editor](https://addons.mozilla.org/en-US/firefox/addon/edit-cookie/)




### WebRTC Disable

Protect VPN IP from WebRTC leaks

WebRTC (Web Real-Time Communication) is a technology that allows audio, video, and data sharing between browsers without the need for plugins. While it's incredibly useful for applications like video conferencing, it can also expose your real IP address, even if you're using a VPN. This is known as a WebRTC leak.

Disabling WebRTC can help protect your privacy by preventing these leaks. There are several ways to disable WebRTC in your browser:

1. **Browser Extensions**: You can use browser extensions like WebRTC Control to easily enable or disable WebRTC. These extensions often provide a toggle button to control [WebRTC settings](https://chromewebstore.google.com/detail/webrtc-control/fjkmabmdepjfammlpliljpnbhleegehm).

> [Disable WebRTC – Get this Extension for 🦊 Firefox (en-US)](https://addons.mozilla.org/en-US/firefox/addon/happy-bonobo-disable-webrtc/)



### Open Multiple URLs

Open multiple sites simultaneously

> [Multiple Urls](https://addons.mozilla.org/en-US/firefox/addon/open-multiple-urls/)



### DotGit

Find exposed .git repositories for potential P1 information disclosure

[DotGit](https://addons.mozilla.org/en-US/firefox/addon/dotgit/)


### Page Translator

Translate websites to your preferred language

> [TWP - Translate Web Pages – Get this Extension for 🦊 Firefox (en-US)](https://addons.mozilla.org/en-US/firefox/addon/traduzir-paginas-web/)


### User-Agent Switcher

Test sites with different user-agent strings

> [User-Agent Switcher – Get this Extension for 🦊 Firefox (en-US)](https://addons.mozilla.org/en-US/firefox/addon/uaswitcher/)



### WaybackURLs

Fetch URLs from Wayback Machine archive

> [WaybackURL – Get this Extension for 🦊 Firefox (en-US)](https://addons.mozilla.org/en-US/firefox/addon/waybackurl/)


>  **Hack-Tools.**

![None](https://miro.medium.com/v2/resize:fit:700/1*_Fo3TF23PvH_E_Tz-ps8fA.png)

![None](https://miro.medium.com/v2/resize:fit:700/1*ssbSsOW-uYDvDpYAsKF57w.png)

**This is the one of the best extension for bug bounty.**

**this extension contains .php payloads, reverse shells, linux commands, lfi payloads, xss payloads, sql injection payloads and etc..**

![None](https://miro.medium.com/v2/resize:fit:700/1*4ocJzMS6FhypzAvkRldFeg.png)

**In this below image you can see sql injection payloads.**

![None](https://miro.medium.com/v2/resize:fit:700/1*tP7K08jXY9w9nMfAWKQRqA.png)

**Xss payloads.**

![None](https://miro.medium.com/v2/resize:fit:700/1*hGLWg8-bhtBbh88DqnxuFA.png)


> **Mitaka.**

![None](https://miro.medium.com/v2/resize:fit:700/1*z2U2JSnRawsYDIf6lQ6VQw.png)

**Mitaka is a browser extension for searching IP, domain, URL, hash, etc. via the context menu.**

![None](https://miro.medium.com/v2/resize:fit:700/1*XnIYGcWwey0BT_qDHWV5Iw.png)

If you searched **ssl.ibm.com** in shodan.

![None](https://miro.medium.com/v2/resize:fit:700/1*j01PnfK0xNVpsYLARoW5dA.png)

**select ibm.com. Right click and select mitaka.**

![None](https://miro.medium.com/v2/resize:fit:700/1*p5myaeQJ9LIBR29qSN2qZg.png)

**I am selected pulsedive. you can select anything you want.**

![None](https://miro.medium.com/v2/resize:fit:700/1*zeXhoQOhBQ97TGW-tfQXAQ.png)

**select ip related your target .**

![None](https://miro.medium.com/v2/resize:fit:700/1*B98eKtsPD6_JbbQnJlM0MQ.png)

![None](https://miro.medium.com/v2/resize:fit:700/1*K4G7Y3_ds7-jZDdnTYk0pQ.png)

![None](https://miro.medium.com/v2/resize:fit:700/1*fC9IqKWqE35qjyIeAk6v4g.png)

**This is a good extension. explore all things.**

> **Findsomething.**

**This extension is used to quickly extract some interesting information from the HTML source code or JS code of the web page, including possible requested resources, interface URLs, possible requested IPs and domain names, leaked ID numbers, mobile phone numbers, email addresses, etc.**

![None](https://miro.medium.com/v2/resize:fit:700/1*D-Dq1Q7qCYhDxhrbiGqWFQ.png)

**When you visited a website example yahoo.com**

![None](https://miro.medium.com/v2/resize:fit:700/1*HE6O9F9zng6WFmkMG5VHIA.png)

**After loding that website click on Findsomething extension.**

**Domain names.**

![None](https://miro.medium.com/v2/resize:fit:700/1*rlrPnw7SqjUB20H-PV00bA.png)

**You can see in below image . it gives hidden endpoints and hidden parameters.**

![None](https://miro.medium.com/v2/resize:fit:700/1*C3KbB64ulURjFnQPtQjCvw.png)

**Secrets.**

![None](https://miro.medium.com/v2/resize:fit:700/1*Z88-3EKTDe-4M7BHIIHAQg.png)

**.js endpoints.**

![None](https://miro.medium.com/v2/resize:fit:700/1*YCLSgpxTs1OdPL79MkgeBg.png)

**Explore your self this is also a one of the best extension .**

**retire.js**

![None](https://miro.medium.com/v2/resize:fit:700/1*Mw5bJYtwmmKzrVss2XDVyQ.png)

**This extension scan website for vulnberable js libraries.**

**Jsbeautifier.**

![None](https://miro.medium.com/v2/resize:fit:700/1*bHz1nboLdh5wMDO4VBlO-Q.png)

**This extension will helpful read js,css and html easily.**

> **Subdomain center.**

**For Finding subdomains .with one click.**

![None](https://miro.medium.com/v2/resize:fit:700/1*p2bTKdEOsPxy1Q4iHRUGmA.png)

![None](https://miro.medium.com/v2/resize:fit:700/1*5ngQBiSlvoj0BGWCKRF2Vg.png)

**Your target is ibm.com. visit that website and one click on this extension.**

![None](https://miro.medium.com/v2/resize:fit:700/1*2ZXh4bmMwdh0XlmW7jbXuw.png)

![None](https://miro.medium.com/v2/resize:fit:700/1*l0xFVcwNfsKRkpQDybt60w.png)

**After it will asks where to save this file.**

![None](https://miro.medium.com/v2/resize:fit:700/1*VRFTaSNJPmrpiVnRJtjdkA.png)

**Go your downloads tab click on show in folder.**

![None](https://miro.medium.com/v2/resize:fit:700/1*EW9JEvKDlw-T4CBDUMyJrA.png)

**I opened this file in notepad.**

**You can see results in below image.**

![None](https://miro.medium.com/v2/resize:fit:700/1*g6QplWG_PgQhzekPpZ2_Bw.png)

**by using this extension you will find some hidden subs .**

> **Hunter .**

![None](https://miro.medium.com/v2/resize:fit:700/1*2pC_ESgfLEhROVay9eXtWg.png)

**Using this extension you can email related to your target website.**

**Find email addresses from anywhere on the web, with just one click.**

**So lets see how to find email using this extension.**

**Enter your target website**

![None](https://miro.medium.com/v2/resize:fit:700/1*3egCzWZnCz--Jq0Fd3wmgg.png)

**Click on hunter extension.**

![None](https://miro.medium.com/v2/resize:fit:700/1*YusGScWRZyqpWuNkE-d8Ew.png)

Vunlers Web Scanner.**

![None](https://miro.medium.com/v2/resize:fit:700/1*sRuvFPrlWWwzl6SfFTTODQ.png)

**Tiny vulnerability security scanner based on vulners.com vulnerability database. It provides you ability to passively scan websites that you surf, on known vulnerabilities.**

**This is a good extension to detect vulnerabilities when you are surfining your target websites.**

![None](https://miro.medium.com/v2/resize:fit:700/1*6hPWC3HYkXIWRrGNRBvbfw.png)



**OWASP Penetration Testing Kit.**

![None](https://miro.medium.com/v2/resize:fit:700/1*46HC3thsmkOLQWVRRRUMbw.png)

**Key Features: In-Browser Runtime Scanning: PTK offers Dynamic Application Security Testing (DAST) and Software Composition Analysis (SCA) scanning right within your browser. Detect SQL Injections, Command Line Injections, Stored and Reflected Cross-Site Scripting (XSS) vulnerabilities, and more. It even identifies complex threats like SQL Authentication Bypass, XPath injections, and JWT attacks.**

**You can see in Below image it detects waf. tenchnology are using in this website.**

![None](https://miro.medium.com/v2/resize:fit:700/1*fwudCpeWB7CjExiA4WMV0A.png)

**You can check cheatsheets also.**

![None](https://miro.medium.com/v2/resize:fit:700/1*1nqtX_Tzb2uiFBehRJGaYg.png)

![None](https://miro.medium.com/v2/resize:fit:700/1*I-uwjnBwH9-LT2-mElNVaQ.png)

**You can do portscanning also using this extension.**

![None](https://miro.medium.com/v2/resize:fit:700/1*nrC-OjojqniIaryBETgjow.png)

![None](https://miro.medium.com/v2/resize:fit:700/1*8C6DCyAWhNiUNvpg0xzzNQ.png)

 **HackBar.**

![None](https://miro.medium.com/v2/resize:fit:700/1*5yzmEF--l8fD0g0HP4ZuxA.png)

**This extension will give sql,xss,xxe,lfi payloads with diffrent encoded values etc..**

![None](https://miro.medium.com/v2/resize:fit:700/1*_Z5WgcDNiYBXKt3ivNJ5wg.png)

![None](https://miro.medium.com/v2/resize:fit:700/1*gNP9QrEdK-a54aIyG_zzQA.png)

**User-Agent switcher.**

![None](https://miro.medium.com/v2/resize:fit:700/1*yzLfyjuVf-5-qFB0LT2NIQ.png)

**You can cange your user agent using this extension.**

**You can use all these browser agents.**

![None](https://miro.medium.com/v2/resize:fit:700/1*rP7SuJMaNP4pWE5yc5z1Bg.png)

**ModHeader.**

![None](https://miro.medium.com/v2/resize:fit:700/1*a40VC_bKFGJg2NLBcipzjA.png)

**You can can edit/modify request header and responce headers using this extension and etc..**

**pwnfox.**

![None](https://miro.medium.com/v2/resize:fit:700/1*09uu2T_r8FaoyOEyDnYMHg.png)

**In bug bounty hunting time we need to create multiple accounts. for testing. using this extension we will able to create multiple accounts staying in same tab.**

**I clicked on this extension .**

![None](https://miro.medium.com/v2/resize:fit:700/1*QUK8qzzGoLYVTIDJiCUE0Q.png)

**I clicked on yellow tab and red tab.**

![None](https://miro.medium.com/v2/resize:fit:700/1*1zhLEgZ5tsVe4w5aVvs1HA.png)

![None](https://miro.medium.com/v2/resize:fit:700/1*KQB4Z2e0GegSxOd-b5smRw.png)

**TWP-TranslateWebPages.**

**In bug bounty hunting time we want to change some other language to english or your language. This extension will helpful to change language without opening new tab.**

**Example :**

![None](https://miro.medium.com/v2/resize:fit:700/1*WDzSYYvwYdPyN9v43UrCtw.png)

![None](https://miro.medium.com/v2/resize:fit:700/1*gt9L9rBtMRHbmGElKIw4GA.png)

![None](https://miro.medium.com/v2/resize:fit:700/1*nYpcU7wspDaBjdQORUZzQA.png)




---
## Suggested Random Tools (just resource)

Zzl - A powerful reconnaissance tool for subdomain discovery through SSL certificates.

Key Feature:
Zzl scans IP ranges and extracts subdomains from SSL certificates, making it an essential tool for thorough reconnaissance and security research.

https://github.com/DEMON1A/zzl

🔍 gitlab-subdomains - A Go-based tool to uncover subdomains via GitLab searches.

🔗 https://github.com/gwen001/gitlab-subdomains

